{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CityCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     77\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 78\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     loss \u001b[38;5;241m=\u001b[39m citywide_loss(pred, target, region_prior)\n\u001b[0;32m     80\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36mCityCAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m     43\u001b[0m     local_outputs, global_output \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mglobal_output\u001b[49m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, num_regions, hidden_dim]\u001b[39;00m\n\u001b[0;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_output(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GlobalLocalAttentionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, local_window_size):\n",
    "        super(GlobalLocalAttentionEncoder, self).__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "        self.global_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.local_window_size = local_window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, num_regions, seq_len, input_dim]\n",
    "        batch_size, num_regions, seq_len, input_dim = x.size()\n",
    "        x = x.view(batch_size * num_regions, seq_len, input_dim)\n",
    "        x = self.fc(x)  # [batch_size*num_regions, seq_len, hidden_dim]\n",
    "        \n",
    "        # Local Attention\n",
    "        local_outputs = []\n",
    "        for i in range(seq_len - self.local_window_size + 1):\n",
    "            local_x = x[:, i:i + self.local_window_size, :]\n",
    "            local_output, _ = self.local_attn(local_x, local_x, local_x)\n",
    "            local_outputs.append(local_output[:, -1, :])\n",
    "        local_outputs = torch.stack(local_outputs, dim=1)\n",
    "        \n",
    "        # Global Attention\n",
    "        global_output, _ = self.global_attn(x, x, x)\n",
    "        \n",
    "        return local_outputs, global_output\n",
    "\n",
    "class CityCAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, local_window_size, num_blocks):\n",
    "        super(CityCAN, self).__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GlobalLocalAttentionEncoder(input_dim if i == 0 else hidden_dim, hidden_dim, num_heads, local_window_size)\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "        self.fc_output = nn.Linear(hidden_dim, 1)  # Final output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            local_outputs, global_output = block(x)\n",
    "            x = local_outputs + global_output\n",
    "        \n",
    "        x = x.mean(dim=1)  # [batch_size, num_regions, hidden_dim]\n",
    "        out = self.fc_output(x)\n",
    "        return out\n",
    "\n",
    "# Loss function\n",
    "def citywide_loss(pred, target, region_prior):\n",
    "    mse_loss = F.mse_loss(pred, target)\n",
    "    cosine_loss = 1 - F.cosine_similarity(pred, target).mean()\n",
    "    return mse_loss + cosine_loss * region_prior\n",
    "\n",
    "# Example usage\n",
    "batch_size = 16\n",
    "num_regions = 77\n",
    "seq_len = 6\n",
    "input_dim = 32\n",
    "hidden_dim = 128\n",
    "num_heads = 4\n",
    "local_window_size = 3\n",
    "num_blocks = 3\n",
    "\n",
    "model = CityCAN(input_dim, hidden_dim, num_heads, local_window_size, num_blocks)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example input\n",
    "x = torch.randn(batch_size, num_regions, seq_len, input_dim)\n",
    "target = torch.randn(batch_size, num_regions, 1)\n",
    "region_prior = torch.ones(batch_size, num_regions)  # Example prior\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = citywide_loss(pred, target, region_prior)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
