{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 运行具有“pytorch_env (Python 3.11.9)”的单元格需要ipykernel包。\n",
    "# 运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
    "# 命令: \"conda install -p c:\\Users\\fengq\\anaconda3\\envs\\pytorch_env ipykernel --update-deps --force-reinstall\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# net.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "\n",
    "# stage one ,unsupervised learning\n",
    "class SimCLRStage1(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(SimCLRStage1, self).__init__()\n",
    "\n",
    "        self.f = []\n",
    "        for name, module in resnet50().named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                self.f.append(module)\n",
    "        # encoder\n",
    "        self.f = nn.Sequential(*self.f)\n",
    "        # projection head\n",
    "        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False),\n",
    "                               nn.BatchNorm1d(512),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Linear(512, feature_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.g(feature)\n",
    "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n",
    "\n",
    "\n",
    "# stage two ,supervised learning\n",
    "class SimCLRStage2(torch.nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(SimCLRStage2, self).__init__()\n",
    "        # encoder\n",
    "        self.f = SimCLRStage1().f\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
    "\n",
    "        for param in self.f.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc(feature)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss,self).__init__()\n",
    "\n",
    "    def forward(self,out_1,out_2,batch_size,temperature=0.5):\n",
    "        # 分母 ：X.X.T，再去掉对角线值，分析结果一行，可以看成它与除了这行外的其他行都进行了点积运算（包括out_1和out_2）,\n",
    "        # 而每一行为一个batch的一个取值，即一个输入图像的特征表示，\n",
    "        # 因此，X.X.T，再去掉对角线值表示，每个输入图像的特征与其所有输出特征（包括out_1和out_2）的点积，用点积来衡量相似性\n",
    "        # 加上exp操作，该操作实际计算了分母\n",
    "        # [2*B, D]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        # [2*B, 2*B]\n",
    "        sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
    "        mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n",
    "        # [2*B, 2*B-1]\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
    "\n",
    "        # 分子： *为对应位置相乘，也是点积\n",
    "        # compute loss\n",
    "        pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        # [2*B]\n",
    "        pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "        return (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    for name, module in resnet50().named_children():\n",
    "        print(name,module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、配置文件\n",
    "\n",
    "公共参数写入配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "use_gpu=True\n",
    "gpu_name=1\n",
    "\n",
    "pre_model=os.path.join('pth','model.pth')\n",
    "\n",
    "save_path=\"pth\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、无监督学习数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用CIFAR-10数据集,一共包含10个类别的RGB彩色图片：飞机（airplane）、汽车（automobile）、鸟类（bird）、猫（cat）、鹿（deer）、狗（dog）、蛙类（frog）、马（horse）、船（ship）和卡车（truck）。图片的尺寸为32×32，数据集中一共有50000张训练图片片和10000张测试图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(tensor([[[-0.1804, -0.2967, -0.3936,  ..., -0.1029, -0.1029, -0.0641],\n",
      "         [-0.2192, -0.2773, -0.3549,  ..., -0.1029, -0.1416, -0.1610],\n",
      "         [-0.2580, -0.2580, -0.2967,  ..., -0.1223, -0.1998, -0.3161],\n",
      "         ...,\n",
      "         [ 2.3784,  1.5061,  0.9439,  ...,  0.1491,  0.3430,  0.4981],\n",
      "         [ 2.1458,  1.4285,  0.9245,  ..., -0.2386,  0.0328,  0.3624],\n",
      "         [ 2.0101,  1.3898,  0.9051,  ..., -0.5100, -0.1804,  0.2848]],\n",
      "\n",
      "        [[-0.8646, -0.9629, -1.0612,  ..., -0.7662, -0.7466, -0.7072],\n",
      "         [-0.8646, -0.9236, -0.9826,  ..., -0.7662, -0.7859, -0.8056],\n",
      "         [-0.8842, -0.8646, -0.8842,  ..., -0.7662, -0.8646, -0.9826],\n",
      "         ...,\n",
      "         [ 2.3608,  1.1218,  0.1974,  ..., -0.4712, -0.1566,  0.0991],\n",
      "         [ 2.0068,  0.9841,  0.1778,  ..., -0.8056, -0.4122,  0.0204],\n",
      "         [ 1.7708,  0.8858,  0.1778,  ..., -1.0219, -0.5696, -0.0386]],\n",
      "\n",
      "        [[-1.3629, -1.4410, -1.5190,  ..., -1.2264, -1.1873, -1.1093],\n",
      "         [-1.3629, -1.4020, -1.4410,  ..., -1.2264, -1.2264, -1.2264],\n",
      "         [-1.3629, -1.3434, -1.3239,  ..., -1.2459, -1.3044, -1.4020],\n",
      "         ...,\n",
      "         [ 2.2270,  0.8417, -0.2899,  ..., -1.7531, -1.4800, -1.1678],\n",
      "         [ 1.8367,  0.6466, -0.3679,  ..., -1.8312, -1.5190, -1.1678],\n",
      "         [ 1.5636,  0.5100, -0.4264,  ..., -1.8702, -1.5580, -1.1678]]]), tensor([[[ 0.7888,  0.7888,  0.8276,  ..., -0.8395, -0.5875, -0.5100],\n",
      "         [ 0.8664,  0.8664,  0.8858,  ..., -0.8589, -0.5875, -0.5100],\n",
      "         [ 0.9633,  0.9633,  1.0214,  ..., -0.8977, -0.5875, -0.4712],\n",
      "         ...,\n",
      "         [-0.3161, -0.3355, -0.4712,  ..., -0.5681, -0.6069, -0.6263],\n",
      "         [-0.3936, -0.4130, -0.5100,  ..., -0.5875, -0.5681, -0.5681],\n",
      "         [-0.4130, -0.4518, -0.5293,  ..., -0.5875, -0.5487, -0.5293]],\n",
      "\n",
      "        [[ 0.6891,  0.6891,  0.6891,  ..., -1.0612, -0.8056, -0.7269],\n",
      "         [ 0.7874,  0.7678,  0.7678,  ..., -1.0809, -0.8056, -0.7269],\n",
      "         [ 0.9841,  0.9644,  1.0038,  ..., -1.1202, -0.7859, -0.7072],\n",
      "         ...,\n",
      "         [-0.5499, -0.5892, -0.7072,  ..., -0.8449, -0.8646, -0.8646],\n",
      "         [-0.6286, -0.6876, -0.7859,  ..., -0.7859, -0.7859, -0.7859],\n",
      "         [-0.6876, -0.7269, -0.7859,  ..., -0.8056, -0.7662, -0.7466]],\n",
      "\n",
      "        [[-0.0753, -0.0558,  0.0028,  ..., -1.2654, -1.1093, -1.0703],\n",
      "         [ 0.0613,  0.0613,  0.1198,  ..., -1.2849, -1.1093, -1.0508],\n",
      "         [ 0.3149,  0.3735,  0.4515,  ..., -1.3434, -1.0898, -1.0313],\n",
      "         ...,\n",
      "         [-0.9337, -0.9532, -1.0703,  ..., -1.2654, -1.3044, -1.3044],\n",
      "         [-1.0118, -1.0118, -1.0898,  ..., -1.3239, -1.3239, -1.3434],\n",
      "         [-1.0313, -1.0508, -1.0898,  ..., -1.3434, -1.3434, -1.3434]]]), 6)\n"
     ]
    }
   ],
   "source": [
    "# loaddataset.py\n",
    "from torchvision.datasets import CIFAR10\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PreDataset(CIFAR10):\n",
    "    def __getitem__(self, item):\n",
    "        img,target=self.data[item],self.targets[item]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            imgL = self.transform(img)\n",
    "            imgR = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return imgL, imgR, target\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    import config\n",
    "    train_data = PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、无监督训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]\n",
      "                             [--max_epoch MAX_EPOCH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\fengq\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-12752CzsAL5cAth9m.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# trainstage1.py\n",
    "import torch,argparse,os\n",
    "import net,config,loaddataset\n",
    "\n",
    "\n",
    "# train stage one\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:\" + str(config.gpu_name))\n",
    "        # 每次训练计算图改动较小使用，在开始前选取较优的基础算法（比如选择一种当前高效的卷积算法）\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current deveice:\", DEVICE)\n",
    "\n",
    "    train_dataset=loaddataset.PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data=torch.utils.data.DataLoader(train_dataset,batch_size=args.batch_size, shuffle=True, num_workers=16 , drop_last=True)\n",
    "\n",
    "    model =net.SimCLRStage1().to(DEVICE)\n",
    "    lossLR=net.Loss().to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    for epoch in range(1,args.max_epoch+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch,(imgL,imgR,labels) in enumerate(train_data):\n",
    "            imgL,imgR,labels=imgL.to(DEVICE),imgR.to(DEVICE),labels.to(DEVICE)\n",
    "\n",
    "            _, pre_L=model(imgL)\n",
    "            _, pre_R=model(imgR)\n",
    "\n",
    "            loss=lossLR(pre_L,pre_R,args.batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        print(\"epoch loss:\",total_loss/len(train_dataset)*args.batch_size)\n",
    "\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss/len(train_dataset)*args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='')\n",
    "    parser.add_argument('--max_epoch', default=1000, type=int, help='')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda:1\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Filter out unrecognized arguments\u001b[39;00m\n\u001b[0;32m     53\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[1;32m---> 55\u001b[0m train(args)\n",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m loaddataset\u001b[38;5;241m.\u001b[39mPreDataset(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_transform, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m train_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mSimCLRStage1()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     17\u001b[0m lossLR \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mLoss()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1161\u001b[0m         device,\n\u001b[0;32m   1162\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1163\u001b[0m         non_blocking,\n\u001b[0;32m   1164\u001b[0m     )\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch, argparse, os, sys\n",
    "import net, config, loaddataset\n",
    "\n",
    "# train stage one\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:\" + str(config.gpu_name))\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current device:\", DEVICE)\n",
    "\n",
    "    train_dataset = loaddataset.PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16, drop_last=True)\n",
    "\n",
    "    model = net.SimCLRStage1().to(DEVICE)\n",
    "    lossLR = net.Loss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    for epoch in range(1, args.max_epoch + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, (imgL, imgR, labels) in enumerate(train_data):\n",
    "            imgL, imgR, labels = imgL.to(DEVICE), imgR.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            _, pre_L = model(imgL)\n",
    "            _, pre_R = model(imgR)\n",
    "\n",
    "            loss = lossLR(pre_L, pre_R, args.batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        print(\"epoch loss:\", total_loss / len(train_dataset) * args.batch_size)\n",
    "\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset) * args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='')\n",
    "    parser.add_argument('--max_epoch', default=1000, type=int, help='')\n",
    "\n",
    "    # Filter out unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用的GPU数量: 1\n",
      "GPU 0: NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"可用的GPU数量:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and config.use_gpu:\n",
    "    DEVICE = torch.device(\"cuda:0\")  # 或者使用正确的GPU编号\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 7.66 GiB is allocated by PyTorch, and 52.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Filter out unrecognized arguments\u001b[39;00m\n\u001b[0;32m     53\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[1;32m---> 55\u001b[0m train(args)\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     25\u001b[0m imgL, imgR, labels \u001b[38;5;241m=\u001b[39m imgL\u001b[38;5;241m.\u001b[39mto(DEVICE), imgR\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     27\u001b[0m _, pre_L \u001b[38;5;241m=\u001b[39m model(imgL)\n\u001b[1;32m---> 28\u001b[0m _, pre_R \u001b[38;5;241m=\u001b[39m model(imgR)\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossLR(pre_L, pre_R, args\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\vscode\\SimCLR\\02-case01\\net.py:28\u001b[0m, in \u001b[0;36mSimCLRStage1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(x)\n\u001b[0;32m     29\u001b[0m     feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg(feature)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torchvision\\models\\resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    185\u001b[0m     bn_training,\n\u001b[0;32m    186\u001b[0m     exponential_average_factor,\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    188\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2513\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m   2514\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 7.66 GiB is allocated by PyTorch, and 52.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch, argparse, os, sys\n",
    "import net, config, loaddataset\n",
    "\n",
    "# train stage one\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:0\")  # 修改为使用正确的GPU编号\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current device:\", DEVICE)\n",
    "\n",
    "    train_dataset = loaddataset.PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16, drop_last=True)\n",
    "\n",
    "    model = net.SimCLRStage1().to(DEVICE)\n",
    "    lossLR = net.Loss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    for epoch in range(1, args.max_epoch + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, (imgL, imgR, labels) in enumerate(train_data):\n",
    "            imgL, imgR, labels = imgL.to(DEVICE), imgR.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            _, pre_L = model(imgL)\n",
    "            _, pre_R = model(imgR)\n",
    "\n",
    "            loss = lossLR(pre_L, pre_R, args.batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        print(\"epoch loss:\", total_loss / len(train_dataset) * args.batch_size)\n",
    "\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset) * args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='')\n",
    "    parser.add_argument('--max_epoch', default=1000, type=int, help='')\n",
    "\n",
    "    # Filter out unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fengq\\AppData\\Local\\Temp\\ipykernel_17352\\3511281425.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 过滤掉未识别的参数\u001b[39;00m\n\u001b[0;32m     80\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[1;32m---> 82\u001b[0m train(args)\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     38\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (imgL, imgR, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data):\n\u001b[0;32m     41\u001b[0m     imgL, imgR, labels \u001b[38;5;241m=\u001b[39m imgL\u001b[38;5;241m.\u001b[39mto(DEVICE), imgR\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import net\n",
    "import config\n",
    "import loaddataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# train stage one\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:0\")  # 使用GPU\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current device:\", DEVICE)\n",
    "\n",
    "    # 数据加载\n",
    "    train_dataset = loaddataset.PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16, drop_last=True)\n",
    "\n",
    "    # 模型和优化器\n",
    "    model = net.SimCLRStage1().to(DEVICE)\n",
    "    lossLR = net.Loss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "    \n",
    "    # 混合精度训练\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    \n",
    "    # 设定梯度累积步数\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    for epoch in range(1, args.max_epoch + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch, (imgL, imgR, labels) in enumerate(train_data):\n",
    "            imgL, imgR, labels = imgL.to(DEVICE), imgR.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():  # 使用混合精度训练\n",
    "                _, pre_L = model(imgL)\n",
    "                _, pre_R = model(imgR)\n",
    "                loss = lossLR(pre_L, pre_R, args.batch_size / accumulation_steps)\n",
    "            \n",
    "            # 梯度累积\n",
    "            scaler.scale(loss).backward()\n",
    "            if (batch + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        # 打印并记录每个epoch的损失\n",
    "        print(\"epoch loss:\", total_loss / len(train_dataset) * args.batch_size)\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset) * args.batch_size) + \" \")\n",
    "\n",
    "        # 每5个epoch保存一次模型\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "        \n",
    "        # 释放显存\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    # parser.add_argument('--batch_size', default=200, type=int, help='Batch size for training')\n",
    "    parser.add_argument('--batch_size', default=5, type=int, help='Batch size for training')\n",
    "    # parser.add_argument('--max_epoch', default=1000, type=int, help='Maximum number of epochs for training')\n",
    "    parser.add_argument('--max_epoch', default=10, type=int, help='Maximum number of epochs for training')\n",
    "\n",
    "    # 过滤掉未识别的参数\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fengq\\AppData\\Local\\Temp\\ipykernel_5668\\1209677497.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\fengq\\AppData\\Local\\Temp\\ipykernel_5668\\1209677497.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # 使用混合精度训练\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 0 loss: 3.4076178073883057\n",
      "epoch 1 batch 1 loss: 3.3302674293518066\n",
      "epoch 1 batch 2 loss: 3.3203444480895996\n",
      "epoch 1 batch 3 loss: 3.3986613750457764\n",
      "epoch 1 batch 4 loss: 3.348224639892578\n",
      "epoch 1 batch 5 loss: 3.398970365524292\n",
      "epoch 1 batch 6 loss: 3.4123501777648926\n",
      "epoch 1 batch 7 loss: 3.365262269973755\n",
      "epoch 1 batch 8 loss: 3.3868789672851562\n",
      "epoch 1 batch 9 loss: 3.387326717376709\n",
      "epoch 1 batch 10 loss: 3.3323373794555664\n",
      "epoch 1 batch 11 loss: 3.24664044380188\n",
      "epoch 1 batch 12 loss: 3.579559326171875\n",
      "epoch 1 batch 13 loss: 3.443418502807617\n",
      "epoch 1 batch 14 loss: 3.1393046379089355\n",
      "epoch 1 batch 15 loss: 3.4033331871032715\n",
      "epoch 1 batch 16 loss: 3.423140048980713\n",
      "epoch 1 batch 17 loss: 3.464508295059204\n",
      "epoch 1 batch 18 loss: 3.2549211978912354\n",
      "epoch 1 batch 19 loss: 3.3435544967651367\n",
      "epoch 1 batch 20 loss: 3.2035908699035645\n",
      "epoch 1 batch 21 loss: 3.3636114597320557\n",
      "epoch 1 batch 22 loss: 3.310441493988037\n",
      "epoch 1 batch 23 loss: 3.3575797080993652\n",
      "epoch 1 batch 24 loss: 3.314507246017456\n",
      "epoch 1 batch 25 loss: 3.4547009468078613\n",
      "epoch 1 batch 26 loss: 3.0566000938415527\n",
      "epoch 1 batch 27 loss: 3.3995485305786133\n",
      "epoch 1 batch 28 loss: 3.3805699348449707\n",
      "epoch 1 batch 29 loss: 3.2538554668426514\n",
      "epoch 1 batch 30 loss: 3.4381000995635986\n",
      "epoch 1 batch 31 loss: 3.363060474395752\n",
      "epoch 1 batch 32 loss: 3.2077224254608154\n",
      "epoch 1 batch 33 loss: 3.3855605125427246\n",
      "epoch 1 batch 34 loss: 3.3292760848999023\n",
      "epoch 1 batch 35 loss: 3.400019645690918\n",
      "epoch 1 batch 36 loss: 3.4646589756011963\n",
      "epoch 1 batch 37 loss: 3.344658613204956\n",
      "epoch 1 batch 38 loss: 3.250157594680786\n",
      "epoch 1 batch 39 loss: 3.2853641510009766\n",
      "epoch 1 batch 40 loss: 3.2417991161346436\n",
      "epoch 1 batch 41 loss: 3.4614486694335938\n",
      "epoch 1 batch 42 loss: 3.2025842666625977\n",
      "epoch 1 batch 43 loss: 3.3019795417785645\n",
      "epoch 1 batch 44 loss: 3.4974136352539062\n",
      "epoch 1 batch 45 loss: 3.1639490127563477\n",
      "epoch 1 batch 46 loss: 3.35568904876709\n",
      "epoch 1 batch 47 loss: 3.3913674354553223\n",
      "epoch 1 batch 48 loss: 3.333162307739258\n",
      "epoch 1 batch 49 loss: 3.141270637512207\n",
      "epoch 1 batch 50 loss: 3.4183461666107178\n",
      "epoch 1 batch 51 loss: 3.2907204627990723\n",
      "epoch 1 batch 52 loss: 3.5208983421325684\n",
      "epoch 1 batch 53 loss: 3.2649528980255127\n",
      "epoch 1 batch 54 loss: 3.2638049125671387\n",
      "epoch 1 batch 55 loss: 3.2189738750457764\n",
      "epoch 1 batch 56 loss: 3.2512574195861816\n",
      "epoch 1 batch 57 loss: 3.2381134033203125\n",
      "epoch 1 batch 58 loss: 3.204988479614258\n",
      "epoch 1 batch 59 loss: 3.1702070236206055\n",
      "epoch 1 batch 60 loss: 3.289576768875122\n",
      "epoch 1 batch 61 loss: 3.2376818656921387\n",
      "epoch 1 batch 62 loss: 3.061734437942505\n",
      "epoch 1 batch 63 loss: 3.3071017265319824\n",
      "epoch 1 batch 64 loss: 3.3104333877563477\n",
      "epoch 1 batch 65 loss: 3.2540476322174072\n",
      "epoch 1 batch 66 loss: 3.376138210296631\n",
      "epoch 1 batch 67 loss: 3.331817626953125\n",
      "epoch 1 batch 68 loss: 3.258585214614868\n",
      "epoch 1 batch 69 loss: 3.584670066833496\n",
      "epoch 1 batch 70 loss: 3.210925340652466\n",
      "epoch 1 batch 71 loss: 3.2185468673706055\n",
      "epoch 1 batch 72 loss: 3.2083473205566406\n",
      "epoch 1 batch 73 loss: 3.4950973987579346\n",
      "epoch 1 batch 74 loss: 3.2098827362060547\n",
      "epoch 1 batch 75 loss: 3.2160258293151855\n",
      "epoch 1 batch 76 loss: 3.545850992202759\n",
      "epoch 1 batch 77 loss: 3.388134002685547\n",
      "epoch 1 batch 78 loss: 3.3062171936035156\n",
      "epoch 1 batch 79 loss: 3.21425199508667\n",
      "epoch 1 batch 80 loss: 3.2787647247314453\n",
      "epoch 1 batch 81 loss: 3.154031276702881\n",
      "epoch 1 batch 82 loss: 3.238976001739502\n",
      "epoch 1 batch 83 loss: 3.129730701446533\n",
      "epoch 1 batch 84 loss: 3.204882860183716\n",
      "epoch 1 batch 85 loss: 3.106274127960205\n",
      "epoch 1 batch 86 loss: 3.187009572982788\n",
      "epoch 1 batch 87 loss: 3.0276172161102295\n",
      "epoch 1 batch 88 loss: 3.292180061340332\n",
      "epoch 1 batch 89 loss: 3.310605525970459\n",
      "epoch 1 batch 90 loss: 3.5048959255218506\n",
      "epoch 1 batch 91 loss: 3.135458469390869\n",
      "epoch 1 batch 92 loss: 3.027799129486084\n",
      "epoch 1 batch 93 loss: 3.478303909301758\n",
      "epoch 1 batch 94 loss: 3.120457887649536\n",
      "epoch 1 batch 95 loss: 3.006437301635742\n",
      "epoch 1 batch 96 loss: 2.7940807342529297\n",
      "epoch 1 batch 97 loss: 3.236776828765869\n",
      "epoch 1 batch 98 loss: 3.553389072418213\n",
      "epoch 1 batch 99 loss: 2.8246889114379883\n",
      "epoch 1 batch 100 loss: 2.9910902976989746\n",
      "epoch 1 batch 101 loss: 3.1729564666748047\n",
      "epoch 1 batch 102 loss: 3.256875991821289\n",
      "epoch 1 batch 103 loss: 3.22316837310791\n",
      "epoch 1 batch 104 loss: 3.162468433380127\n",
      "epoch 1 batch 105 loss: 3.200500965118408\n",
      "epoch 1 batch 106 loss: 3.226149082183838\n",
      "epoch 1 batch 107 loss: 3.5775420665740967\n",
      "epoch 1 batch 108 loss: 3.23846435546875\n",
      "epoch 1 batch 109 loss: 3.435640573501587\n",
      "epoch 1 batch 110 loss: 3.388779640197754\n",
      "epoch 1 batch 111 loss: 3.2627081871032715\n",
      "epoch 1 batch 112 loss: 3.351546287536621\n",
      "epoch 1 batch 113 loss: 3.365009069442749\n",
      "epoch 1 batch 114 loss: 3.262220859527588\n",
      "epoch 1 batch 115 loss: 3.34049916267395\n",
      "epoch 1 batch 116 loss: 3.0400261878967285\n",
      "epoch 1 batch 117 loss: 3.257246255874634\n",
      "epoch 1 batch 118 loss: 3.2464256286621094\n",
      "epoch 1 batch 119 loss: 3.1921706199645996\n",
      "epoch 1 batch 120 loss: 3.3987643718719482\n",
      "epoch 1 batch 121 loss: 3.378615617752075\n",
      "epoch 1 batch 122 loss: 3.3835108280181885\n",
      "epoch 1 batch 123 loss: 3.515021800994873\n",
      "epoch 1 batch 124 loss: 3.079050302505493\n",
      "epoch 1 batch 125 loss: 3.1399247646331787\n",
      "epoch 1 batch 126 loss: 3.1554112434387207\n",
      "epoch 1 batch 127 loss: 3.256467580795288\n",
      "epoch 1 batch 128 loss: 3.5693588256835938\n",
      "epoch 1 batch 129 loss: 3.1273891925811768\n",
      "epoch 1 batch 130 loss: 3.301959991455078\n",
      "epoch 1 batch 131 loss: 2.831148147583008\n",
      "epoch 1 batch 132 loss: 3.019277572631836\n",
      "epoch 1 batch 133 loss: 3.1826865673065186\n",
      "epoch 1 batch 134 loss: 3.4430489540100098\n",
      "epoch 1 batch 135 loss: 3.3065664768218994\n",
      "epoch 1 batch 136 loss: 2.978895664215088\n",
      "epoch 1 batch 137 loss: 3.4683914184570312\n",
      "epoch 1 batch 138 loss: 3.2485828399658203\n",
      "epoch 1 batch 139 loss: 3.3965346813201904\n",
      "epoch 1 batch 140 loss: 2.9368491172790527\n",
      "epoch 1 batch 141 loss: 3.0488743782043457\n",
      "epoch 1 batch 142 loss: 3.155703544616699\n",
      "epoch 1 batch 143 loss: 3.1634202003479004\n",
      "epoch 1 batch 144 loss: 3.058276891708374\n",
      "epoch 1 batch 145 loss: 3.0758779048919678\n",
      "epoch 1 batch 146 loss: 3.058274269104004\n",
      "epoch 1 batch 147 loss: 3.0636627674102783\n",
      "epoch 1 batch 148 loss: 3.251638412475586\n",
      "epoch 1 batch 149 loss: 3.3333520889282227\n",
      "epoch 1 batch 150 loss: 3.3577280044555664\n",
      "epoch 1 batch 151 loss: 3.123128890991211\n",
      "epoch 1 batch 152 loss: 2.994593381881714\n",
      "epoch 1 batch 153 loss: 3.1747207641601562\n",
      "epoch 1 batch 154 loss: 3.043219566345215\n",
      "epoch 1 batch 155 loss: 3.6434595584869385\n",
      "epoch 1 batch 156 loss: 3.1692728996276855\n",
      "epoch 1 batch 157 loss: 3.1585240364074707\n",
      "epoch 1 batch 158 loss: 2.971827507019043\n",
      "epoch 1 batch 159 loss: 3.091822624206543\n",
      "epoch 1 batch 160 loss: 2.9802143573760986\n",
      "epoch 1 batch 161 loss: 3.2287094593048096\n",
      "epoch 1 batch 162 loss: 3.1382381916046143\n",
      "epoch 1 batch 163 loss: 3.0994296073913574\n",
      "epoch 1 batch 164 loss: 3.2698974609375\n",
      "epoch 1 batch 165 loss: 3.1911048889160156\n",
      "epoch 1 batch 166 loss: 2.830362319946289\n",
      "epoch 1 batch 167 loss: 3.023669958114624\n",
      "epoch 1 batch 168 loss: 3.156048536300659\n",
      "epoch 1 batch 169 loss: 3.119135856628418\n",
      "epoch 1 batch 170 loss: 3.3490376472473145\n",
      "epoch 1 batch 171 loss: 2.8711953163146973\n",
      "epoch 1 batch 172 loss: 2.8617992401123047\n",
      "epoch 1 batch 173 loss: 3.0856423377990723\n",
      "epoch 1 batch 174 loss: 3.281892776489258\n",
      "epoch 1 batch 175 loss: 3.141909599304199\n",
      "epoch 1 batch 176 loss: 3.1278076171875\n",
      "epoch 1 batch 177 loss: 3.0243184566497803\n",
      "epoch 1 batch 178 loss: 3.215515613555908\n",
      "epoch 1 batch 179 loss: 2.9719419479370117\n",
      "epoch 1 batch 180 loss: 2.833625316619873\n",
      "epoch 1 batch 181 loss: 3.378596305847168\n",
      "epoch 1 batch 182 loss: 2.9678826332092285\n",
      "epoch 1 batch 183 loss: 2.874819278717041\n",
      "epoch 1 batch 184 loss: 2.9884328842163086\n",
      "epoch 1 batch 185 loss: 2.8750600814819336\n",
      "epoch 1 batch 186 loss: 2.705559015274048\n",
      "epoch 1 batch 187 loss: 2.8269500732421875\n",
      "epoch 1 batch 188 loss: 3.2705349922180176\n",
      "epoch 1 batch 189 loss: 3.0941715240478516\n",
      "epoch 1 batch 190 loss: 3.2360570430755615\n",
      "epoch 1 batch 191 loss: 3.425051212310791\n",
      "epoch 1 batch 192 loss: 2.875277519226074\n",
      "epoch 1 batch 193 loss: 3.228834390640259\n",
      "epoch 1 batch 194 loss: 3.26682448387146\n",
      "epoch 1 batch 195 loss: 2.9912986755371094\n",
      "epoch 1 batch 196 loss: 3.523284435272217\n",
      "epoch 1 batch 197 loss: 2.65751576423645\n",
      "epoch 1 batch 198 loss: 3.238163709640503\n",
      "epoch 1 batch 199 loss: 3.096714973449707\n",
      "epoch 1 batch 200 loss: 2.910703659057617\n",
      "epoch 1 batch 201 loss: 3.296550750732422\n",
      "epoch 1 batch 202 loss: 3.0310940742492676\n",
      "epoch 1 batch 203 loss: 2.8969221115112305\n",
      "epoch 1 batch 204 loss: 2.7357726097106934\n",
      "epoch 1 batch 205 loss: 3.195028781890869\n",
      "epoch 1 batch 206 loss: 3.3224215507507324\n",
      "epoch 1 batch 207 loss: 2.673095226287842\n",
      "epoch 1 batch 208 loss: 3.1664769649505615\n",
      "epoch 1 batch 209 loss: 2.7832109928131104\n",
      "epoch 1 batch 210 loss: 2.9894046783447266\n",
      "epoch 1 batch 211 loss: 2.823554515838623\n",
      "epoch 1 batch 212 loss: 3.228954315185547\n",
      "epoch 1 batch 213 loss: 2.986591339111328\n",
      "epoch 1 batch 214 loss: 2.8818955421447754\n",
      "epoch 1 batch 215 loss: 2.8737666606903076\n",
      "epoch 1 batch 216 loss: 2.7214393615722656\n",
      "epoch 1 batch 217 loss: 3.485591411590576\n",
      "epoch 1 batch 218 loss: 3.088665008544922\n",
      "epoch 1 batch 219 loss: 2.9103925228118896\n",
      "epoch 1 batch 220 loss: 3.1028048992156982\n",
      "epoch 1 batch 221 loss: 3.013211250305176\n",
      "epoch 1 batch 222 loss: 2.796520948410034\n",
      "epoch 1 batch 223 loss: 2.9225170612335205\n",
      "epoch 1 batch 224 loss: 3.2564022541046143\n",
      "epoch 1 batch 225 loss: 2.8768978118896484\n",
      "epoch 1 batch 226 loss: 3.109445333480835\n",
      "epoch 1 batch 227 loss: 3.3088650703430176\n",
      "epoch 1 batch 228 loss: 2.8317208290100098\n",
      "epoch 1 batch 229 loss: 2.787053346633911\n",
      "epoch 1 batch 230 loss: 3.1829962730407715\n",
      "epoch 1 batch 231 loss: 2.6550955772399902\n",
      "epoch 1 batch 232 loss: 2.939300298690796\n",
      "epoch 1 batch 233 loss: 3.297668933868408\n",
      "epoch 1 batch 234 loss: 3.349809169769287\n",
      "epoch 1 batch 235 loss: 2.650696039199829\n",
      "epoch 1 batch 236 loss: 3.005871534347534\n",
      "epoch 1 batch 237 loss: 3.2649788856506348\n",
      "epoch 1 batch 238 loss: 2.6336655616760254\n",
      "epoch 1 batch 239 loss: 2.810823440551758\n",
      "epoch 1 batch 240 loss: 3.2057571411132812\n",
      "epoch 1 batch 241 loss: 3.371704339981079\n",
      "epoch 1 batch 242 loss: 3.170750617980957\n",
      "epoch 1 batch 243 loss: 3.186234712600708\n",
      "epoch 1 batch 244 loss: 3.0263919830322266\n",
      "epoch 1 batch 245 loss: 3.4297358989715576\n",
      "epoch 1 batch 246 loss: 2.9806857109069824\n",
      "epoch 1 batch 247 loss: 3.038527488708496\n",
      "epoch 1 batch 248 loss: 3.2533998489379883\n",
      "epoch 1 batch 249 loss: 3.549644947052002\n",
      "epoch 1 batch 250 loss: 2.9175620079040527\n",
      "epoch 1 batch 251 loss: 3.2967889308929443\n",
      "epoch 1 batch 252 loss: 3.153289794921875\n",
      "epoch 1 batch 253 loss: 2.611607313156128\n",
      "epoch 1 batch 254 loss: 3.231762170791626\n",
      "epoch 1 batch 255 loss: 2.8791465759277344\n",
      "epoch 1 batch 256 loss: 3.492615222930908\n",
      "epoch 1 batch 257 loss: 2.8840222358703613\n",
      "epoch 1 batch 258 loss: 3.1516315937042236\n",
      "epoch 1 batch 259 loss: 2.886019706726074\n",
      "epoch 1 batch 260 loss: 3.11303973197937\n",
      "epoch 1 batch 261 loss: 2.8633294105529785\n",
      "epoch 1 batch 262 loss: 2.870290517807007\n",
      "epoch 1 batch 263 loss: 3.4935669898986816\n",
      "epoch 1 batch 264 loss: 2.981597900390625\n",
      "epoch 1 batch 265 loss: 2.62734317779541\n",
      "epoch 1 batch 266 loss: 3.0972542762756348\n",
      "epoch 1 batch 267 loss: 3.1088368892669678\n",
      "epoch 1 batch 268 loss: 3.618436574935913\n",
      "epoch 1 batch 269 loss: 2.8404064178466797\n",
      "epoch 1 batch 270 loss: 3.5105996131896973\n",
      "epoch 1 batch 271 loss: 3.1435329914093018\n",
      "epoch 1 batch 272 loss: 3.2932872772216797\n",
      "epoch 1 batch 273 loss: 2.955273151397705\n",
      "epoch 1 batch 274 loss: 3.153964042663574\n",
      "epoch 1 batch 275 loss: 3.42240309715271\n",
      "epoch 1 batch 276 loss: 3.060321807861328\n",
      "epoch 1 batch 277 loss: 2.774333953857422\n",
      "epoch 1 batch 278 loss: 2.974693775177002\n",
      "epoch 1 batch 279 loss: 3.179657459259033\n",
      "epoch 1 batch 280 loss: 3.363766670227051\n",
      "epoch 1 batch 281 loss: 3.383485794067383\n",
      "epoch 1 batch 282 loss: 3.0156731605529785\n",
      "epoch 1 batch 283 loss: 3.0422720909118652\n",
      "epoch 1 batch 284 loss: 3.2095677852630615\n",
      "epoch 1 batch 285 loss: 3.1225943565368652\n",
      "epoch 1 batch 286 loss: 2.9546616077423096\n",
      "epoch 1 batch 287 loss: 3.107961654663086\n",
      "epoch 1 batch 288 loss: 3.440514087677002\n",
      "epoch 1 batch 289 loss: 2.8682777881622314\n",
      "epoch 1 batch 290 loss: 3.0532779693603516\n",
      "epoch 1 batch 291 loss: 3.181074619293213\n",
      "epoch 1 batch 292 loss: 3.1518096923828125\n",
      "epoch 1 batch 293 loss: 2.954458236694336\n",
      "epoch 1 batch 294 loss: 2.831778049468994\n",
      "epoch 1 batch 295 loss: 2.604663848876953\n",
      "epoch 1 batch 296 loss: 3.0888447761535645\n",
      "epoch 1 batch 297 loss: 3.095202922821045\n",
      "epoch 1 batch 298 loss: 2.7307443618774414\n",
      "epoch 1 batch 299 loss: 3.0494024753570557\n",
      "epoch 1 batch 300 loss: 2.743082046508789\n",
      "epoch 1 batch 301 loss: 3.1030163764953613\n",
      "epoch 1 batch 302 loss: 3.32828950881958\n",
      "epoch 1 batch 303 loss: 2.8368382453918457\n",
      "epoch 1 batch 304 loss: 2.7091894149780273\n",
      "epoch 1 batch 305 loss: 3.407464027404785\n",
      "epoch 1 batch 306 loss: 2.628784656524658\n",
      "epoch 1 batch 307 loss: 3.2723159790039062\n",
      "epoch 1 batch 308 loss: 2.734748363494873\n",
      "epoch 1 batch 309 loss: 2.9228172302246094\n",
      "epoch 1 batch 310 loss: 2.691944122314453\n",
      "epoch 1 batch 311 loss: 2.747440814971924\n",
      "epoch 1 batch 312 loss: 2.781789541244507\n",
      "epoch 1 batch 313 loss: 2.9673361778259277\n",
      "epoch 1 batch 314 loss: 3.1679604053497314\n",
      "epoch 1 batch 315 loss: 2.8981189727783203\n",
      "epoch 1 batch 316 loss: 3.0296101570129395\n",
      "epoch 1 batch 317 loss: 3.3000292778015137\n",
      "epoch 1 batch 318 loss: 2.6737990379333496\n",
      "epoch 1 batch 319 loss: 3.0165295600891113\n",
      "epoch 1 batch 320 loss: 2.9897677898406982\n",
      "epoch 1 batch 321 loss: 3.282158374786377\n",
      "epoch 1 batch 322 loss: 2.8744568824768066\n",
      "epoch 1 batch 323 loss: 2.6418745517730713\n",
      "epoch 1 batch 324 loss: 2.890901565551758\n",
      "epoch 1 batch 325 loss: 2.9646072387695312\n",
      "epoch 1 batch 326 loss: 2.985640048980713\n",
      "epoch 1 batch 327 loss: 3.071558713912964\n",
      "epoch 1 batch 328 loss: 2.858252763748169\n",
      "epoch 1 batch 329 loss: 2.82328462600708\n",
      "epoch 1 batch 330 loss: 2.94242525100708\n",
      "epoch 1 batch 331 loss: 3.043966770172119\n",
      "epoch 1 batch 332 loss: 3.186021089553833\n",
      "epoch 1 batch 333 loss: 3.426292657852173\n",
      "epoch 1 batch 334 loss: 3.285889148712158\n",
      "epoch 1 batch 335 loss: 2.904170036315918\n",
      "epoch 1 batch 336 loss: 3.3075366020202637\n",
      "epoch 1 batch 337 loss: 2.9157214164733887\n",
      "epoch 1 batch 338 loss: 3.115772247314453\n",
      "epoch 1 batch 339 loss: 3.346506357192993\n",
      "epoch 1 batch 340 loss: 2.973851203918457\n",
      "epoch 1 batch 341 loss: 2.937852621078491\n",
      "epoch 1 batch 342 loss: 2.7797820568084717\n",
      "epoch 1 batch 343 loss: 3.2385215759277344\n",
      "epoch 1 batch 344 loss: 3.047302722930908\n",
      "epoch 1 batch 345 loss: 3.5064949989318848\n",
      "epoch 1 batch 346 loss: 2.8206491470336914\n",
      "epoch 1 batch 347 loss: 3.0560965538024902\n",
      "epoch 1 batch 348 loss: 2.6837658882141113\n",
      "epoch 1 batch 349 loss: 3.221292495727539\n",
      "epoch 1 batch 350 loss: 2.8750782012939453\n",
      "epoch 1 batch 351 loss: 3.0989370346069336\n",
      "epoch 1 batch 352 loss: 3.227830410003662\n",
      "epoch 1 batch 353 loss: 3.0493807792663574\n",
      "epoch 1 batch 354 loss: 2.9483659267425537\n",
      "epoch 1 batch 355 loss: 2.838385581970215\n",
      "epoch 1 batch 356 loss: 3.024047613143921\n",
      "epoch 1 batch 357 loss: 3.066270351409912\n",
      "epoch 1 batch 358 loss: 2.780061721801758\n",
      "epoch 1 batch 359 loss: 3.177790641784668\n",
      "epoch 1 batch 360 loss: 3.339073657989502\n",
      "epoch 1 batch 361 loss: 2.739804744720459\n",
      "epoch 1 batch 362 loss: 2.6859569549560547\n",
      "epoch 1 batch 363 loss: 3.33274507522583\n",
      "epoch 1 batch 364 loss: 3.1099801063537598\n",
      "epoch 1 batch 365 loss: 2.7702441215515137\n",
      "epoch 1 batch 366 loss: 3.3061752319335938\n",
      "epoch 1 batch 367 loss: 3.3153462409973145\n",
      "epoch 1 batch 368 loss: 2.8799188137054443\n",
      "epoch 1 batch 369 loss: 3.0431010723114014\n",
      "epoch 1 batch 370 loss: 3.0199637413024902\n",
      "epoch 1 batch 371 loss: 2.7663326263427734\n",
      "epoch 1 batch 372 loss: 3.4669013023376465\n",
      "epoch 1 batch 373 loss: 2.8976612091064453\n",
      "epoch 1 batch 374 loss: 2.757810115814209\n",
      "epoch 1 batch 375 loss: 3.221798896789551\n",
      "epoch 1 batch 376 loss: 2.89036226272583\n",
      "epoch 1 batch 377 loss: 3.0004148483276367\n",
      "epoch 1 batch 378 loss: 3.3635668754577637\n",
      "epoch 1 batch 379 loss: 2.808953285217285\n",
      "epoch 1 batch 380 loss: 3.3931379318237305\n",
      "epoch 1 batch 381 loss: 2.79905366897583\n",
      "epoch 1 batch 382 loss: 3.1585850715637207\n",
      "epoch 1 batch 383 loss: 2.73539662361145\n",
      "epoch 1 batch 384 loss: 2.932255744934082\n",
      "epoch 1 batch 385 loss: 3.327031373977661\n",
      "epoch 1 batch 386 loss: 3.0733096599578857\n",
      "epoch 1 batch 387 loss: 2.8547048568725586\n",
      "epoch 1 batch 388 loss: 3.269562005996704\n",
      "epoch 1 batch 389 loss: 2.6195125579833984\n",
      "epoch 1 batch 390 loss: 2.944437026977539\n",
      "epoch 1 batch 391 loss: 3.2322068214416504\n",
      "epoch 1 batch 392 loss: 3.076930522918701\n",
      "epoch 1 batch 393 loss: 2.7795236110687256\n",
      "epoch 1 batch 394 loss: 3.1519322395324707\n",
      "epoch 1 batch 395 loss: 3.2204031944274902\n",
      "epoch 1 batch 396 loss: 3.180997610092163\n",
      "epoch 1 batch 397 loss: 3.024740219116211\n",
      "epoch 1 batch 398 loss: 2.9313440322875977\n",
      "epoch 1 batch 399 loss: 3.042107105255127\n",
      "epoch 1 batch 400 loss: 3.0701394081115723\n",
      "epoch 1 batch 401 loss: 3.0769169330596924\n",
      "epoch 1 batch 402 loss: 2.893360137939453\n",
      "epoch 1 batch 403 loss: 2.6655702590942383\n",
      "epoch 1 batch 404 loss: 2.7010912895202637\n",
      "epoch 1 batch 405 loss: 2.780322313308716\n",
      "epoch 1 batch 406 loss: 2.8229095935821533\n",
      "epoch 1 batch 407 loss: 3.3716094493865967\n",
      "epoch 1 batch 408 loss: 3.325897455215454\n",
      "epoch 1 batch 409 loss: 2.9561686515808105\n",
      "epoch 1 batch 410 loss: 3.0131683349609375\n",
      "epoch 1 batch 411 loss: 3.161550283432007\n",
      "epoch 1 batch 412 loss: 3.037853717803955\n",
      "epoch 1 batch 413 loss: 2.7538046836853027\n",
      "epoch 1 batch 414 loss: 3.1261215209960938\n",
      "epoch 1 batch 415 loss: 2.645171880722046\n",
      "epoch 1 batch 416 loss: 3.110948085784912\n",
      "epoch 1 batch 417 loss: 2.9853854179382324\n",
      "epoch 1 batch 418 loss: 2.8545587062835693\n",
      "epoch 1 batch 419 loss: 2.8205361366271973\n",
      "epoch 1 batch 420 loss: 2.6968746185302734\n",
      "epoch 1 batch 421 loss: 2.9353561401367188\n",
      "epoch 1 batch 422 loss: 3.05122971534729\n",
      "epoch 1 batch 423 loss: 2.9298667907714844\n",
      "epoch 1 batch 424 loss: 2.7795019149780273\n",
      "epoch 1 batch 425 loss: 2.8102481365203857\n",
      "epoch 1 batch 426 loss: 2.687727451324463\n",
      "epoch 1 batch 427 loss: 2.6904892921447754\n",
      "epoch 1 batch 428 loss: 3.108377456665039\n",
      "epoch 1 batch 429 loss: 3.2553939819335938\n",
      "epoch 1 batch 430 loss: 2.4281601905822754\n",
      "epoch 1 batch 431 loss: 3.1842904090881348\n",
      "epoch 1 batch 432 loss: 3.1255855560302734\n",
      "epoch 1 batch 433 loss: 3.144404649734497\n",
      "epoch 1 batch 434 loss: 2.8330063819885254\n",
      "epoch 1 batch 435 loss: 3.3017349243164062\n",
      "epoch 1 batch 436 loss: 2.642580509185791\n",
      "epoch 1 batch 437 loss: 2.788968086242676\n",
      "epoch 1 batch 438 loss: 3.1763525009155273\n",
      "epoch 1 batch 439 loss: 2.5131897926330566\n",
      "epoch 1 batch 440 loss: 2.9022908210754395\n",
      "epoch 1 batch 441 loss: 2.9161529541015625\n",
      "epoch 1 batch 442 loss: 2.8260865211486816\n",
      "epoch 1 batch 443 loss: 2.842801094055176\n",
      "epoch 1 batch 444 loss: 2.899413585662842\n",
      "epoch 1 batch 445 loss: 3.1588940620422363\n",
      "epoch 1 batch 446 loss: 3.2276580333709717\n",
      "epoch 1 batch 447 loss: 2.4982547760009766\n",
      "epoch 1 batch 448 loss: 2.746609926223755\n",
      "epoch 1 batch 449 loss: 2.8937647342681885\n",
      "epoch 1 batch 450 loss: 3.399322986602783\n",
      "epoch 1 batch 451 loss: 2.7898871898651123\n",
      "epoch 1 batch 452 loss: 3.222174644470215\n",
      "epoch 1 batch 453 loss: 3.392007827758789\n",
      "epoch 1 batch 454 loss: 3.0295567512512207\n",
      "epoch 1 batch 455 loss: 2.8463425636291504\n",
      "epoch 1 batch 456 loss: 3.196732521057129\n",
      "epoch 1 batch 457 loss: 3.2712979316711426\n",
      "epoch 1 batch 458 loss: 2.9834060668945312\n",
      "epoch 1 batch 459 loss: 3.2561240196228027\n",
      "epoch 1 batch 460 loss: 3.1345200538635254\n",
      "epoch 1 batch 461 loss: 3.358396530151367\n",
      "epoch 1 batch 462 loss: 2.751386880874634\n",
      "epoch 1 batch 463 loss: 2.9905953407287598\n",
      "epoch 1 batch 464 loss: 3.1433122158050537\n",
      "epoch 1 batch 465 loss: 2.7488932609558105\n",
      "epoch 1 batch 466 loss: 2.8528945446014404\n",
      "epoch 1 batch 467 loss: 3.0137293338775635\n",
      "epoch 1 batch 468 loss: 2.903116226196289\n",
      "epoch 1 batch 469 loss: 2.6905722618103027\n",
      "epoch 1 batch 470 loss: 2.9684815406799316\n",
      "epoch 1 batch 471 loss: 3.350656509399414\n",
      "epoch 1 batch 472 loss: 2.4361870288848877\n",
      "epoch 1 batch 473 loss: 3.0161044597625732\n",
      "epoch 1 batch 474 loss: 3.0172643661499023\n",
      "epoch 1 batch 475 loss: 2.809504508972168\n",
      "epoch 1 batch 476 loss: 2.649949312210083\n",
      "epoch 1 batch 477 loss: 3.307286262512207\n",
      "epoch 1 batch 478 loss: 3.031623125076294\n",
      "epoch 1 batch 479 loss: 2.803933620452881\n",
      "epoch 1 batch 480 loss: 3.239807605743408\n",
      "epoch 1 batch 481 loss: 2.71980357170105\n",
      "epoch 1 batch 482 loss: 3.375814914703369\n",
      "epoch 1 batch 483 loss: 3.3584938049316406\n",
      "epoch 1 batch 484 loss: 2.883300304412842\n",
      "epoch 1 batch 485 loss: 2.9453139305114746\n",
      "epoch 1 batch 486 loss: 3.3569045066833496\n",
      "epoch 1 batch 487 loss: 3.2476744651794434\n",
      "epoch 1 batch 488 loss: 2.9687023162841797\n",
      "epoch 1 batch 489 loss: 2.783698081970215\n",
      "epoch 1 batch 490 loss: 2.5922014713287354\n",
      "epoch 1 batch 491 loss: 2.950432777404785\n",
      "epoch 1 batch 492 loss: 2.8080034255981445\n",
      "epoch 1 batch 493 loss: 3.343620777130127\n",
      "epoch 1 batch 494 loss: 2.9972305297851562\n",
      "epoch 1 batch 495 loss: 2.7790729999542236\n",
      "epoch 1 batch 496 loss: 2.6375467777252197\n",
      "epoch 1 batch 497 loss: 2.769195556640625\n",
      "epoch 1 batch 498 loss: 2.9240081310272217\n",
      "epoch 1 batch 499 loss: 2.9930896759033203\n",
      "epoch 1 batch 500 loss: 2.876253128051758\n",
      "epoch 1 batch 501 loss: 2.8501393795013428\n",
      "epoch 1 batch 502 loss: 2.9826793670654297\n",
      "epoch 1 batch 503 loss: 2.663532257080078\n",
      "epoch 1 batch 504 loss: 3.6752679347991943\n",
      "epoch 1 batch 505 loss: 3.0278406143188477\n",
      "epoch 1 batch 506 loss: 3.1850266456604004\n",
      "epoch 1 batch 507 loss: 3.025634288787842\n",
      "epoch 1 batch 508 loss: 3.327636957168579\n",
      "epoch 1 batch 509 loss: 2.9088077545166016\n",
      "epoch 1 batch 510 loss: 3.0862154960632324\n",
      "epoch 1 batch 511 loss: 3.1290478706359863\n",
      "epoch 1 batch 512 loss: 3.2822775840759277\n",
      "epoch 1 batch 513 loss: 2.5653393268585205\n",
      "epoch 1 batch 514 loss: 2.84796404838562\n",
      "epoch 1 batch 515 loss: 2.9526400566101074\n",
      "epoch 1 batch 516 loss: 2.855660915374756\n",
      "epoch 1 batch 517 loss: 2.7797598838806152\n",
      "epoch 1 batch 518 loss: 2.7123477458953857\n",
      "epoch 1 batch 519 loss: 2.907043933868408\n",
      "epoch 1 batch 520 loss: 3.111514091491699\n",
      "epoch 1 batch 521 loss: 2.910147190093994\n",
      "epoch 1 batch 522 loss: 3.10496187210083\n",
      "epoch 1 batch 523 loss: 3.127777576446533\n",
      "epoch 1 batch 524 loss: 3.01613187789917\n",
      "epoch 1 batch 525 loss: 3.260935068130493\n",
      "epoch 1 batch 526 loss: 3.764174461364746\n",
      "epoch 1 batch 527 loss: 3.2969093322753906\n",
      "epoch 1 batch 528 loss: 2.880923271179199\n",
      "epoch 1 batch 529 loss: 3.600105047225952\n",
      "epoch 1 batch 530 loss: 2.8247594833374023\n",
      "epoch 1 batch 531 loss: 3.0541255474090576\n",
      "epoch 1 batch 532 loss: 2.9009571075439453\n",
      "epoch 1 batch 533 loss: 2.8024628162384033\n",
      "epoch 1 batch 534 loss: 2.9721851348876953\n",
      "epoch 1 batch 535 loss: 2.712428569793701\n",
      "epoch 1 batch 536 loss: 2.932276964187622\n",
      "epoch 1 batch 537 loss: 2.878434896469116\n",
      "epoch 1 batch 538 loss: 3.5109150409698486\n",
      "epoch 1 batch 539 loss: 2.802781105041504\n",
      "epoch 1 batch 540 loss: 3.008934736251831\n",
      "epoch 1 batch 541 loss: 3.1860125064849854\n",
      "epoch 1 batch 542 loss: 2.842376232147217\n",
      "epoch 1 batch 543 loss: 2.976289987564087\n",
      "epoch 1 batch 544 loss: 3.1133646965026855\n",
      "epoch 1 batch 545 loss: 3.0605783462524414\n",
      "epoch 1 batch 546 loss: 3.3508243560791016\n",
      "epoch 1 batch 547 loss: 2.5164060592651367\n",
      "epoch 1 batch 548 loss: 2.7437424659729004\n",
      "epoch 1 batch 549 loss: 3.0883915424346924\n",
      "epoch 1 batch 550 loss: 2.943214178085327\n",
      "epoch 1 batch 551 loss: 2.9072299003601074\n",
      "epoch 1 batch 552 loss: 2.965937852859497\n",
      "epoch 1 batch 553 loss: 2.989663600921631\n",
      "epoch 1 batch 554 loss: 3.435521364212036\n",
      "epoch 1 batch 555 loss: 2.9657559394836426\n",
      "epoch 1 batch 556 loss: 3.0368809700012207\n",
      "epoch 1 batch 557 loss: 3.3452389240264893\n",
      "epoch 1 batch 558 loss: 3.025904417037964\n",
      "epoch 1 batch 559 loss: 3.0594143867492676\n",
      "epoch 1 batch 560 loss: 2.8680944442749023\n",
      "epoch 1 batch 561 loss: 2.8246536254882812\n",
      "epoch 1 batch 562 loss: 2.794271230697632\n",
      "epoch 1 batch 563 loss: 3.070699691772461\n",
      "epoch 1 batch 564 loss: 2.7680912017822266\n",
      "epoch 1 batch 565 loss: 3.380096912384033\n",
      "epoch 1 batch 566 loss: 2.8211193084716797\n",
      "epoch 1 batch 567 loss: 3.0791449546813965\n",
      "epoch 1 batch 568 loss: 3.216083288192749\n",
      "epoch 1 batch 569 loss: 3.2270474433898926\n",
      "epoch 1 batch 570 loss: 3.104342222213745\n",
      "epoch 1 batch 571 loss: 3.178281307220459\n",
      "epoch 1 batch 572 loss: 3.491421699523926\n",
      "epoch 1 batch 573 loss: 3.09356427192688\n",
      "epoch 1 batch 574 loss: 3.0634913444519043\n",
      "epoch 1 batch 575 loss: 2.971717119216919\n",
      "epoch 1 batch 576 loss: 2.794107437133789\n",
      "epoch 1 batch 577 loss: 2.7624268531799316\n",
      "epoch 1 batch 578 loss: 2.844714641571045\n",
      "epoch 1 batch 579 loss: 2.852285861968994\n",
      "epoch 1 batch 580 loss: 3.680891990661621\n",
      "epoch 1 batch 581 loss: 3.028618812561035\n",
      "epoch 1 batch 582 loss: 3.8126888275146484\n",
      "epoch 1 batch 583 loss: 3.2293996810913086\n",
      "epoch 1 batch 584 loss: 2.9143569469451904\n",
      "epoch 1 batch 585 loss: 3.489476442337036\n",
      "epoch 1 batch 586 loss: 3.4642930030822754\n",
      "epoch 1 batch 587 loss: 3.0826499462127686\n",
      "epoch 1 batch 588 loss: 3.0428452491760254\n",
      "epoch 1 batch 589 loss: 3.474377155303955\n",
      "epoch 1 batch 590 loss: 3.1269960403442383\n",
      "epoch 1 batch 591 loss: 3.115450859069824\n",
      "epoch 1 batch 592 loss: 3.0321102142333984\n",
      "epoch 1 batch 593 loss: 3.472445487976074\n",
      "epoch 1 batch 594 loss: 2.8214874267578125\n",
      "epoch 1 batch 595 loss: 3.3329386711120605\n",
      "epoch 1 batch 596 loss: 3.120839834213257\n",
      "epoch 1 batch 597 loss: 2.937615156173706\n",
      "epoch 1 batch 598 loss: 2.9471583366394043\n",
      "epoch 1 batch 599 loss: 2.9192748069763184\n",
      "epoch 1 batch 600 loss: 2.959378719329834\n",
      "epoch 1 batch 601 loss: 3.0175039768218994\n",
      "epoch 1 batch 602 loss: 2.825772762298584\n",
      "epoch 1 batch 603 loss: 3.003676414489746\n",
      "epoch 1 batch 604 loss: 3.3988451957702637\n",
      "epoch 1 batch 605 loss: 3.191209554672241\n",
      "epoch 1 batch 606 loss: 3.2223944664001465\n",
      "epoch 1 batch 607 loss: 3.0174331665039062\n",
      "epoch 1 batch 608 loss: 3.35772442817688\n",
      "epoch 1 batch 609 loss: 3.1746082305908203\n",
      "epoch 1 batch 610 loss: 2.676297664642334\n",
      "epoch 1 batch 611 loss: 3.1581552028656006\n",
      "epoch 1 batch 612 loss: 2.670637845993042\n",
      "epoch 1 batch 613 loss: 2.963705062866211\n",
      "epoch 1 batch 614 loss: 2.903761625289917\n",
      "epoch 1 batch 615 loss: 3.2449421882629395\n",
      "epoch 1 batch 616 loss: 3.1686601638793945\n",
      "epoch 1 batch 617 loss: 2.9982566833496094\n",
      "epoch 1 batch 618 loss: 3.021789073944092\n",
      "epoch 1 batch 619 loss: 2.78212833404541\n",
      "epoch 1 batch 620 loss: 3.0660793781280518\n",
      "epoch 1 batch 621 loss: 3.261369466781616\n",
      "epoch 1 batch 622 loss: 3.074099540710449\n",
      "epoch 1 batch 623 loss: 3.198035478591919\n",
      "epoch 1 batch 624 loss: 2.602306365966797\n",
      "epoch 1 batch 625 loss: 3.0236868858337402\n",
      "epoch 1 batch 626 loss: 3.147298812866211\n",
      "epoch 1 batch 627 loss: 2.980008125305176\n",
      "epoch 1 batch 628 loss: 2.9097981452941895\n",
      "epoch 1 batch 629 loss: 3.0695865154266357\n",
      "epoch 1 batch 630 loss: 3.0752639770507812\n",
      "epoch 1 batch 631 loss: 2.571000099182129\n",
      "epoch 1 batch 632 loss: 3.4466357231140137\n",
      "epoch 1 batch 633 loss: 3.0049400329589844\n",
      "epoch 1 batch 634 loss: 2.821337938308716\n",
      "epoch 1 batch 635 loss: 3.1286048889160156\n",
      "epoch 1 batch 636 loss: 3.332754135131836\n",
      "epoch 1 batch 637 loss: 2.966275930404663\n",
      "epoch 1 batch 638 loss: 2.7304604053497314\n",
      "epoch 1 batch 639 loss: 2.8458569049835205\n",
      "epoch 1 batch 640 loss: 3.167778968811035\n",
      "epoch 1 batch 641 loss: 2.805814743041992\n",
      "epoch 1 batch 642 loss: 2.773560047149658\n",
      "epoch 1 batch 643 loss: 2.9822535514831543\n",
      "epoch 1 batch 644 loss: 3.2880592346191406\n",
      "epoch 1 batch 645 loss: 2.568932294845581\n",
      "epoch 1 batch 646 loss: 2.79426908493042\n",
      "epoch 1 batch 647 loss: 2.99576735496521\n",
      "epoch 1 batch 648 loss: 2.7029271125793457\n",
      "epoch 1 batch 649 loss: 3.034588575363159\n",
      "epoch 1 batch 650 loss: 2.901184558868408\n",
      "epoch 1 batch 651 loss: 3.211909770965576\n",
      "epoch 1 batch 652 loss: 2.877480983734131\n",
      "epoch 1 batch 653 loss: 2.5675888061523438\n",
      "epoch 1 batch 654 loss: 3.1353912353515625\n",
      "epoch 1 batch 655 loss: 3.296727180480957\n",
      "epoch 1 batch 656 loss: 2.7136173248291016\n",
      "epoch 1 batch 657 loss: 3.2253341674804688\n",
      "epoch 1 batch 658 loss: 2.640768527984619\n",
      "epoch 1 batch 659 loss: 2.888127088546753\n",
      "epoch 1 batch 660 loss: 3.100905656814575\n",
      "epoch 1 batch 661 loss: 2.78454327583313\n",
      "epoch 1 batch 662 loss: 2.956763744354248\n",
      "epoch 1 batch 663 loss: 2.9098756313323975\n",
      "epoch 1 batch 664 loss: 2.959256172180176\n",
      "epoch 1 batch 665 loss: 3.23974347114563\n",
      "epoch 1 batch 666 loss: 2.5818498134613037\n",
      "epoch 1 batch 667 loss: 2.7412967681884766\n",
      "epoch 1 batch 668 loss: 2.8577561378479004\n",
      "epoch 1 batch 669 loss: 3.037179946899414\n",
      "epoch 1 batch 670 loss: 3.0054430961608887\n",
      "epoch 1 batch 671 loss: 3.0638651847839355\n",
      "epoch 1 batch 672 loss: 3.3847854137420654\n",
      "epoch 1 batch 673 loss: 3.0702226161956787\n",
      "epoch 1 batch 674 loss: 2.6592631340026855\n",
      "epoch 1 batch 675 loss: 3.2003655433654785\n",
      "epoch 1 batch 676 loss: 3.099545955657959\n",
      "epoch 1 batch 677 loss: 2.7491464614868164\n",
      "epoch 1 batch 678 loss: 2.7856411933898926\n",
      "epoch 1 batch 679 loss: 3.152076244354248\n",
      "epoch 1 batch 680 loss: 2.7975101470947266\n",
      "epoch 1 batch 681 loss: 2.9342474937438965\n",
      "epoch 1 batch 682 loss: 3.4593045711517334\n",
      "epoch 1 batch 683 loss: 2.846156358718872\n",
      "epoch 1 batch 684 loss: 3.1036224365234375\n",
      "epoch 1 batch 685 loss: 2.920926570892334\n",
      "epoch 1 batch 686 loss: 2.9999215602874756\n",
      "epoch 1 batch 687 loss: 2.8565874099731445\n",
      "epoch 1 batch 688 loss: 3.259223461151123\n",
      "epoch 1 batch 689 loss: 2.6872897148132324\n",
      "epoch 1 batch 690 loss: 2.873398542404175\n",
      "epoch 1 batch 691 loss: 2.6410584449768066\n",
      "epoch 1 batch 692 loss: 3.071028232574463\n",
      "epoch 1 batch 693 loss: 3.0294156074523926\n",
      "epoch 1 batch 694 loss: 2.823539972305298\n",
      "epoch 1 batch 695 loss: 2.7205348014831543\n",
      "epoch 1 batch 696 loss: 3.225193977355957\n",
      "epoch 1 batch 697 loss: 2.644270181655884\n",
      "epoch 1 batch 698 loss: 2.651556968688965\n",
      "epoch 1 batch 699 loss: 3.4620778560638428\n",
      "epoch 1 batch 700 loss: 2.7001800537109375\n",
      "epoch 1 batch 701 loss: 3.0235230922698975\n",
      "epoch 1 batch 702 loss: 2.5177102088928223\n",
      "epoch 1 batch 703 loss: 3.0327272415161133\n",
      "epoch 1 batch 704 loss: 3.36698579788208\n",
      "epoch 1 batch 705 loss: 3.6436610221862793\n",
      "epoch 1 batch 706 loss: 3.2496628761291504\n",
      "epoch 1 batch 707 loss: 2.6530849933624268\n",
      "epoch 1 batch 708 loss: 2.7941253185272217\n",
      "epoch 1 batch 709 loss: 3.2388687133789062\n",
      "epoch 1 batch 710 loss: 2.9388203620910645\n",
      "epoch 1 batch 711 loss: 3.2649669647216797\n",
      "epoch 1 batch 712 loss: 3.3417930603027344\n",
      "epoch 1 batch 713 loss: 2.7780909538269043\n",
      "epoch 1 batch 714 loss: 2.985896587371826\n",
      "epoch 1 batch 715 loss: 2.870279312133789\n",
      "epoch 1 batch 716 loss: 3.002987861633301\n",
      "epoch 1 batch 717 loss: 3.022815704345703\n",
      "epoch 1 batch 718 loss: 3.1203773021698\n",
      "epoch 1 batch 719 loss: 2.991852283477783\n",
      "epoch 1 batch 720 loss: 3.146472930908203\n",
      "epoch 1 batch 721 loss: 2.7806618213653564\n",
      "epoch 1 batch 722 loss: 2.6441352367401123\n",
      "epoch 1 batch 723 loss: 2.515017509460449\n",
      "epoch 1 batch 724 loss: 3.2051124572753906\n",
      "epoch 1 batch 725 loss: 3.198946952819824\n",
      "epoch 1 batch 726 loss: 2.7586309909820557\n",
      "epoch 1 batch 727 loss: 3.254948616027832\n",
      "epoch 1 batch 728 loss: 2.660600185394287\n",
      "epoch 1 batch 729 loss: 3.0226168632507324\n",
      "epoch 1 batch 730 loss: 3.0302109718322754\n",
      "epoch 1 batch 731 loss: 3.2417831420898438\n",
      "epoch 1 batch 732 loss: 3.3230819702148438\n",
      "epoch 1 batch 733 loss: 3.2696497440338135\n",
      "epoch 1 batch 734 loss: 3.3073713779449463\n",
      "epoch 1 batch 735 loss: 2.7523648738861084\n",
      "epoch 1 batch 736 loss: 3.134749412536621\n",
      "epoch 1 batch 737 loss: 3.0549111366271973\n",
      "epoch 1 batch 738 loss: 3.1077466011047363\n",
      "epoch 1 batch 739 loss: 2.9409546852111816\n",
      "epoch 1 batch 740 loss: 2.991804838180542\n",
      "epoch 1 batch 741 loss: 2.8860692977905273\n",
      "epoch 1 batch 742 loss: 3.0921378135681152\n",
      "epoch 1 batch 743 loss: 2.7647502422332764\n",
      "epoch 1 batch 744 loss: 2.9757673740386963\n",
      "epoch 1 batch 745 loss: 3.0022969245910645\n",
      "epoch 1 batch 746 loss: 2.775502920150757\n",
      "epoch 1 batch 747 loss: 3.383729934692383\n",
      "epoch 1 batch 748 loss: 3.198634147644043\n",
      "epoch 1 batch 749 loss: 2.6272711753845215\n",
      "epoch 1 batch 750 loss: 3.0869364738464355\n",
      "epoch 1 batch 751 loss: 2.933353900909424\n",
      "epoch 1 batch 752 loss: 2.6969079971313477\n",
      "epoch 1 batch 753 loss: 2.815153121948242\n",
      "epoch 1 batch 754 loss: 3.3089547157287598\n",
      "epoch 1 batch 755 loss: 2.7380857467651367\n",
      "epoch 1 batch 756 loss: 2.8938636779785156\n",
      "epoch 1 batch 757 loss: 3.224440574645996\n",
      "epoch 1 batch 758 loss: 3.068171977996826\n",
      "epoch 1 batch 759 loss: 3.1993255615234375\n",
      "epoch 1 batch 760 loss: 3.1467549800872803\n",
      "epoch 1 batch 761 loss: 3.0684354305267334\n",
      "epoch 1 batch 762 loss: 2.8932619094848633\n",
      "epoch 1 batch 763 loss: 3.334773540496826\n",
      "epoch 1 batch 764 loss: 2.9054348468780518\n",
      "epoch 1 batch 765 loss: 3.012004852294922\n",
      "epoch 1 batch 766 loss: 2.6937193870544434\n",
      "epoch 1 batch 767 loss: 2.9121203422546387\n",
      "epoch 1 batch 768 loss: 2.6725311279296875\n",
      "epoch 1 batch 769 loss: 3.06732177734375\n",
      "epoch 1 batch 770 loss: 3.001441717147827\n",
      "epoch 1 batch 771 loss: 3.3007731437683105\n",
      "epoch 1 batch 772 loss: 3.33729887008667\n",
      "epoch 1 batch 773 loss: 3.0721352100372314\n",
      "epoch 1 batch 774 loss: 3.0396041870117188\n",
      "epoch 1 batch 775 loss: 3.1445131301879883\n",
      "epoch 1 batch 776 loss: 3.303133964538574\n",
      "epoch 1 batch 777 loss: 2.8806047439575195\n",
      "epoch 1 batch 778 loss: 3.3182156085968018\n",
      "epoch 1 batch 779 loss: 2.8707022666931152\n",
      "epoch 1 batch 780 loss: 2.990612506866455\n",
      "epoch 1 batch 781 loss: 2.647765874862671\n",
      "epoch 1 batch 782 loss: 3.132518768310547\n",
      "epoch 1 batch 783 loss: 2.902233123779297\n",
      "epoch 1 batch 784 loss: 2.855717897415161\n",
      "epoch 1 batch 785 loss: 3.5143814086914062\n",
      "epoch 1 batch 786 loss: 3.043478012084961\n",
      "epoch 1 batch 787 loss: 3.000972270965576\n",
      "epoch 1 batch 788 loss: 2.887603282928467\n",
      "epoch 1 batch 789 loss: 2.717881202697754\n",
      "epoch 1 batch 790 loss: 2.574963331222534\n",
      "epoch 1 batch 791 loss: 2.8693690299987793\n",
      "epoch 1 batch 792 loss: 2.877889633178711\n",
      "epoch 1 batch 793 loss: 3.280419111251831\n",
      "epoch 1 batch 794 loss: 2.9729015827178955\n",
      "epoch 1 batch 795 loss: 2.6473264694213867\n",
      "epoch 1 batch 796 loss: 2.8601417541503906\n",
      "epoch 1 batch 797 loss: 2.8031647205352783\n",
      "epoch 1 batch 798 loss: 3.001396656036377\n",
      "epoch 1 batch 799 loss: 2.4362783432006836\n",
      "epoch 1 batch 800 loss: 3.3869593143463135\n",
      "epoch 1 batch 801 loss: 2.720087766647339\n",
      "epoch 1 batch 802 loss: 3.0746631622314453\n",
      "epoch 1 batch 803 loss: 2.8156681060791016\n",
      "epoch 1 batch 804 loss: 2.825056552886963\n",
      "epoch 1 batch 805 loss: 2.712571382522583\n",
      "epoch 1 batch 806 loss: 2.605180263519287\n",
      "epoch 1 batch 807 loss: 2.5115606784820557\n",
      "epoch 1 batch 808 loss: 2.837646722793579\n",
      "epoch 1 batch 809 loss: 2.8528242111206055\n",
      "epoch 1 batch 810 loss: 2.8888468742370605\n",
      "epoch 1 batch 811 loss: 3.087824821472168\n",
      "epoch 1 batch 812 loss: 2.9614639282226562\n",
      "epoch 1 batch 813 loss: 3.1972765922546387\n",
      "epoch 1 batch 814 loss: 3.142272710800171\n",
      "epoch 1 batch 815 loss: 2.468863010406494\n",
      "epoch 1 batch 816 loss: 3.0407114028930664\n",
      "epoch 1 batch 817 loss: 2.95758056640625\n",
      "epoch 1 batch 818 loss: 2.855118751525879\n",
      "epoch 1 batch 819 loss: 3.463348865509033\n",
      "epoch 1 batch 820 loss: 3.1573143005371094\n",
      "epoch 1 batch 821 loss: 2.5198190212249756\n",
      "epoch 1 batch 822 loss: 2.9358081817626953\n",
      "epoch 1 batch 823 loss: 2.8764071464538574\n",
      "epoch 1 batch 824 loss: 2.8015449047088623\n",
      "epoch 1 batch 825 loss: 2.939793586730957\n",
      "epoch 1 batch 826 loss: 2.727738380432129\n",
      "epoch 1 batch 827 loss: 3.075995445251465\n",
      "epoch 1 batch 828 loss: 2.8459033966064453\n",
      "epoch 1 batch 829 loss: 2.9815945625305176\n",
      "epoch 1 batch 830 loss: 2.8553924560546875\n",
      "epoch 1 batch 831 loss: 3.137361526489258\n",
      "epoch 1 batch 832 loss: 3.1599695682525635\n",
      "epoch 1 batch 833 loss: 3.2347495555877686\n",
      "epoch 1 batch 834 loss: 2.8185720443725586\n",
      "epoch 1 batch 835 loss: 3.1394753456115723\n",
      "epoch 1 batch 836 loss: 2.996936321258545\n",
      "epoch 1 batch 837 loss: 2.9355154037475586\n",
      "epoch 1 batch 838 loss: 2.5760035514831543\n",
      "epoch 1 batch 839 loss: 2.982534885406494\n",
      "epoch 1 batch 840 loss: 2.56291127204895\n",
      "epoch 1 batch 841 loss: 3.413174867630005\n",
      "epoch 1 batch 842 loss: 3.1096386909484863\n",
      "epoch 1 batch 843 loss: 2.942939043045044\n",
      "epoch 1 batch 844 loss: 2.9949967861175537\n",
      "epoch 1 batch 845 loss: 2.7894344329833984\n",
      "epoch 1 batch 846 loss: 2.7896947860717773\n",
      "epoch 1 batch 847 loss: 2.662959098815918\n",
      "epoch 1 batch 848 loss: 2.803644895553589\n",
      "epoch 1 batch 849 loss: 2.818535089492798\n",
      "epoch 1 batch 850 loss: 3.171065330505371\n",
      "epoch 1 batch 851 loss: 2.953617572784424\n",
      "epoch 1 batch 852 loss: 3.1277389526367188\n",
      "epoch 1 batch 853 loss: 2.809946060180664\n",
      "epoch 1 batch 854 loss: 3.067513942718506\n",
      "epoch 1 batch 855 loss: 3.0347533226013184\n",
      "epoch 1 batch 856 loss: 2.9897947311401367\n",
      "epoch 1 batch 857 loss: 2.7921433448791504\n",
      "epoch 1 batch 858 loss: 2.5734949111938477\n",
      "epoch 1 batch 859 loss: 2.8793416023254395\n",
      "epoch 1 batch 860 loss: 2.9213669300079346\n",
      "epoch 1 batch 861 loss: 2.9466500282287598\n",
      "epoch 1 batch 862 loss: 3.027094841003418\n",
      "epoch 1 batch 863 loss: 3.0068626403808594\n",
      "epoch 1 batch 864 loss: 2.879091739654541\n",
      "epoch 1 batch 865 loss: 2.5988478660583496\n",
      "epoch 1 batch 866 loss: 2.535919189453125\n",
      "epoch 1 batch 867 loss: 2.4661028385162354\n",
      "epoch 1 batch 868 loss: 3.10992431640625\n",
      "epoch 1 batch 869 loss: 3.1144537925720215\n",
      "epoch 1 batch 870 loss: 3.2951622009277344\n",
      "epoch 1 batch 871 loss: 2.6572518348693848\n",
      "epoch 1 batch 872 loss: 2.83780574798584\n",
      "epoch 1 batch 873 loss: 2.5202817916870117\n",
      "epoch 1 batch 874 loss: 2.8710100650787354\n",
      "epoch 1 batch 875 loss: 3.1230950355529785\n",
      "epoch 1 batch 876 loss: 2.9344000816345215\n",
      "epoch 1 batch 877 loss: 2.8900442123413086\n",
      "epoch 1 batch 878 loss: 3.1532528400421143\n",
      "epoch 1 batch 879 loss: 3.0134291648864746\n",
      "epoch 1 batch 880 loss: 3.4252829551696777\n",
      "epoch 1 batch 881 loss: 3.2373199462890625\n",
      "epoch 1 batch 882 loss: 3.147908926010132\n",
      "epoch 1 batch 883 loss: 3.3153200149536133\n",
      "epoch 1 batch 884 loss: 2.575204849243164\n",
      "epoch 1 batch 885 loss: 2.671647310256958\n",
      "epoch 1 batch 886 loss: 3.1757466793060303\n",
      "epoch 1 batch 887 loss: 2.582247257232666\n",
      "epoch 1 batch 888 loss: 2.6532235145568848\n",
      "epoch 1 batch 889 loss: 2.756296157836914\n",
      "epoch 1 batch 890 loss: 2.9612889289855957\n",
      "epoch 1 batch 891 loss: 3.02518892288208\n",
      "epoch 1 batch 892 loss: 2.8952476978302\n",
      "epoch 1 batch 893 loss: 2.962602138519287\n",
      "epoch 1 batch 894 loss: 3.017418146133423\n",
      "epoch 1 batch 895 loss: 2.9473347663879395\n",
      "epoch 1 batch 896 loss: 3.253232479095459\n",
      "epoch 1 batch 897 loss: 2.805908203125\n",
      "epoch 1 batch 898 loss: 2.8956634998321533\n",
      "epoch 1 batch 899 loss: 2.7024121284484863\n",
      "epoch 1 batch 900 loss: 2.875674247741699\n",
      "epoch 1 batch 901 loss: 3.112321376800537\n",
      "epoch 1 batch 902 loss: 2.8066599369049072\n",
      "epoch 1 batch 903 loss: 2.670699119567871\n",
      "epoch 1 batch 904 loss: 3.048107147216797\n",
      "epoch 1 batch 905 loss: 3.1671009063720703\n",
      "epoch 1 batch 906 loss: 2.650547981262207\n",
      "epoch 1 batch 907 loss: 3.051968574523926\n",
      "epoch 1 batch 908 loss: 3.301917791366577\n",
      "epoch 1 batch 909 loss: 2.568171977996826\n",
      "epoch 1 batch 910 loss: 2.635758876800537\n",
      "epoch 1 batch 911 loss: 2.7048592567443848\n",
      "epoch 1 batch 912 loss: 2.8966662883758545\n",
      "epoch 1 batch 913 loss: 2.7118420600891113\n",
      "epoch 1 batch 914 loss: 2.89650559425354\n",
      "epoch 1 batch 915 loss: 2.5629444122314453\n",
      "epoch 1 batch 916 loss: 3.0034642219543457\n",
      "epoch 1 batch 917 loss: 3.1890225410461426\n",
      "epoch 1 batch 918 loss: 2.705015182495117\n",
      "epoch 1 batch 919 loss: 2.66882061958313\n",
      "epoch 1 batch 920 loss: 2.882901191711426\n",
      "epoch 1 batch 921 loss: 3.0028724670410156\n",
      "epoch 1 batch 922 loss: 2.9336156845092773\n",
      "epoch 1 batch 923 loss: 2.7829291820526123\n",
      "epoch 1 batch 924 loss: 2.9183902740478516\n",
      "epoch 1 batch 925 loss: 3.4970216751098633\n",
      "epoch 1 batch 926 loss: 2.951972484588623\n",
      "epoch 1 batch 927 loss: 2.4281721115112305\n",
      "epoch 1 batch 928 loss: 3.1657745838165283\n",
      "epoch 1 batch 929 loss: 3.0835790634155273\n",
      "epoch 1 batch 930 loss: 2.8688459396362305\n",
      "epoch 1 batch 931 loss: 2.652228832244873\n",
      "epoch 1 batch 932 loss: 2.7577972412109375\n",
      "epoch 1 batch 933 loss: 2.8778879642486572\n",
      "epoch 1 batch 934 loss: 2.797367572784424\n",
      "epoch 1 batch 935 loss: 3.256885051727295\n",
      "epoch 1 batch 936 loss: 2.7638092041015625\n",
      "epoch 1 batch 937 loss: 3.0820608139038086\n",
      "epoch 1 batch 938 loss: 3.2624545097351074\n",
      "epoch 1 batch 939 loss: 3.2790367603302\n",
      "epoch 1 batch 940 loss: 3.144681930541992\n",
      "epoch 1 batch 941 loss: 2.6611709594726562\n",
      "epoch 1 batch 942 loss: 2.7773313522338867\n",
      "epoch 1 batch 943 loss: 2.9571099281311035\n",
      "epoch 1 batch 944 loss: 3.123289108276367\n",
      "epoch 1 batch 945 loss: 2.643578290939331\n",
      "epoch 1 batch 946 loss: 2.802865505218506\n",
      "epoch 1 batch 947 loss: 2.6376953125\n",
      "epoch 1 batch 948 loss: 2.7955734729766846\n",
      "epoch 1 batch 949 loss: 2.9558563232421875\n",
      "epoch 1 batch 950 loss: 2.7729759216308594\n",
      "epoch 1 batch 951 loss: 2.8163022994995117\n",
      "epoch 1 batch 952 loss: 2.661674976348877\n",
      "epoch 1 batch 953 loss: 2.696965456008911\n",
      "epoch 1 batch 954 loss: 2.6284937858581543\n",
      "epoch 1 batch 955 loss: 2.747316837310791\n",
      "epoch 1 batch 956 loss: 3.146031618118286\n",
      "epoch 1 batch 957 loss: 2.9665088653564453\n",
      "epoch 1 batch 958 loss: 2.9772238731384277\n",
      "epoch 1 batch 959 loss: 2.8500609397888184\n",
      "epoch 1 batch 960 loss: 2.5171666145324707\n",
      "epoch 1 batch 961 loss: 3.1422805786132812\n",
      "epoch 1 batch 962 loss: 3.097921848297119\n",
      "epoch 1 batch 963 loss: 2.824286460876465\n",
      "epoch 1 batch 964 loss: 2.970707893371582\n",
      "epoch 1 batch 965 loss: 2.9935710430145264\n",
      "epoch 1 batch 966 loss: 3.0149950981140137\n",
      "epoch 1 batch 967 loss: 2.544809341430664\n",
      "epoch 1 batch 968 loss: 2.76590895652771\n",
      "epoch 1 batch 969 loss: 2.8810455799102783\n",
      "epoch 1 batch 970 loss: 2.9139833450317383\n",
      "epoch 1 batch 971 loss: 2.725389242172241\n",
      "epoch 1 batch 972 loss: 2.9909090995788574\n",
      "epoch 1 batch 973 loss: 3.3777549266815186\n",
      "epoch 1 batch 974 loss: 3.01608943939209\n",
      "epoch 1 batch 975 loss: 2.996490955352783\n",
      "epoch 1 batch 976 loss: 2.685595750808716\n",
      "epoch 1 batch 977 loss: 2.7346925735473633\n",
      "epoch 1 batch 978 loss: 2.8477518558502197\n",
      "epoch 1 batch 979 loss: 2.753251552581787\n",
      "epoch 1 batch 980 loss: 2.8963630199432373\n",
      "epoch 1 batch 981 loss: 2.8842523097991943\n",
      "epoch 1 batch 982 loss: 2.6389212608337402\n",
      "epoch 1 batch 983 loss: 3.0742814540863037\n",
      "epoch 1 batch 984 loss: 2.8680996894836426\n",
      "epoch 1 batch 985 loss: 3.1904306411743164\n",
      "epoch 1 batch 986 loss: 3.1701345443725586\n",
      "epoch 1 batch 987 loss: 3.065161943435669\n",
      "epoch 1 batch 988 loss: 3.0569005012512207\n",
      "epoch 1 batch 989 loss: 2.935692071914673\n",
      "epoch 1 batch 990 loss: 2.788851737976074\n",
      "epoch 1 batch 991 loss: 2.7064828872680664\n",
      "epoch 1 batch 992 loss: 3.1033544540405273\n",
      "epoch 1 batch 993 loss: 2.5282936096191406\n",
      "epoch 1 batch 994 loss: 3.124166965484619\n",
      "epoch 1 batch 995 loss: 2.95381498336792\n",
      "epoch 1 batch 996 loss: 3.376969814300537\n",
      "epoch 1 batch 997 loss: 2.8508572578430176\n",
      "epoch 1 batch 998 loss: 3.183183193206787\n",
      "epoch 1 batch 999 loss: 3.063150644302368\n",
      "epoch 1 batch 1000 loss: 2.48166823387146\n",
      "epoch 1 batch 1001 loss: 2.9511187076568604\n",
      "epoch 1 batch 1002 loss: 2.982639789581299\n",
      "epoch 1 batch 1003 loss: 2.7117810249328613\n",
      "epoch 1 batch 1004 loss: 2.895112991333008\n",
      "epoch 1 batch 1005 loss: 2.6830191612243652\n",
      "epoch 1 batch 1006 loss: 2.754889726638794\n",
      "epoch 1 batch 1007 loss: 2.914205312728882\n",
      "epoch 1 batch 1008 loss: 2.9468955993652344\n",
      "epoch 1 batch 1009 loss: 2.9899585247039795\n",
      "epoch 1 batch 1010 loss: 2.8116164207458496\n",
      "epoch 1 batch 1011 loss: 2.8817453384399414\n",
      "epoch 1 batch 1012 loss: 3.169126510620117\n",
      "epoch 1 batch 1013 loss: 2.9654290676116943\n",
      "epoch 1 batch 1014 loss: 3.014613389968872\n",
      "epoch 1 batch 1015 loss: 3.175105571746826\n",
      "epoch 1 batch 1016 loss: 3.3982625007629395\n",
      "epoch 1 batch 1017 loss: 2.7494606971740723\n",
      "epoch 1 batch 1018 loss: 2.974475860595703\n",
      "epoch 1 batch 1019 loss: 2.6291229724884033\n",
      "epoch 1 batch 1020 loss: 2.6182260513305664\n",
      "epoch 1 batch 1021 loss: 2.8030877113342285\n",
      "epoch 1 batch 1022 loss: 3.0467019081115723\n",
      "epoch 1 batch 1023 loss: 2.810486316680908\n",
      "epoch 1 batch 1024 loss: 2.8595967292785645\n",
      "epoch 1 batch 1025 loss: 2.824985980987549\n",
      "epoch 1 batch 1026 loss: 3.1344594955444336\n",
      "epoch 1 batch 1027 loss: 2.548023223876953\n",
      "epoch 1 batch 1028 loss: 2.8823280334472656\n",
      "epoch 1 batch 1029 loss: 2.9414501190185547\n",
      "epoch 1 batch 1030 loss: 3.268054485321045\n",
      "epoch 1 batch 1031 loss: 3.787386417388916\n",
      "epoch 1 batch 1032 loss: 3.221230983734131\n",
      "epoch 1 batch 1033 loss: 3.003829002380371\n",
      "epoch 1 batch 1034 loss: 2.6950843334198\n",
      "epoch 1 batch 1035 loss: 2.852198362350464\n",
      "epoch 1 batch 1036 loss: 3.282832145690918\n",
      "epoch 1 batch 1037 loss: 3.0370543003082275\n",
      "epoch 1 batch 1038 loss: 2.9940319061279297\n",
      "epoch 1 batch 1039 loss: 2.772505283355713\n",
      "epoch 1 batch 1040 loss: 2.6555113792419434\n",
      "epoch 1 batch 1041 loss: 2.8811137676239014\n",
      "epoch 1 batch 1042 loss: 3.030461311340332\n",
      "epoch 1 batch 1043 loss: 3.0062546730041504\n",
      "epoch 1 batch 1044 loss: 2.6627769470214844\n",
      "epoch 1 batch 1045 loss: 2.909868001937866\n",
      "epoch 1 batch 1046 loss: 2.9038925170898438\n",
      "epoch 1 batch 1047 loss: 2.720808744430542\n",
      "epoch 1 batch 1048 loss: 2.8872931003570557\n",
      "epoch 1 batch 1049 loss: 2.6865835189819336\n",
      "epoch 1 batch 1050 loss: 3.196155071258545\n",
      "epoch 1 batch 1051 loss: 2.7176215648651123\n",
      "epoch 1 batch 1052 loss: 2.5797441005706787\n",
      "epoch 1 batch 1053 loss: 2.632066249847412\n",
      "epoch 1 batch 1054 loss: 2.7610318660736084\n",
      "epoch 1 batch 1055 loss: 3.05234432220459\n",
      "epoch 1 batch 1056 loss: 2.50726318359375\n",
      "epoch 1 batch 1057 loss: 2.658811569213867\n",
      "epoch 1 batch 1058 loss: 3.146571636199951\n",
      "epoch 1 batch 1059 loss: 2.9879016876220703\n",
      "epoch 1 batch 1060 loss: 3.2098004817962646\n",
      "epoch 1 batch 1061 loss: 2.7449426651000977\n",
      "epoch 1 batch 1062 loss: 2.7079999446868896\n",
      "epoch 1 batch 1063 loss: 3.1026742458343506\n",
      "epoch 1 batch 1064 loss: 2.7219924926757812\n",
      "epoch 1 batch 1065 loss: 3.1070926189422607\n",
      "epoch 1 batch 1066 loss: 3.6465327739715576\n",
      "epoch 1 batch 1067 loss: 3.1830921173095703\n",
      "epoch 1 batch 1068 loss: 2.9264626502990723\n",
      "epoch 1 batch 1069 loss: 2.9149022102355957\n",
      "epoch 1 batch 1070 loss: 2.8273963928222656\n",
      "epoch 1 batch 1071 loss: 2.897890567779541\n",
      "epoch 1 batch 1072 loss: 2.82248592376709\n",
      "epoch 1 batch 1073 loss: 2.683359384536743\n",
      "epoch 1 batch 1074 loss: 2.862318515777588\n",
      "epoch 1 batch 1075 loss: 2.88278865814209\n",
      "epoch 1 batch 1076 loss: 2.869605779647827\n",
      "epoch 1 batch 1077 loss: 3.0159897804260254\n",
      "epoch 1 batch 1078 loss: 2.652256965637207\n",
      "epoch 1 batch 1079 loss: 2.8062381744384766\n",
      "epoch 1 batch 1080 loss: 2.609783411026001\n",
      "epoch 1 batch 1081 loss: 2.451687812805176\n",
      "epoch 1 batch 1082 loss: 3.0840935707092285\n",
      "epoch 1 batch 1083 loss: 2.7009565830230713\n",
      "epoch 1 batch 1084 loss: 2.762225389480591\n",
      "epoch 1 batch 1085 loss: 2.8357958793640137\n",
      "epoch 1 batch 1086 loss: 2.477663040161133\n",
      "epoch 1 batch 1087 loss: 3.0100457668304443\n",
      "epoch 1 batch 1088 loss: 2.7201590538024902\n",
      "epoch 1 batch 1089 loss: 2.826266050338745\n",
      "epoch 1 batch 1090 loss: 2.718837261199951\n",
      "epoch 1 batch 1091 loss: 2.554497718811035\n",
      "epoch 1 batch 1092 loss: 2.507721424102783\n",
      "epoch 1 batch 1093 loss: 2.820531129837036\n",
      "epoch 1 batch 1094 loss: 2.6854710578918457\n",
      "epoch 1 batch 1095 loss: 2.9499058723449707\n",
      "epoch 1 batch 1096 loss: 3.056727647781372\n",
      "epoch 1 batch 1097 loss: 2.95945405960083\n",
      "epoch 1 batch 1098 loss: 2.617978572845459\n",
      "epoch 1 batch 1099 loss: 2.957329273223877\n",
      "epoch 1 batch 1100 loss: 2.922001361846924\n",
      "epoch 1 batch 1101 loss: 2.930717945098877\n",
      "epoch 1 batch 1102 loss: 2.785656690597534\n",
      "epoch 1 batch 1103 loss: 2.811483383178711\n",
      "epoch 1 batch 1104 loss: 2.98476505279541\n",
      "epoch 1 batch 1105 loss: 2.746939182281494\n",
      "epoch 1 batch 1106 loss: 3.2009966373443604\n",
      "epoch 1 batch 1107 loss: 2.843353748321533\n",
      "epoch 1 batch 1108 loss: 2.8792014122009277\n",
      "epoch 1 batch 1109 loss: 2.6825037002563477\n",
      "epoch 1 batch 1110 loss: 3.1025164127349854\n",
      "epoch 1 batch 1111 loss: 2.958940267562866\n",
      "epoch 1 batch 1112 loss: 2.8574583530426025\n",
      "epoch 1 batch 1113 loss: 2.488637685775757\n",
      "epoch 1 batch 1114 loss: 3.181657314300537\n",
      "epoch 1 batch 1115 loss: 3.1871471405029297\n",
      "epoch 1 batch 1116 loss: 2.8467118740081787\n",
      "epoch 1 batch 1117 loss: 3.2248482704162598\n",
      "epoch 1 batch 1118 loss: 2.9944863319396973\n",
      "epoch 1 batch 1119 loss: 3.1978864669799805\n",
      "epoch 1 batch 1120 loss: 2.6961798667907715\n",
      "epoch 1 batch 1121 loss: 2.9106605052948\n",
      "epoch 1 batch 1122 loss: 2.978196382522583\n",
      "epoch 1 batch 1123 loss: 2.786654233932495\n",
      "epoch 1 batch 1124 loss: 2.576181650161743\n",
      "epoch 1 batch 1125 loss: 2.9532432556152344\n",
      "epoch 1 batch 1126 loss: 2.9268126487731934\n",
      "epoch 1 batch 1127 loss: 3.021885871887207\n",
      "epoch 1 batch 1128 loss: 2.736384868621826\n",
      "epoch 1 batch 1129 loss: 2.851210117340088\n",
      "epoch 1 batch 1130 loss: 2.5623772144317627\n",
      "epoch 1 batch 1131 loss: 2.961441993713379\n",
      "epoch 1 batch 1132 loss: 2.620389938354492\n",
      "epoch 1 batch 1133 loss: 2.8709287643432617\n",
      "epoch 1 batch 1134 loss: 2.962393283843994\n",
      "epoch 1 batch 1135 loss: 2.6544721126556396\n",
      "epoch 1 batch 1136 loss: 2.6552534103393555\n",
      "epoch 1 batch 1137 loss: 2.7336244583129883\n",
      "epoch 1 batch 1138 loss: 3.401956558227539\n",
      "epoch 1 batch 1139 loss: 2.4367215633392334\n",
      "epoch 1 batch 1140 loss: 2.8747940063476562\n",
      "epoch 1 batch 1141 loss: 2.9072883129119873\n",
      "epoch 1 batch 1142 loss: 2.73331356048584\n",
      "epoch 1 batch 1143 loss: 2.591325283050537\n",
      "epoch 1 batch 1144 loss: 2.602499008178711\n",
      "epoch 1 batch 1145 loss: 2.72346568107605\n",
      "epoch 1 batch 1146 loss: 2.7069091796875\n",
      "epoch 1 batch 1147 loss: 2.6254804134368896\n",
      "epoch 1 batch 1148 loss: 2.733386516571045\n",
      "epoch 1 batch 1149 loss: 2.5297341346740723\n",
      "epoch 1 batch 1150 loss: 2.9793365001678467\n",
      "epoch 1 batch 1151 loss: 2.94692325592041\n",
      "epoch 1 batch 1152 loss: 3.4250614643096924\n",
      "epoch 1 batch 1153 loss: 3.0194344520568848\n",
      "epoch 1 batch 1154 loss: 2.7960991859436035\n",
      "epoch 1 batch 1155 loss: 3.0217103958129883\n",
      "epoch 1 batch 1156 loss: 3.1385316848754883\n",
      "epoch 1 batch 1157 loss: 3.234138011932373\n",
      "epoch 1 batch 1158 loss: 3.1464576721191406\n",
      "epoch 1 batch 1159 loss: 2.507848024368286\n",
      "epoch 1 batch 1160 loss: 2.9039134979248047\n",
      "epoch 1 batch 1161 loss: 3.0241189002990723\n",
      "epoch 1 batch 1162 loss: 2.819260835647583\n",
      "epoch 1 batch 1163 loss: 3.4537787437438965\n",
      "epoch 1 batch 1164 loss: 2.932816505432129\n",
      "epoch 1 batch 1165 loss: 2.6673920154571533\n",
      "epoch 1 batch 1166 loss: 2.9191410541534424\n",
      "epoch 1 batch 1167 loss: 2.5499072074890137\n",
      "epoch 1 batch 1168 loss: 2.9747133255004883\n",
      "epoch 1 batch 1169 loss: 2.5807666778564453\n",
      "epoch 1 batch 1170 loss: 3.003694534301758\n",
      "epoch 1 batch 1171 loss: 2.6466479301452637\n",
      "epoch 1 batch 1172 loss: 3.0724782943725586\n",
      "epoch 1 batch 1173 loss: 3.032613515853882\n",
      "epoch 1 batch 1174 loss: 3.236593246459961\n",
      "epoch 1 batch 1175 loss: 2.896104335784912\n",
      "epoch 1 batch 1176 loss: 3.0602025985717773\n",
      "epoch 1 batch 1177 loss: 2.997121810913086\n",
      "epoch 1 batch 1178 loss: 2.8816680908203125\n",
      "epoch 1 batch 1179 loss: 3.012355327606201\n",
      "epoch 1 batch 1180 loss: 2.856501340866089\n",
      "epoch 1 batch 1181 loss: 2.5329391956329346\n",
      "epoch 1 batch 1182 loss: 3.173487663269043\n",
      "epoch 1 batch 1183 loss: 2.8115053176879883\n",
      "epoch 1 batch 1184 loss: 2.7192673683166504\n",
      "epoch 1 batch 1185 loss: 2.7058215141296387\n",
      "epoch 1 batch 1186 loss: 2.881843090057373\n",
      "epoch 1 batch 1187 loss: 2.755711078643799\n",
      "epoch 1 batch 1188 loss: 2.885916233062744\n",
      "epoch 1 batch 1189 loss: 2.374168634414673\n",
      "epoch 1 batch 1190 loss: 2.7986838817596436\n",
      "epoch 1 batch 1191 loss: 2.734957218170166\n",
      "epoch 1 batch 1192 loss: 2.6471519470214844\n",
      "epoch 1 batch 1193 loss: 2.692359685897827\n",
      "epoch 1 batch 1194 loss: 3.193251609802246\n",
      "epoch 1 batch 1195 loss: 2.656712293624878\n",
      "epoch 1 batch 1196 loss: 2.6233787536621094\n",
      "epoch 1 batch 1197 loss: 3.0220518112182617\n",
      "epoch 1 batch 1198 loss: 2.741219997406006\n",
      "epoch 1 batch 1199 loss: 2.5881800651550293\n",
      "epoch 1 batch 1200 loss: 3.2937605381011963\n",
      "epoch 1 batch 1201 loss: 2.5801405906677246\n",
      "epoch 1 batch 1202 loss: 2.9980826377868652\n",
      "epoch 1 batch 1203 loss: 3.0459392070770264\n",
      "epoch 1 batch 1204 loss: 2.826385021209717\n",
      "epoch 1 batch 1205 loss: 2.7381062507629395\n",
      "epoch 1 batch 1206 loss: 2.8954524993896484\n",
      "epoch 1 batch 1207 loss: 3.296977996826172\n",
      "epoch 1 batch 1208 loss: 2.6800947189331055\n",
      "epoch 1 batch 1209 loss: 2.7578158378601074\n",
      "epoch 1 batch 1210 loss: 2.521818161010742\n",
      "epoch 1 batch 1211 loss: 3.136953353881836\n",
      "epoch 1 batch 1212 loss: 3.0600085258483887\n",
      "epoch 1 batch 1213 loss: 2.759582281112671\n",
      "epoch 1 batch 1214 loss: 2.532148599624634\n",
      "epoch 1 batch 1215 loss: 2.7174746990203857\n",
      "epoch 1 batch 1216 loss: 2.9184389114379883\n",
      "epoch 1 batch 1217 loss: 3.170865297317505\n",
      "epoch 1 batch 1218 loss: 3.479196071624756\n",
      "epoch 1 batch 1219 loss: 2.814934253692627\n",
      "epoch 1 batch 1220 loss: 2.4491472244262695\n",
      "epoch 1 batch 1221 loss: 2.6981606483459473\n",
      "epoch 1 batch 1222 loss: 2.847200393676758\n",
      "epoch 1 batch 1223 loss: 2.804121971130371\n",
      "epoch 1 batch 1224 loss: 2.619460105895996\n",
      "epoch 1 batch 1225 loss: 3.033543586730957\n",
      "epoch 1 batch 1226 loss: 2.6840643882751465\n",
      "epoch 1 batch 1227 loss: 2.8083672523498535\n",
      "epoch 1 batch 1228 loss: 3.182063579559326\n",
      "epoch 1 batch 1229 loss: 2.427258014678955\n",
      "epoch 1 batch 1230 loss: 2.973155975341797\n",
      "epoch 1 batch 1231 loss: 3.0003209114074707\n",
      "epoch 1 batch 1232 loss: 2.9024014472961426\n",
      "epoch 1 batch 1233 loss: 2.7816243171691895\n",
      "epoch 1 batch 1234 loss: 2.8651773929595947\n",
      "epoch 1 batch 1235 loss: 3.204190254211426\n",
      "epoch 1 batch 1236 loss: 3.130506992340088\n",
      "epoch 1 batch 1237 loss: 2.8117494583129883\n",
      "epoch 1 batch 1238 loss: 2.497593402862549\n",
      "epoch 1 batch 1239 loss: 2.8238916397094727\n",
      "epoch 1 batch 1240 loss: 3.2461977005004883\n",
      "epoch 1 batch 1241 loss: 2.704746961593628\n",
      "epoch 1 batch 1242 loss: 3.1436028480529785\n",
      "epoch 1 batch 1243 loss: 2.845585584640503\n",
      "epoch 1 batch 1244 loss: 2.598109722137451\n",
      "epoch 1 batch 1245 loss: 2.9686875343322754\n",
      "epoch 1 batch 1246 loss: 2.8285164833068848\n",
      "epoch 1 batch 1247 loss: 3.4448297023773193\n",
      "epoch 1 batch 1248 loss: 3.0903100967407227\n",
      "epoch 1 batch 1249 loss: 2.6039819717407227\n",
      "epoch 1 batch 1250 loss: 2.850059747695923\n",
      "epoch 1 batch 1251 loss: 3.1311774253845215\n",
      "epoch 1 batch 1252 loss: 3.159010410308838\n",
      "epoch 1 batch 1253 loss: 2.826277732849121\n",
      "epoch 1 batch 1254 loss: 2.6394317150115967\n",
      "epoch 1 batch 1255 loss: 2.7512130737304688\n",
      "epoch 1 batch 1256 loss: 2.9484198093414307\n",
      "epoch 1 batch 1257 loss: 2.7536511421203613\n",
      "epoch 1 batch 1258 loss: 2.787935256958008\n",
      "epoch 1 batch 1259 loss: 3.466608762741089\n",
      "epoch 1 batch 1260 loss: 2.875051498413086\n",
      "epoch 1 batch 1261 loss: 2.476444721221924\n",
      "epoch 1 batch 1262 loss: 2.7851130962371826\n",
      "epoch 1 batch 1263 loss: 3.1333022117614746\n",
      "epoch 1 batch 1264 loss: 2.670558452606201\n",
      "epoch 1 batch 1265 loss: 2.8246140480041504\n",
      "epoch 1 batch 1266 loss: 3.380553960800171\n",
      "epoch 1 batch 1267 loss: 2.9968059062957764\n",
      "epoch 1 batch 1268 loss: 2.958804130554199\n",
      "epoch 1 batch 1269 loss: 2.6935386657714844\n",
      "epoch 1 batch 1270 loss: 2.805546522140503\n",
      "epoch 1 batch 1271 loss: 2.770322322845459\n",
      "epoch 1 batch 1272 loss: 2.5133471488952637\n",
      "epoch 1 batch 1273 loss: 3.120239019393921\n",
      "epoch 1 batch 1274 loss: 2.778336763381958\n",
      "epoch 1 batch 1275 loss: 2.8958544731140137\n",
      "epoch 1 batch 1276 loss: 3.014402151107788\n",
      "epoch 1 batch 1277 loss: 2.766327142715454\n",
      "epoch 1 batch 1278 loss: 2.8094582557678223\n",
      "epoch 1 batch 1279 loss: 2.720174789428711\n",
      "epoch 1 batch 1280 loss: 2.634476661682129\n",
      "epoch 1 batch 1281 loss: 2.644406318664551\n",
      "epoch 1 batch 1282 loss: 2.9318454265594482\n",
      "epoch 1 batch 1283 loss: 3.1451239585876465\n",
      "epoch 1 batch 1284 loss: 3.005125045776367\n",
      "epoch 1 batch 1285 loss: 2.64363431930542\n",
      "epoch 1 batch 1286 loss: 3.108363389968872\n",
      "epoch 1 batch 1287 loss: 2.6606500148773193\n",
      "epoch 1 batch 1288 loss: 3.0138375759124756\n",
      "epoch 1 batch 1289 loss: 2.9222583770751953\n",
      "epoch 1 batch 1290 loss: 2.6252217292785645\n",
      "epoch 1 batch 1291 loss: 2.8539979457855225\n",
      "epoch 1 batch 1292 loss: 2.528299570083618\n",
      "epoch 1 batch 1293 loss: 2.723299026489258\n",
      "epoch 1 batch 1294 loss: 2.636521577835083\n",
      "epoch 1 batch 1295 loss: 2.9254863262176514\n",
      "epoch 1 batch 1296 loss: 2.7635443210601807\n",
      "epoch 1 batch 1297 loss: 2.5106842517852783\n",
      "epoch 1 batch 1298 loss: 3.18709135055542\n",
      "epoch 1 batch 1299 loss: 2.5097196102142334\n",
      "epoch 1 batch 1300 loss: 2.8552088737487793\n",
      "epoch 1 batch 1301 loss: 2.858883857727051\n",
      "epoch 1 batch 1302 loss: 3.0951144695281982\n",
      "epoch 1 batch 1303 loss: 2.7498087882995605\n",
      "epoch 1 batch 1304 loss: 2.8158035278320312\n",
      "epoch 1 batch 1305 loss: 2.8108766078948975\n",
      "epoch 1 batch 1306 loss: 3.062065362930298\n",
      "epoch 1 batch 1307 loss: 2.8923635482788086\n",
      "epoch 1 batch 1308 loss: 2.657578468322754\n",
      "epoch 1 batch 1309 loss: 2.916161060333252\n",
      "epoch 1 batch 1310 loss: 2.7291922569274902\n",
      "epoch 1 batch 1311 loss: 2.839033603668213\n",
      "epoch 1 batch 1312 loss: 2.955929756164551\n",
      "epoch 1 batch 1313 loss: 2.6834464073181152\n",
      "epoch 1 batch 1314 loss: 2.646653652191162\n",
      "epoch 1 batch 1315 loss: 2.7736682891845703\n",
      "epoch 1 batch 1316 loss: 3.2692580223083496\n",
      "epoch 1 batch 1317 loss: 2.6713271141052246\n",
      "epoch 1 batch 1318 loss: 2.7324023246765137\n",
      "epoch 1 batch 1319 loss: 2.7210588455200195\n",
      "epoch 1 batch 1320 loss: 2.7907605171203613\n",
      "epoch 1 batch 1321 loss: 2.5022976398468018\n",
      "epoch 1 batch 1322 loss: 3.1952743530273438\n",
      "epoch 1 batch 1323 loss: 2.3382606506347656\n",
      "epoch 1 batch 1324 loss: 2.6113088130950928\n",
      "epoch 1 batch 1325 loss: 2.999147891998291\n",
      "epoch 1 batch 1326 loss: 2.4969687461853027\n",
      "epoch 1 batch 1327 loss: 2.759523868560791\n",
      "epoch 1 batch 1328 loss: 2.793491840362549\n",
      "epoch 1 batch 1329 loss: 2.9501969814300537\n",
      "epoch 1 batch 1330 loss: 2.7510576248168945\n",
      "epoch 1 batch 1331 loss: 3.106713056564331\n",
      "epoch 1 batch 1332 loss: 2.6828534603118896\n",
      "epoch 1 batch 1333 loss: 2.7601675987243652\n",
      "epoch 1 batch 1334 loss: 3.235201835632324\n",
      "epoch 1 batch 1335 loss: 2.661339282989502\n",
      "epoch 1 batch 1336 loss: 2.649785041809082\n",
      "epoch 1 batch 1337 loss: 2.863990545272827\n",
      "epoch 1 batch 1338 loss: 2.7991511821746826\n",
      "epoch 1 batch 1339 loss: 2.8694889545440674\n",
      "epoch 1 batch 1340 loss: 3.3642983436584473\n",
      "epoch 1 batch 1341 loss: 2.9738965034484863\n",
      "epoch 1 batch 1342 loss: 2.901181221008301\n",
      "epoch 1 batch 1343 loss: 2.9629709720611572\n",
      "epoch 1 batch 1344 loss: 2.6438517570495605\n",
      "epoch 1 batch 1345 loss: 2.6657798290252686\n",
      "epoch 1 batch 1346 loss: 2.6121225357055664\n",
      "epoch 1 batch 1347 loss: 2.624917984008789\n",
      "epoch 1 batch 1348 loss: 2.7861452102661133\n",
      "epoch 1 batch 1349 loss: 2.702791690826416\n",
      "epoch 1 batch 1350 loss: 2.942089796066284\n",
      "epoch 1 batch 1351 loss: 2.6036951541900635\n",
      "epoch 1 batch 1352 loss: 2.5701661109924316\n",
      "epoch 1 batch 1353 loss: 2.707146644592285\n",
      "epoch 1 batch 1354 loss: 3.318871259689331\n",
      "epoch 1 batch 1355 loss: 2.8230807781219482\n",
      "epoch 1 batch 1356 loss: 2.768263816833496\n",
      "epoch 1 batch 1357 loss: 2.4970860481262207\n",
      "epoch 1 batch 1358 loss: 2.5467569828033447\n",
      "epoch 1 batch 1359 loss: 3.232198715209961\n",
      "epoch 1 batch 1360 loss: 2.518871307373047\n",
      "epoch 1 batch 1361 loss: 2.7560458183288574\n",
      "epoch 1 batch 1362 loss: 3.073573589324951\n",
      "epoch 1 batch 1363 loss: 3.0090041160583496\n",
      "epoch 1 batch 1364 loss: 3.209171772003174\n",
      "epoch 1 batch 1365 loss: 3.0499839782714844\n",
      "epoch 1 batch 1366 loss: 2.8938229084014893\n",
      "epoch 1 batch 1367 loss: 2.429502487182617\n",
      "epoch 1 batch 1368 loss: 2.6091372966766357\n",
      "epoch 1 batch 1369 loss: 2.8853211402893066\n",
      "epoch 1 batch 1370 loss: 2.762302875518799\n",
      "epoch 1 batch 1371 loss: 2.7288060188293457\n",
      "epoch 1 batch 1372 loss: 2.731926441192627\n",
      "epoch 1 batch 1373 loss: 2.6554315090179443\n",
      "epoch 1 batch 1374 loss: 2.9939115047454834\n",
      "epoch 1 batch 1375 loss: 2.7989845275878906\n",
      "epoch 1 batch 1376 loss: 2.945772647857666\n",
      "epoch 1 batch 1377 loss: 2.9276046752929688\n",
      "epoch 1 batch 1378 loss: 2.974876880645752\n",
      "epoch 1 batch 1379 loss: 3.028533458709717\n",
      "epoch 1 batch 1380 loss: 2.632434129714966\n",
      "epoch 1 batch 1381 loss: 2.3637640476226807\n",
      "epoch 1 batch 1382 loss: 2.9220097064971924\n",
      "epoch 1 batch 1383 loss: 2.8154659271240234\n",
      "epoch 1 batch 1384 loss: 3.197028636932373\n",
      "epoch 1 batch 1385 loss: 2.653541088104248\n",
      "epoch 1 batch 1386 loss: 3.085756778717041\n",
      "epoch 1 batch 1387 loss: 2.519031047821045\n",
      "epoch 1 batch 1388 loss: 2.5940968990325928\n",
      "epoch 1 batch 1389 loss: 3.0938382148742676\n",
      "epoch 1 batch 1390 loss: 2.5805859565734863\n",
      "epoch 1 batch 1391 loss: 2.9668631553649902\n",
      "epoch 1 batch 1392 loss: 3.1746134757995605\n",
      "epoch 1 batch 1393 loss: 2.8285388946533203\n",
      "epoch 1 batch 1394 loss: 2.739668846130371\n",
      "epoch 1 batch 1395 loss: 2.9446630477905273\n",
      "epoch 1 batch 1396 loss: 2.704803466796875\n",
      "epoch 1 batch 1397 loss: 2.904818534851074\n",
      "epoch 1 batch 1398 loss: 3.0202107429504395\n",
      "epoch 1 batch 1399 loss: 3.275458335876465\n",
      "epoch 1 batch 1400 loss: 2.7526438236236572\n",
      "epoch 1 batch 1401 loss: 2.6815595626831055\n",
      "epoch 1 batch 1402 loss: 2.7846858501434326\n",
      "epoch 1 batch 1403 loss: 2.658904552459717\n",
      "epoch 1 batch 1404 loss: 2.9410204887390137\n",
      "epoch 1 batch 1405 loss: 2.4543495178222656\n",
      "epoch 1 batch 1406 loss: 2.9123706817626953\n",
      "epoch 1 batch 1407 loss: 2.6660702228546143\n",
      "epoch 1 batch 1408 loss: 2.6272339820861816\n",
      "epoch 1 batch 1409 loss: 2.561678886413574\n",
      "epoch 1 batch 1410 loss: 2.962132453918457\n",
      "epoch 1 batch 1411 loss: 2.6969470977783203\n",
      "epoch 1 batch 1412 loss: 2.667567729949951\n",
      "epoch 1 batch 1413 loss: 2.6263532638549805\n",
      "epoch 1 batch 1414 loss: 2.5358071327209473\n",
      "epoch 1 batch 1415 loss: 3.0494918823242188\n",
      "epoch 1 batch 1416 loss: 2.866331100463867\n",
      "epoch 1 batch 1417 loss: 2.918130397796631\n",
      "epoch 1 batch 1418 loss: 2.769394874572754\n",
      "epoch 1 batch 1419 loss: 3.0279369354248047\n",
      "epoch 1 batch 1420 loss: 2.5061450004577637\n",
      "epoch 1 batch 1421 loss: 2.528316020965576\n",
      "epoch 1 batch 1422 loss: 2.898381233215332\n",
      "epoch 1 batch 1423 loss: 2.8568639755249023\n",
      "epoch 1 batch 1424 loss: 2.758139133453369\n",
      "epoch 1 batch 1425 loss: 2.8293912410736084\n",
      "epoch 1 batch 1426 loss: 2.9708099365234375\n",
      "epoch 1 batch 1427 loss: 2.7237963676452637\n",
      "epoch 1 batch 1428 loss: 2.4800000190734863\n",
      "epoch 1 batch 1429 loss: 2.478119373321533\n",
      "epoch 1 batch 1430 loss: 3.083583354949951\n",
      "epoch 1 batch 1431 loss: 3.183056354522705\n",
      "epoch 1 batch 1432 loss: 2.758726119995117\n",
      "epoch 1 batch 1433 loss: 2.3573527336120605\n",
      "epoch 1 batch 1434 loss: 2.8934051990509033\n",
      "epoch 1 batch 1435 loss: 3.0114097595214844\n",
      "epoch 1 batch 1436 loss: 2.794736862182617\n",
      "epoch 1 batch 1437 loss: 2.852963924407959\n",
      "epoch 1 batch 1438 loss: 2.51281476020813\n",
      "epoch 1 batch 1439 loss: 2.5444583892822266\n",
      "epoch 1 batch 1440 loss: 2.6039843559265137\n",
      "epoch 1 batch 1441 loss: 2.749912977218628\n",
      "epoch 1 batch 1442 loss: 2.784599781036377\n",
      "epoch 1 batch 1443 loss: 3.0252456665039062\n",
      "epoch 1 batch 1444 loss: 2.801154136657715\n",
      "epoch 1 batch 1445 loss: 2.992152452468872\n",
      "epoch 1 batch 1446 loss: 2.827712059020996\n",
      "epoch 1 batch 1447 loss: 2.8572869300842285\n",
      "epoch 1 batch 1448 loss: 2.786567211151123\n",
      "epoch 1 batch 1449 loss: 3.2821617126464844\n",
      "epoch 1 batch 1450 loss: 3.187364101409912\n",
      "epoch 1 batch 1451 loss: 2.3437700271606445\n",
      "epoch 1 batch 1452 loss: 2.9947128295898438\n",
      "epoch 1 batch 1453 loss: 2.86340594291687\n",
      "epoch 1 batch 1454 loss: 2.9697670936584473\n",
      "epoch 1 batch 1455 loss: 2.659926414489746\n",
      "epoch 1 batch 1456 loss: 2.8845014572143555\n",
      "epoch 1 batch 1457 loss: 2.9265198707580566\n",
      "epoch 1 batch 1458 loss: 2.9665989875793457\n",
      "epoch 1 batch 1459 loss: 3.047959327697754\n",
      "epoch 1 batch 1460 loss: 3.042189121246338\n",
      "epoch 1 batch 1461 loss: 2.8856852054595947\n",
      "epoch 1 batch 1462 loss: 2.9351935386657715\n",
      "epoch 1 batch 1463 loss: 2.797856330871582\n",
      "epoch 1 batch 1464 loss: 2.5834813117980957\n",
      "epoch 1 batch 1465 loss: 2.90504789352417\n",
      "epoch 1 batch 1466 loss: 2.697995662689209\n",
      "epoch 1 batch 1467 loss: 2.9335341453552246\n",
      "epoch 1 batch 1468 loss: 3.082962989807129\n",
      "epoch 1 batch 1469 loss: 2.5095672607421875\n",
      "epoch 1 batch 1470 loss: 3.0980052947998047\n",
      "epoch 1 batch 1471 loss: 2.8904671669006348\n",
      "epoch 1 batch 1472 loss: 2.982089042663574\n",
      "epoch 1 batch 1473 loss: 3.3583526611328125\n",
      "epoch 1 batch 1474 loss: 2.9358677864074707\n",
      "epoch 1 batch 1475 loss: 2.5015645027160645\n",
      "epoch 1 batch 1476 loss: 2.887629747390747\n",
      "epoch 1 batch 1477 loss: 3.1391119956970215\n",
      "epoch 1 batch 1478 loss: 2.8059825897216797\n",
      "epoch 1 batch 1479 loss: 2.40092134475708\n",
      "epoch 1 batch 1480 loss: 2.6148953437805176\n",
      "epoch 1 batch 1481 loss: 3.173316478729248\n",
      "epoch 1 batch 1482 loss: 2.662128448486328\n",
      "epoch 1 batch 1483 loss: 2.6189122200012207\n",
      "epoch 1 batch 1484 loss: 2.9341225624084473\n",
      "epoch 1 batch 1485 loss: 3.0193934440612793\n",
      "epoch 1 batch 1486 loss: 2.764613628387451\n",
      "epoch 1 batch 1487 loss: 2.6783571243286133\n",
      "epoch 1 batch 1488 loss: 3.0211777687072754\n",
      "epoch 1 batch 1489 loss: 2.904470920562744\n",
      "epoch 1 batch 1490 loss: 2.902068614959717\n",
      "epoch 1 batch 1491 loss: 2.9553184509277344\n",
      "epoch 1 batch 1492 loss: 2.766749382019043\n",
      "epoch 1 batch 1493 loss: 3.609088897705078\n",
      "epoch 1 batch 1494 loss: 2.674133539199829\n",
      "epoch 1 batch 1495 loss: 2.651932954788208\n",
      "epoch 1 batch 1496 loss: 3.0512332916259766\n",
      "epoch 1 batch 1497 loss: 3.0612120628356934\n",
      "epoch 1 batch 1498 loss: 2.9588356018066406\n",
      "epoch 1 batch 1499 loss: 2.8778486251831055\n",
      "epoch 1 batch 1500 loss: 3.002516031265259\n",
      "epoch 1 batch 1501 loss: 3.0774929523468018\n",
      "epoch 1 batch 1502 loss: 2.547210216522217\n",
      "epoch 1 batch 1503 loss: 2.6354563236236572\n",
      "epoch 1 batch 1504 loss: 3.0889992713928223\n",
      "epoch 1 batch 1505 loss: 2.978109359741211\n",
      "epoch 1 batch 1506 loss: 2.9415009021759033\n",
      "epoch 1 batch 1507 loss: 2.693150043487549\n",
      "epoch 1 batch 1508 loss: 2.906433582305908\n",
      "epoch 1 batch 1509 loss: 2.7333507537841797\n",
      "epoch 1 batch 1510 loss: 2.545034646987915\n",
      "epoch 1 batch 1511 loss: 2.6566219329833984\n",
      "epoch 1 batch 1512 loss: 2.6515026092529297\n",
      "epoch 1 batch 1513 loss: 3.000858783721924\n",
      "epoch 1 batch 1514 loss: 2.856546401977539\n",
      "epoch 1 batch 1515 loss: 2.380390167236328\n",
      "epoch 1 batch 1516 loss: 2.6456961631774902\n",
      "epoch 1 batch 1517 loss: 2.7288427352905273\n",
      "epoch 1 batch 1518 loss: 3.001056671142578\n",
      "epoch 1 batch 1519 loss: 2.6749892234802246\n",
      "epoch 1 batch 1520 loss: 2.6732749938964844\n",
      "epoch 1 batch 1521 loss: 2.410270929336548\n",
      "epoch 1 batch 1522 loss: 2.7126076221466064\n",
      "epoch 1 batch 1523 loss: 2.524028778076172\n",
      "epoch 1 batch 1524 loss: 2.715485095977783\n",
      "epoch 1 batch 1525 loss: 2.98469877243042\n",
      "epoch 1 batch 1526 loss: 2.875159502029419\n",
      "epoch 1 batch 1527 loss: 2.7459044456481934\n",
      "epoch 1 batch 1528 loss: 2.391906261444092\n",
      "epoch 1 batch 1529 loss: 2.754671096801758\n",
      "epoch 1 batch 1530 loss: 2.438981771469116\n",
      "epoch 1 batch 1531 loss: 2.9050207138061523\n",
      "epoch 1 batch 1532 loss: 2.6986517906188965\n",
      "epoch 1 batch 1533 loss: 2.6777548789978027\n",
      "epoch 1 batch 1534 loss: 2.982694625854492\n",
      "epoch 1 batch 1535 loss: 2.8791613578796387\n",
      "epoch 1 batch 1536 loss: 2.856161594390869\n",
      "epoch 1 batch 1537 loss: 2.7118349075317383\n",
      "epoch 1 batch 1538 loss: 2.9212288856506348\n",
      "epoch 1 batch 1539 loss: 2.680530548095703\n",
      "epoch 1 batch 1540 loss: 2.9155614376068115\n",
      "epoch 1 batch 1541 loss: 2.732715606689453\n",
      "epoch 1 batch 1542 loss: 2.901249408721924\n",
      "epoch 1 batch 1543 loss: 2.5376405715942383\n",
      "epoch 1 batch 1544 loss: 2.6583518981933594\n",
      "epoch 1 batch 1545 loss: 3.2823991775512695\n",
      "epoch 1 batch 1546 loss: 2.8497471809387207\n",
      "epoch 1 batch 1547 loss: 2.7040936946868896\n",
      "epoch 1 batch 1548 loss: 2.7231812477111816\n",
      "epoch 1 batch 1549 loss: 2.5382847785949707\n",
      "epoch 1 batch 1550 loss: 2.5573678016662598\n",
      "epoch 1 batch 1551 loss: 2.881218671798706\n",
      "epoch 1 batch 1552 loss: 2.8531789779663086\n",
      "epoch 1 batch 1553 loss: 3.1850271224975586\n",
      "epoch 1 batch 1554 loss: 2.8556084632873535\n",
      "epoch 1 batch 1555 loss: 2.994722366333008\n",
      "epoch 1 batch 1556 loss: 2.554684638977051\n",
      "epoch 1 batch 1557 loss: 3.0635287761688232\n",
      "epoch 1 batch 1558 loss: 2.623328447341919\n",
      "epoch 1 batch 1559 loss: 2.735968828201294\n",
      "epoch 1 batch 1560 loss: 2.846935987472534\n",
      "epoch 1 batch 1561 loss: 2.5435500144958496\n",
      "epoch 1 batch 1562 loss: 3.051774024963379\n",
      "epoch 1 batch 1563 loss: 2.840618133544922\n",
      "epoch 1 batch 1564 loss: 2.834656000137329\n",
      "epoch 1 batch 1565 loss: 3.1403961181640625\n",
      "epoch 1 batch 1566 loss: 2.7728865146636963\n",
      "epoch 1 batch 1567 loss: 2.8422656059265137\n",
      "epoch 1 batch 1568 loss: 2.8439087867736816\n",
      "epoch 1 batch 1569 loss: 2.7614731788635254\n",
      "epoch 1 batch 1570 loss: 2.683807373046875\n",
      "epoch 1 batch 1571 loss: 2.806303024291992\n",
      "epoch 1 batch 1572 loss: 2.8417229652404785\n",
      "epoch 1 batch 1573 loss: 2.741084098815918\n",
      "epoch 1 batch 1574 loss: 3.124629020690918\n",
      "epoch 1 batch 1575 loss: 2.76204776763916\n",
      "epoch 1 batch 1576 loss: 2.7384984493255615\n",
      "epoch 1 batch 1577 loss: 2.5646846294403076\n",
      "epoch 1 batch 1578 loss: 3.0509862899780273\n",
      "epoch 1 batch 1579 loss: 2.7229018211364746\n",
      "epoch 1 batch 1580 loss: 2.791590690612793\n",
      "epoch 1 batch 1581 loss: 2.765608310699463\n",
      "epoch 1 batch 1582 loss: 2.8982717990875244\n",
      "epoch 1 batch 1583 loss: 3.0799031257629395\n",
      "epoch 1 batch 1584 loss: 2.348742961883545\n",
      "epoch 1 batch 1585 loss: 2.600015163421631\n",
      "epoch 1 batch 1586 loss: 2.673618793487549\n",
      "epoch 1 batch 1587 loss: 2.4789938926696777\n",
      "epoch 1 batch 1588 loss: 2.947235584259033\n",
      "epoch 1 batch 1589 loss: 2.95674729347229\n",
      "epoch 1 batch 1590 loss: 2.606682300567627\n",
      "epoch 1 batch 1591 loss: 2.7559256553649902\n",
      "epoch 1 batch 1592 loss: 2.8056154251098633\n",
      "epoch 1 batch 1593 loss: 2.813889741897583\n",
      "epoch 1 batch 1594 loss: 2.7496402263641357\n",
      "epoch 1 batch 1595 loss: 2.6009957790374756\n",
      "epoch 1 batch 1596 loss: 2.636969566345215\n",
      "epoch 1 batch 1597 loss: 2.8026957511901855\n",
      "epoch 1 batch 1598 loss: 2.767690896987915\n",
      "epoch 1 batch 1599 loss: 2.812544107437134\n",
      "epoch 1 batch 1600 loss: 2.9231719970703125\n",
      "epoch 1 batch 1601 loss: 2.853423595428467\n",
      "epoch 1 batch 1602 loss: 2.979055881500244\n",
      "epoch 1 batch 1603 loss: 2.9584033489227295\n",
      "epoch 1 batch 1604 loss: 2.662477493286133\n",
      "epoch 1 batch 1605 loss: 2.917961835861206\n",
      "epoch 1 batch 1606 loss: 2.5256614685058594\n",
      "epoch 1 batch 1607 loss: 2.3976054191589355\n",
      "epoch 1 batch 1608 loss: 2.903545379638672\n",
      "epoch 1 batch 1609 loss: 2.922421932220459\n",
      "epoch 1 batch 1610 loss: 2.755117416381836\n",
      "epoch 1 batch 1611 loss: 2.4264440536499023\n",
      "epoch 1 batch 1612 loss: 2.6430110931396484\n",
      "epoch 1 batch 1613 loss: 2.8056750297546387\n",
      "epoch 1 batch 1614 loss: 2.49746036529541\n",
      "epoch 1 batch 1615 loss: 3.1248397827148438\n",
      "epoch 1 batch 1616 loss: 2.762220621109009\n",
      "epoch 1 batch 1617 loss: 2.745185613632202\n",
      "epoch 1 batch 1618 loss: 2.8045713901519775\n",
      "epoch 1 batch 1619 loss: 2.469860076904297\n",
      "epoch 1 batch 1620 loss: 2.536576747894287\n",
      "epoch 1 batch 1621 loss: 3.026761531829834\n",
      "epoch 1 batch 1622 loss: 2.7195630073547363\n",
      "epoch 1 batch 1623 loss: 2.6784729957580566\n",
      "epoch 1 batch 1624 loss: 2.8026113510131836\n",
      "epoch 1 batch 1625 loss: 2.7713499069213867\n",
      "epoch 1 batch 1626 loss: 2.5021393299102783\n",
      "epoch 1 batch 1627 loss: 3.0386922359466553\n",
      "epoch 1 batch 1628 loss: 2.790809154510498\n",
      "epoch 1 batch 1629 loss: 2.924450397491455\n",
      "epoch 1 batch 1630 loss: 2.62111234664917\n",
      "epoch 1 batch 1631 loss: 3.0380709171295166\n",
      "epoch 1 batch 1632 loss: 2.616612672805786\n",
      "epoch 1 batch 1633 loss: 2.5021066665649414\n",
      "epoch 1 batch 1634 loss: 2.4971559047698975\n",
      "epoch 1 batch 1635 loss: 2.58211612701416\n",
      "epoch 1 batch 1636 loss: 3.29655122756958\n",
      "epoch 1 batch 1637 loss: 2.6171417236328125\n",
      "epoch 1 batch 1638 loss: 2.9136643409729004\n",
      "epoch 1 batch 1639 loss: 2.8033220767974854\n",
      "epoch 1 batch 1640 loss: 2.626992702484131\n",
      "epoch 1 batch 1641 loss: 2.392990827560425\n",
      "epoch 1 batch 1642 loss: 2.8347830772399902\n",
      "epoch 1 batch 1643 loss: 2.6780731678009033\n",
      "epoch 1 batch 1644 loss: 2.9148168563842773\n",
      "epoch 1 batch 1645 loss: 2.980520486831665\n",
      "epoch 1 batch 1646 loss: 3.163119077682495\n",
      "epoch 1 batch 1647 loss: 2.8416635990142822\n",
      "epoch 1 batch 1648 loss: 2.7697362899780273\n",
      "epoch 1 batch 1649 loss: 2.4385993480682373\n",
      "epoch 1 batch 1650 loss: 2.98414945602417\n",
      "epoch 1 batch 1651 loss: 2.7126920223236084\n",
      "epoch 1 batch 1652 loss: 2.538848400115967\n",
      "epoch 1 batch 1653 loss: 2.841630458831787\n",
      "epoch 1 batch 1654 loss: 2.6914188861846924\n",
      "epoch 1 batch 1655 loss: 2.4770007133483887\n",
      "epoch 1 batch 1656 loss: 2.7580668926239014\n",
      "epoch 1 batch 1657 loss: 2.763908863067627\n",
      "epoch 1 batch 1658 loss: 2.8480935096740723\n",
      "epoch 1 batch 1659 loss: 2.7907919883728027\n",
      "epoch 1 batch 1660 loss: 2.456505060195923\n",
      "epoch 1 batch 1661 loss: 2.8838627338409424\n",
      "epoch 1 batch 1662 loss: 2.7702178955078125\n",
      "epoch 1 batch 1663 loss: 2.6397507190704346\n",
      "epoch 1 batch 1664 loss: 2.4270529747009277\n",
      "epoch 1 batch 1665 loss: 2.579972505569458\n",
      "epoch 1 batch 1666 loss: 2.712320327758789\n",
      "epoch 1 batch 1667 loss: 2.5833263397216797\n",
      "epoch 1 batch 1668 loss: 2.6314857006073\n",
      "epoch 1 batch 1669 loss: 2.849289894104004\n",
      "epoch 1 batch 1670 loss: 2.579430103302002\n",
      "epoch 1 batch 1671 loss: 2.796872138977051\n",
      "epoch 1 batch 1672 loss: 2.4697864055633545\n",
      "epoch 1 batch 1673 loss: 2.8310272693634033\n",
      "epoch 1 batch 1674 loss: 2.9379472732543945\n",
      "epoch 1 batch 1675 loss: 2.8202407360076904\n",
      "epoch 1 batch 1676 loss: 3.2671611309051514\n",
      "epoch 1 batch 1677 loss: 2.9358129501342773\n",
      "epoch 1 batch 1678 loss: 2.6938064098358154\n",
      "epoch 1 batch 1679 loss: 2.832613706588745\n",
      "epoch 1 batch 1680 loss: 2.6187641620635986\n",
      "epoch 1 batch 1681 loss: 2.5823559761047363\n",
      "epoch 1 batch 1682 loss: 2.5381686687469482\n",
      "epoch 1 batch 1683 loss: 2.512575387954712\n",
      "epoch 1 batch 1684 loss: 2.8064732551574707\n",
      "epoch 1 batch 1685 loss: 2.5207788944244385\n",
      "epoch 1 batch 1686 loss: 3.149656295776367\n",
      "epoch 1 batch 1687 loss: 2.891376256942749\n",
      "epoch 1 batch 1688 loss: 2.760587692260742\n",
      "epoch 1 batch 1689 loss: 2.3769586086273193\n",
      "epoch 1 batch 1690 loss: 2.6539673805236816\n",
      "epoch 1 batch 1691 loss: 2.45845365524292\n",
      "epoch 1 batch 1692 loss: 2.469888687133789\n",
      "epoch 1 batch 1693 loss: 2.9010558128356934\n",
      "epoch 1 batch 1694 loss: 2.5264973640441895\n",
      "epoch 1 batch 1695 loss: 3.0739450454711914\n",
      "epoch 1 batch 1696 loss: 2.720960855484009\n",
      "epoch 1 batch 1697 loss: 2.6629395484924316\n",
      "epoch 1 batch 1698 loss: 2.6758532524108887\n",
      "epoch 1 batch 1699 loss: 3.3519976139068604\n",
      "epoch 1 batch 1700 loss: 3.4523143768310547\n",
      "epoch 1 batch 1701 loss: 2.982452392578125\n",
      "epoch 1 batch 1702 loss: 2.71221923828125\n",
      "epoch 1 batch 1703 loss: 2.897613286972046\n",
      "epoch 1 batch 1704 loss: 3.0244998931884766\n",
      "epoch 1 batch 1705 loss: 3.400526523590088\n",
      "epoch 1 batch 1706 loss: 2.6204676628112793\n",
      "epoch 1 batch 1707 loss: 2.572441577911377\n",
      "epoch 1 batch 1708 loss: 2.5591890811920166\n",
      "epoch 1 batch 1709 loss: 3.002669095993042\n",
      "epoch 1 batch 1710 loss: 2.9806365966796875\n",
      "epoch 1 batch 1711 loss: 2.775519847869873\n",
      "epoch 1 batch 1712 loss: 2.910315990447998\n",
      "epoch 1 batch 1713 loss: 2.5855231285095215\n",
      "epoch 1 batch 1714 loss: 2.6551504135131836\n",
      "epoch 1 batch 1715 loss: 2.326262950897217\n",
      "epoch 1 batch 1716 loss: 2.587275266647339\n",
      "epoch 1 batch 1717 loss: 2.668262481689453\n",
      "epoch 1 batch 1718 loss: 2.9720535278320312\n",
      "epoch 1 batch 1719 loss: 2.691317558288574\n",
      "epoch 1 batch 1720 loss: 2.463006019592285\n",
      "epoch 1 batch 1721 loss: 2.7868804931640625\n",
      "epoch 1 batch 1722 loss: 2.4844348430633545\n",
      "epoch 1 batch 1723 loss: 2.9431376457214355\n",
      "epoch 1 batch 1724 loss: 2.53249454498291\n",
      "epoch 1 batch 1725 loss: 2.57649564743042\n",
      "epoch 1 batch 1726 loss: 2.578242778778076\n",
      "epoch 1 batch 1727 loss: 3.1798791885375977\n",
      "epoch 1 batch 1728 loss: 2.786407470703125\n",
      "epoch 1 batch 1729 loss: 2.483544111251831\n",
      "epoch 1 batch 1730 loss: 2.693969488143921\n",
      "epoch 1 batch 1731 loss: 2.541452407836914\n",
      "epoch 1 batch 1732 loss: 2.7083630561828613\n",
      "epoch 1 batch 1733 loss: 3.023721933364868\n",
      "epoch 1 batch 1734 loss: 2.487475872039795\n",
      "epoch 1 batch 1735 loss: 3.0916097164154053\n",
      "epoch 1 batch 1736 loss: 2.4813578128814697\n",
      "epoch 1 batch 1737 loss: 2.611264228820801\n",
      "epoch 1 batch 1738 loss: 2.9199728965759277\n",
      "epoch 1 batch 1739 loss: 2.5246081352233887\n",
      "epoch 1 batch 1740 loss: 3.032741069793701\n",
      "epoch 1 batch 1741 loss: 3.2014920711517334\n",
      "epoch 1 batch 1742 loss: 2.6825242042541504\n",
      "epoch 1 batch 1743 loss: 2.60416841506958\n",
      "epoch 1 batch 1744 loss: 2.9013900756835938\n",
      "epoch 1 batch 1745 loss: 2.874403238296509\n",
      "epoch 1 batch 1746 loss: 2.7255477905273438\n",
      "epoch 1 batch 1747 loss: 2.5830419063568115\n",
      "epoch 1 batch 1748 loss: 2.762105941772461\n",
      "epoch 1 batch 1749 loss: 2.710481643676758\n",
      "epoch 1 batch 1750 loss: 3.0201950073242188\n",
      "epoch 1 batch 1751 loss: 2.552340269088745\n",
      "epoch 1 batch 1752 loss: 2.6573007106781006\n",
      "epoch 1 batch 1753 loss: 2.451043128967285\n",
      "epoch 1 batch 1754 loss: 2.957033634185791\n",
      "epoch 1 batch 1755 loss: 2.3078408241271973\n",
      "epoch 1 batch 1756 loss: 3.0106005668640137\n",
      "epoch 1 batch 1757 loss: 2.53902006149292\n",
      "epoch 1 batch 1758 loss: 2.5691280364990234\n",
      "epoch 1 batch 1759 loss: 2.733717918395996\n",
      "epoch 1 batch 1760 loss: 2.7387969493865967\n",
      "epoch 1 batch 1761 loss: 2.6324307918548584\n",
      "epoch 1 batch 1762 loss: 2.6951851844787598\n",
      "epoch 1 batch 1763 loss: 2.5126700401306152\n",
      "epoch 1 batch 1764 loss: 2.4945478439331055\n",
      "epoch 1 batch 1765 loss: 2.616849899291992\n",
      "epoch 1 batch 1766 loss: 2.7459750175476074\n",
      "epoch 1 batch 1767 loss: 2.729949951171875\n",
      "epoch 1 batch 1768 loss: 2.653343677520752\n",
      "epoch 1 batch 1769 loss: 2.378046989440918\n",
      "epoch 1 batch 1770 loss: 2.931464672088623\n",
      "epoch 1 batch 1771 loss: 2.873821973800659\n",
      "epoch 1 batch 1772 loss: 2.389941692352295\n",
      "epoch 1 batch 1773 loss: 2.4207968711853027\n",
      "epoch 1 batch 1774 loss: 2.6730446815490723\n",
      "epoch 1 batch 1775 loss: 2.91237211227417\n",
      "epoch 1 batch 1776 loss: 2.6302413940429688\n",
      "epoch 1 batch 1777 loss: 2.8970398902893066\n",
      "epoch 1 batch 1778 loss: 2.4716243743896484\n",
      "epoch 1 batch 1779 loss: 2.5457041263580322\n",
      "epoch 1 batch 1780 loss: 3.265068531036377\n",
      "epoch 1 batch 1781 loss: 2.818570613861084\n",
      "epoch 1 batch 1782 loss: 2.672391891479492\n",
      "epoch 1 batch 1783 loss: 2.4725592136383057\n",
      "epoch 1 batch 1784 loss: 2.683332920074463\n",
      "epoch 1 batch 1785 loss: 2.46087646484375\n",
      "epoch 1 batch 1786 loss: 2.619825839996338\n",
      "epoch 1 batch 1787 loss: 2.7765731811523438\n",
      "epoch 1 batch 1788 loss: 2.4348788261413574\n",
      "epoch 1 batch 1789 loss: 2.7245590686798096\n",
      "epoch 1 batch 1790 loss: 2.6192331314086914\n",
      "epoch 1 batch 1791 loss: 2.703782081604004\n",
      "epoch 1 batch 1792 loss: 2.750126361846924\n",
      "epoch 1 batch 1793 loss: 2.540088653564453\n",
      "epoch 1 batch 1794 loss: 2.564561605453491\n",
      "epoch 1 batch 1795 loss: 2.547095775604248\n",
      "epoch 1 batch 1796 loss: 3.0808916091918945\n",
      "epoch 1 batch 1797 loss: 2.6134543418884277\n",
      "epoch 1 batch 1798 loss: 2.5457777976989746\n",
      "epoch 1 batch 1799 loss: 2.678987503051758\n",
      "epoch 1 batch 1800 loss: 2.7245798110961914\n",
      "epoch 1 batch 1801 loss: 2.932225227355957\n",
      "epoch 1 batch 1802 loss: 2.611877202987671\n",
      "epoch 1 batch 1803 loss: 2.494353771209717\n",
      "epoch 1 batch 1804 loss: 2.790947437286377\n",
      "epoch 1 batch 1805 loss: 3.144458770751953\n",
      "epoch 1 batch 1806 loss: 2.761108875274658\n",
      "epoch 1 batch 1807 loss: 2.7712082862854004\n",
      "epoch 1 batch 1808 loss: 2.3158726692199707\n",
      "epoch 1 batch 1809 loss: 2.6414408683776855\n",
      "epoch 1 batch 1810 loss: 2.5328781604766846\n",
      "epoch 1 batch 1811 loss: 3.0960376262664795\n",
      "epoch 1 batch 1812 loss: 2.483248710632324\n",
      "epoch 1 batch 1813 loss: 3.0330886840820312\n",
      "epoch 1 batch 1814 loss: 2.4243526458740234\n",
      "epoch 1 batch 1815 loss: 2.9328532218933105\n",
      "epoch 1 batch 1816 loss: 2.2876172065734863\n",
      "epoch 1 batch 1817 loss: 2.7421650886535645\n",
      "epoch 1 batch 1818 loss: 2.5803918838500977\n",
      "epoch 1 batch 1819 loss: 3.2161335945129395\n",
      "epoch 1 batch 1820 loss: 2.662945508956909\n",
      "epoch 1 batch 1821 loss: 2.846318244934082\n",
      "epoch 1 batch 1822 loss: 3.066514730453491\n",
      "epoch 1 batch 1823 loss: 2.646921396255493\n",
      "epoch 1 batch 1824 loss: 2.811500310897827\n",
      "epoch 1 batch 1825 loss: 2.8415167331695557\n",
      "epoch 1 batch 1826 loss: 2.8117423057556152\n",
      "epoch 1 batch 1827 loss: 2.738508701324463\n",
      "epoch 1 batch 1828 loss: 2.4728610515594482\n",
      "epoch 1 batch 1829 loss: 2.497742176055908\n",
      "epoch 1 batch 1830 loss: 2.610713481903076\n",
      "epoch 1 batch 1831 loss: 2.4910380840301514\n",
      "epoch 1 batch 1832 loss: 2.502864360809326\n",
      "epoch 1 batch 1833 loss: 2.8095641136169434\n",
      "epoch 1 batch 1834 loss: 2.692448377609253\n",
      "epoch 1 batch 1835 loss: 2.7372426986694336\n",
      "epoch 1 batch 1836 loss: 2.6115798950195312\n",
      "epoch 1 batch 1837 loss: 2.7569878101348877\n",
      "epoch 1 batch 1838 loss: 2.615762710571289\n",
      "epoch 1 batch 1839 loss: 2.62892484664917\n",
      "epoch 1 batch 1840 loss: 2.958854913711548\n",
      "epoch 1 batch 1841 loss: 2.4902029037475586\n",
      "epoch 1 batch 1842 loss: 3.0135598182678223\n",
      "epoch 1 batch 1843 loss: 2.4224910736083984\n",
      "epoch 1 batch 1844 loss: 2.802927017211914\n",
      "epoch 1 batch 1845 loss: 2.4822449684143066\n",
      "epoch 1 batch 1846 loss: 2.4855682849884033\n",
      "epoch 1 batch 1847 loss: 2.7440876960754395\n",
      "epoch 1 batch 1848 loss: 2.9127864837646484\n",
      "epoch 1 batch 1849 loss: 2.9620585441589355\n",
      "epoch 1 batch 1850 loss: 2.5672619342803955\n",
      "epoch 1 batch 1851 loss: 2.6426312923431396\n",
      "epoch 1 batch 1852 loss: 2.7561564445495605\n",
      "epoch 1 batch 1853 loss: 2.6322948932647705\n",
      "epoch 1 batch 1854 loss: 2.2241530418395996\n",
      "epoch 1 batch 1855 loss: 2.7937827110290527\n",
      "epoch 1 batch 1856 loss: 2.9707388877868652\n",
      "epoch 1 batch 1857 loss: 2.9073095321655273\n",
      "epoch 1 batch 1858 loss: 2.861816883087158\n",
      "epoch 1 batch 1859 loss: 2.8003296852111816\n",
      "epoch 1 batch 1860 loss: 2.8333003520965576\n",
      "epoch 1 batch 1861 loss: 3.277233600616455\n",
      "epoch 1 batch 1862 loss: 2.4776229858398438\n",
      "epoch 1 batch 1863 loss: 2.888416051864624\n",
      "epoch 1 batch 1864 loss: 2.88802170753479\n",
      "epoch 1 batch 1865 loss: 2.6274139881134033\n",
      "epoch 1 batch 1866 loss: 2.6503384113311768\n",
      "epoch 1 batch 1867 loss: 2.9249820709228516\n",
      "epoch 1 batch 1868 loss: 2.6173958778381348\n",
      "epoch 1 batch 1869 loss: 2.9349992275238037\n",
      "epoch 1 batch 1870 loss: 2.635037422180176\n",
      "epoch 1 batch 1871 loss: 2.5029261112213135\n",
      "epoch 1 batch 1872 loss: 2.6331281661987305\n",
      "epoch 1 batch 1873 loss: 2.6162405014038086\n",
      "epoch 1 batch 1874 loss: 2.756314277648926\n",
      "epoch 1 batch 1875 loss: 2.802255630493164\n",
      "epoch 1 batch 1876 loss: 2.6971378326416016\n",
      "epoch 1 batch 1877 loss: 2.509226083755493\n",
      "epoch 1 batch 1878 loss: 3.030724048614502\n",
      "epoch 1 batch 1879 loss: 2.404750347137451\n",
      "epoch 1 batch 1880 loss: 3.002444267272949\n",
      "epoch 1 batch 1881 loss: 2.9508633613586426\n",
      "epoch 1 batch 1882 loss: 2.9346466064453125\n",
      "epoch 1 batch 1883 loss: 2.305540084838867\n",
      "epoch 1 batch 1884 loss: 3.071559429168701\n",
      "epoch 1 batch 1885 loss: 2.9026191234588623\n",
      "epoch 1 batch 1886 loss: 2.9203007221221924\n",
      "epoch 1 batch 1887 loss: 2.618709087371826\n",
      "epoch 1 batch 1888 loss: 2.4632883071899414\n",
      "epoch 1 batch 1889 loss: 3.0097599029541016\n",
      "epoch 1 batch 1890 loss: 2.7481000423431396\n",
      "epoch 1 batch 1891 loss: 2.7369282245635986\n",
      "epoch 1 batch 1892 loss: 2.8738107681274414\n",
      "epoch 1 batch 1893 loss: 2.321357488632202\n",
      "epoch 1 batch 1894 loss: 2.522362232208252\n",
      "epoch 1 batch 1895 loss: 2.4027137756347656\n",
      "epoch 1 batch 1896 loss: 2.9847607612609863\n",
      "epoch 1 batch 1897 loss: 2.506680727005005\n",
      "epoch 1 batch 1898 loss: 2.2856853008270264\n",
      "epoch 1 batch 1899 loss: 2.6884002685546875\n",
      "epoch 1 batch 1900 loss: 2.626211643218994\n",
      "epoch 1 batch 1901 loss: 3.1752614974975586\n",
      "epoch 1 batch 1902 loss: 2.9730639457702637\n",
      "epoch 1 batch 1903 loss: 2.930422067642212\n",
      "epoch 1 batch 1904 loss: 2.458205223083496\n",
      "epoch 1 batch 1905 loss: 2.6009554862976074\n",
      "epoch 1 batch 1906 loss: 2.431227922439575\n",
      "epoch 1 batch 1907 loss: 2.4666781425476074\n",
      "epoch 1 batch 1908 loss: 2.9922733306884766\n",
      "epoch 1 batch 1909 loss: 2.702587366104126\n",
      "epoch 1 batch 1910 loss: 2.3095483779907227\n",
      "epoch 1 batch 1911 loss: 2.553945302963257\n",
      "epoch 1 batch 1912 loss: 2.964756488800049\n",
      "epoch 1 batch 1913 loss: 2.636272668838501\n",
      "epoch 1 batch 1914 loss: 2.647036552429199\n",
      "epoch 1 batch 1915 loss: 2.7591724395751953\n",
      "epoch 1 batch 1916 loss: 2.639768600463867\n",
      "epoch 1 batch 1917 loss: 2.7160305976867676\n",
      "epoch 1 batch 1918 loss: 2.6828513145446777\n",
      "epoch 1 batch 1919 loss: 2.5351903438568115\n",
      "epoch 1 batch 1920 loss: 2.6469240188598633\n",
      "epoch 1 batch 1921 loss: 2.828085422515869\n",
      "epoch 1 batch 1922 loss: 2.7081761360168457\n",
      "epoch 1 batch 1923 loss: 2.3377091884613037\n",
      "epoch 1 batch 1924 loss: 2.2855238914489746\n",
      "epoch 1 batch 1925 loss: 2.344667673110962\n",
      "epoch 1 batch 1926 loss: 2.7099061012268066\n",
      "epoch 1 batch 1927 loss: 2.402503490447998\n",
      "epoch 1 batch 1928 loss: 3.1470694541931152\n",
      "epoch 1 batch 1929 loss: 2.5595195293426514\n",
      "epoch 1 batch 1930 loss: 2.691615104675293\n",
      "epoch 1 batch 1931 loss: 2.2521719932556152\n",
      "epoch 1 batch 1932 loss: 3.0328316688537598\n",
      "epoch 1 batch 1933 loss: 2.855177164077759\n",
      "epoch 1 batch 1934 loss: 2.9962329864501953\n",
      "epoch 1 batch 1935 loss: 2.448585271835327\n",
      "epoch 1 batch 1936 loss: 2.641016960144043\n",
      "epoch 1 batch 1937 loss: 2.905703067779541\n",
      "epoch 1 batch 1938 loss: 2.5998363494873047\n",
      "epoch 1 batch 1939 loss: 2.705982208251953\n",
      "epoch 1 batch 1940 loss: 2.970907688140869\n",
      "epoch 1 batch 1941 loss: 2.899900436401367\n",
      "epoch 1 batch 1942 loss: 2.7776687145233154\n",
      "epoch 1 batch 1943 loss: 2.506397247314453\n",
      "epoch 1 batch 1944 loss: 2.515244483947754\n",
      "epoch 1 batch 1945 loss: 2.6518564224243164\n",
      "epoch 1 batch 1946 loss: 2.7391905784606934\n",
      "epoch 1 batch 1947 loss: 2.7420101165771484\n",
      "epoch 1 batch 1948 loss: 2.631368637084961\n",
      "epoch 1 batch 1949 loss: 2.5849668979644775\n",
      "epoch 1 batch 1950 loss: 2.5933501720428467\n",
      "epoch 1 batch 1951 loss: 2.687420129776001\n",
      "epoch 1 batch 1952 loss: 2.7430882453918457\n",
      "epoch 1 batch 1953 loss: 2.746269702911377\n",
      "epoch 1 batch 1954 loss: 2.608133554458618\n",
      "epoch 1 batch 1955 loss: 2.9192819595336914\n",
      "epoch 1 batch 1956 loss: 2.88969349861145\n",
      "epoch 1 batch 1957 loss: 2.862576961517334\n",
      "epoch 1 batch 1958 loss: 2.865163803100586\n",
      "epoch 1 batch 1959 loss: 2.423102855682373\n",
      "epoch 1 batch 1960 loss: 2.6288139820098877\n",
      "epoch 1 batch 1961 loss: 2.7853636741638184\n",
      "epoch 1 batch 1962 loss: 2.7009329795837402\n",
      "epoch 1 batch 1963 loss: 2.484915018081665\n",
      "epoch 1 batch 1964 loss: 3.0243663787841797\n",
      "epoch 1 batch 1965 loss: 2.560850143432617\n",
      "epoch 1 batch 1966 loss: 3.1103410720825195\n",
      "epoch 1 batch 1967 loss: 2.517261266708374\n",
      "epoch 1 batch 1968 loss: 2.736377000808716\n",
      "epoch 1 batch 1969 loss: 2.7580294609069824\n",
      "epoch 1 batch 1970 loss: 2.6662397384643555\n",
      "epoch 1 batch 1971 loss: 2.82588267326355\n",
      "epoch 1 batch 1972 loss: 3.0795018672943115\n",
      "epoch 1 batch 1973 loss: 2.6583704948425293\n",
      "epoch 1 batch 1974 loss: 2.6936774253845215\n",
      "epoch 1 batch 1975 loss: 2.7168588638305664\n",
      "epoch 1 batch 1976 loss: 2.800535202026367\n",
      "epoch 1 batch 1977 loss: 2.6353559494018555\n",
      "epoch 1 batch 1978 loss: 2.9629766941070557\n",
      "epoch 1 batch 1979 loss: 2.5090248584747314\n",
      "epoch 1 batch 1980 loss: 3.120173931121826\n",
      "epoch 1 batch 1981 loss: 2.8274781703948975\n",
      "epoch 1 batch 1982 loss: 2.8478527069091797\n",
      "epoch 1 batch 1983 loss: 2.4224886894226074\n",
      "epoch 1 batch 1984 loss: 2.7046310901641846\n",
      "epoch 1 batch 1985 loss: 2.997844696044922\n",
      "epoch 1 batch 1986 loss: 2.6178805828094482\n",
      "epoch 1 batch 1987 loss: 2.7184696197509766\n",
      "epoch 1 batch 1988 loss: 2.781892776489258\n",
      "epoch 1 batch 1989 loss: 2.8719451427459717\n",
      "epoch 1 batch 1990 loss: 2.7457752227783203\n",
      "epoch 1 batch 1991 loss: 2.5306410789489746\n",
      "epoch 1 batch 1992 loss: 2.4767818450927734\n",
      "epoch 1 batch 1993 loss: 2.8000199794769287\n",
      "epoch 1 batch 1994 loss: 2.2853140830993652\n",
      "epoch 1 batch 1995 loss: 2.900621175765991\n",
      "epoch 1 batch 1996 loss: 2.5390625\n",
      "epoch 1 batch 1997 loss: 2.9244699478149414\n",
      "epoch 1 batch 1998 loss: 2.6333107948303223\n",
      "epoch 1 batch 1999 loss: 2.634868621826172\n",
      "epoch 1 batch 2000 loss: 2.7528300285339355\n",
      "epoch 1 batch 2001 loss: 2.4195199012756348\n",
      "epoch 1 batch 2002 loss: 2.4204297065734863\n",
      "epoch 1 batch 2003 loss: 2.6156516075134277\n",
      "epoch 1 batch 2004 loss: 2.632493495941162\n",
      "epoch 1 batch 2005 loss: 2.5838818550109863\n",
      "epoch 1 batch 2006 loss: 2.9003183841705322\n",
      "epoch 1 batch 2007 loss: 2.498798370361328\n",
      "epoch 1 batch 2008 loss: 2.3643460273742676\n",
      "epoch 1 batch 2009 loss: 2.7817320823669434\n",
      "epoch 1 batch 2010 loss: 2.7251179218292236\n",
      "epoch 1 batch 2011 loss: 2.6866583824157715\n",
      "epoch 1 batch 2012 loss: 2.4056589603424072\n",
      "epoch 1 batch 2013 loss: 3.173001527786255\n",
      "epoch 1 batch 2014 loss: 2.8634400367736816\n",
      "epoch 1 batch 2015 loss: 2.793278694152832\n",
      "epoch 1 batch 2016 loss: 2.7307827472686768\n",
      "epoch 1 batch 2017 loss: 2.615400791168213\n",
      "epoch 1 batch 2018 loss: 2.5487499237060547\n",
      "epoch 1 batch 2019 loss: 2.621290683746338\n",
      "epoch 1 batch 2020 loss: 2.791576862335205\n",
      "epoch 1 batch 2021 loss: 2.7210564613342285\n",
      "epoch 1 batch 2022 loss: 2.682180166244507\n",
      "epoch 1 batch 2023 loss: 2.2621383666992188\n",
      "epoch 1 batch 2024 loss: 2.5272576808929443\n",
      "epoch 1 batch 2025 loss: 2.831707000732422\n",
      "epoch 1 batch 2026 loss: 2.984877586364746\n",
      "epoch 1 batch 2027 loss: 2.489161491394043\n",
      "epoch 1 batch 2028 loss: 2.5486416816711426\n",
      "epoch 1 batch 2029 loss: 2.460678815841675\n",
      "epoch 1 batch 2030 loss: 2.4604668617248535\n",
      "epoch 1 batch 2031 loss: 2.7678608894348145\n",
      "epoch 1 batch 2032 loss: 2.580841302871704\n",
      "epoch 1 batch 2033 loss: 2.537317991256714\n",
      "epoch 1 batch 2034 loss: 2.519937038421631\n",
      "epoch 1 batch 2035 loss: 2.790142774581909\n",
      "epoch 1 batch 2036 loss: 2.2957663536071777\n",
      "epoch 1 batch 2037 loss: 2.838843822479248\n",
      "epoch 1 batch 2038 loss: 2.67549991607666\n",
      "epoch 1 batch 2039 loss: 2.388261318206787\n",
      "epoch 1 batch 2040 loss: 3.046222448348999\n",
      "epoch 1 batch 2041 loss: 2.4831857681274414\n",
      "epoch 1 batch 2042 loss: 2.687812089920044\n",
      "epoch 1 batch 2043 loss: 2.5129506587982178\n",
      "epoch 1 batch 2044 loss: 2.4348878860473633\n",
      "epoch 1 batch 2045 loss: 2.5027811527252197\n",
      "epoch 1 batch 2046 loss: 3.04341721534729\n",
      "epoch 1 batch 2047 loss: 2.463124990463257\n",
      "epoch 1 batch 2048 loss: 2.6765646934509277\n",
      "epoch 1 batch 2049 loss: 2.5655016899108887\n",
      "epoch 1 batch 2050 loss: 2.4429850578308105\n",
      "epoch 1 batch 2051 loss: 2.9738779067993164\n",
      "epoch 1 batch 2052 loss: 2.7981271743774414\n",
      "epoch 1 batch 2053 loss: 2.5501089096069336\n",
      "epoch 1 batch 2054 loss: 2.621614933013916\n",
      "epoch 1 batch 2055 loss: 2.7522215843200684\n",
      "epoch 1 batch 2056 loss: 2.4790658950805664\n",
      "epoch 1 batch 2057 loss: 2.4842724800109863\n",
      "epoch 1 batch 2058 loss: 2.706209659576416\n",
      "epoch 1 batch 2059 loss: 2.82781720161438\n",
      "epoch 1 batch 2060 loss: 2.6142964363098145\n",
      "epoch 1 batch 2061 loss: 2.9110069274902344\n",
      "epoch 1 batch 2062 loss: 2.383120059967041\n",
      "epoch 1 batch 2063 loss: 3.0096588134765625\n",
      "epoch 1 batch 2064 loss: 2.806839942932129\n",
      "epoch 1 batch 2065 loss: 2.6978182792663574\n",
      "epoch 1 batch 2066 loss: 2.38535213470459\n",
      "epoch 1 batch 2067 loss: 2.6180830001831055\n",
      "epoch 1 batch 2068 loss: 2.7379021644592285\n",
      "epoch 1 batch 2069 loss: 2.781466007232666\n",
      "epoch 1 batch 2070 loss: 2.816049337387085\n",
      "epoch 1 batch 2071 loss: 2.700751781463623\n",
      "epoch 1 batch 2072 loss: 2.922837018966675\n",
      "epoch 1 batch 2073 loss: 2.4420902729034424\n",
      "epoch 1 batch 2074 loss: 2.468440532684326\n",
      "epoch 1 batch 2075 loss: 3.0337700843811035\n",
      "epoch 1 batch 2076 loss: 2.5387678146362305\n",
      "epoch 1 batch 2077 loss: 2.5060994625091553\n",
      "epoch 1 batch 2078 loss: 2.2818069458007812\n",
      "epoch 1 batch 2079 loss: 2.703824996948242\n",
      "epoch 1 batch 2080 loss: 2.80204701423645\n",
      "epoch 1 batch 2081 loss: 2.583517074584961\n",
      "epoch 1 batch 2082 loss: 2.5639638900756836\n",
      "epoch 1 batch 2083 loss: 2.6379880905151367\n",
      "epoch 1 batch 2084 loss: 3.2455573081970215\n",
      "epoch 1 batch 2085 loss: 3.2109827995300293\n",
      "epoch 1 batch 2086 loss: 2.537297248840332\n",
      "epoch 1 batch 2087 loss: 2.7855188846588135\n",
      "epoch 1 batch 2088 loss: 3.0374910831451416\n",
      "epoch 1 batch 2089 loss: 2.4769110679626465\n",
      "epoch 1 batch 2090 loss: 2.5718588829040527\n",
      "epoch 1 batch 2091 loss: 2.6985177993774414\n",
      "epoch 1 batch 2092 loss: 2.786113739013672\n",
      "epoch 1 batch 2093 loss: 2.5232441425323486\n",
      "epoch 1 batch 2094 loss: 2.612481117248535\n",
      "epoch 1 batch 2095 loss: 2.5072836875915527\n",
      "epoch 1 batch 2096 loss: 2.6292028427124023\n",
      "epoch 1 batch 2097 loss: 3.0097270011901855\n",
      "epoch 1 batch 2098 loss: 2.683065176010132\n",
      "epoch 1 batch 2099 loss: 2.7960009574890137\n",
      "epoch 1 batch 2100 loss: 2.9735922813415527\n",
      "epoch 1 batch 2101 loss: 2.4668068885803223\n",
      "epoch 1 batch 2102 loss: 2.6636712551116943\n",
      "epoch 1 batch 2103 loss: 2.6353635787963867\n",
      "epoch 1 batch 2104 loss: 3.1671082973480225\n",
      "epoch 1 batch 2105 loss: 2.5894739627838135\n",
      "epoch 1 batch 2106 loss: 2.8986945152282715\n",
      "epoch 1 batch 2107 loss: 2.657733917236328\n",
      "epoch 1 batch 2108 loss: 2.531370162963867\n",
      "epoch 1 batch 2109 loss: 2.446470022201538\n",
      "epoch 1 batch 2110 loss: 2.478224515914917\n",
      "epoch 1 batch 2111 loss: 3.0204379558563232\n",
      "epoch 1 batch 2112 loss: 2.42120361328125\n",
      "epoch 1 batch 2113 loss: 2.7767276763916016\n",
      "epoch 1 batch 2114 loss: 2.5840628147125244\n",
      "epoch 1 batch 2115 loss: 2.6535749435424805\n",
      "epoch 1 batch 2116 loss: 2.5434484481811523\n",
      "epoch 1 batch 2117 loss: 2.575188636779785\n",
      "epoch 1 batch 2118 loss: 2.6836466789245605\n",
      "epoch 1 batch 2119 loss: 2.8766417503356934\n",
      "epoch 1 batch 2120 loss: 2.5580577850341797\n",
      "epoch 1 batch 2121 loss: 2.8746089935302734\n",
      "epoch 1 batch 2122 loss: 2.5611560344696045\n",
      "epoch 1 batch 2123 loss: 2.8721935749053955\n",
      "epoch 1 batch 2124 loss: 2.9749577045440674\n",
      "epoch 1 batch 2125 loss: 2.651336193084717\n",
      "epoch 1 batch 2126 loss: 2.4585390090942383\n",
      "epoch 1 batch 2127 loss: 2.583566188812256\n",
      "epoch 1 batch 2128 loss: 2.413221597671509\n",
      "epoch 1 batch 2129 loss: 2.8452534675598145\n",
      "epoch 1 batch 2130 loss: 2.943854808807373\n",
      "epoch 1 batch 2131 loss: 2.8897669315338135\n",
      "epoch 1 batch 2132 loss: 2.5081777572631836\n",
      "epoch 1 batch 2133 loss: 2.370936393737793\n",
      "epoch 1 batch 2134 loss: 2.758176326751709\n",
      "epoch 1 batch 2135 loss: 2.514763116836548\n",
      "epoch 1 batch 2136 loss: 2.9103639125823975\n",
      "epoch 1 batch 2137 loss: 2.5522866249084473\n",
      "epoch 1 batch 2138 loss: 2.6395773887634277\n",
      "epoch 1 batch 2139 loss: 2.833672046661377\n",
      "epoch 1 batch 2140 loss: 2.952378273010254\n",
      "epoch 1 batch 2141 loss: 3.171102285385132\n",
      "epoch 1 batch 2142 loss: 2.851853847503662\n",
      "epoch 1 batch 2143 loss: 2.8399219512939453\n",
      "epoch 1 batch 2144 loss: 2.3502378463745117\n",
      "epoch 1 batch 2145 loss: 2.5056023597717285\n",
      "epoch 1 batch 2146 loss: 2.691020965576172\n",
      "epoch 1 batch 2147 loss: 2.425309896469116\n",
      "epoch 1 batch 2148 loss: 2.690127372741699\n",
      "epoch 1 batch 2149 loss: 2.4734363555908203\n",
      "epoch 1 batch 2150 loss: 2.3180594444274902\n",
      "epoch 1 batch 2151 loss: 2.882284641265869\n",
      "epoch 1 batch 2152 loss: 3.079507350921631\n",
      "epoch 1 batch 2153 loss: 3.0289084911346436\n",
      "epoch 1 batch 2154 loss: 2.6551260948181152\n",
      "epoch 1 batch 2155 loss: 2.646397113800049\n",
      "epoch 1 batch 2156 loss: 2.991436004638672\n",
      "epoch 1 batch 2157 loss: 2.7510974407196045\n",
      "epoch 1 batch 2158 loss: 2.4404783248901367\n",
      "epoch 1 batch 2159 loss: 2.5121307373046875\n",
      "epoch 1 batch 2160 loss: 2.5530362129211426\n",
      "epoch 1 batch 2161 loss: 2.2711591720581055\n",
      "epoch 1 batch 2162 loss: 2.67765474319458\n",
      "epoch 1 batch 2163 loss: 2.5471761226654053\n",
      "epoch 1 batch 2164 loss: 2.4894051551818848\n",
      "epoch 1 batch 2165 loss: 2.510528564453125\n",
      "epoch 1 batch 2166 loss: 2.34822940826416\n",
      "epoch 1 batch 2167 loss: 3.1861443519592285\n",
      "epoch 1 batch 2168 loss: 2.5766875743865967\n",
      "epoch 1 batch 2169 loss: 2.954075813293457\n",
      "epoch 1 batch 2170 loss: 2.6130170822143555\n",
      "epoch 1 batch 2171 loss: 2.4461450576782227\n",
      "epoch 1 batch 2172 loss: 2.7257614135742188\n",
      "epoch 1 batch 2173 loss: 2.484380006790161\n",
      "epoch 1 batch 2174 loss: 2.4100494384765625\n",
      "epoch 1 batch 2175 loss: 2.50250506401062\n",
      "epoch 1 batch 2176 loss: 2.7762370109558105\n",
      "epoch 1 batch 2177 loss: 2.9023056030273438\n",
      "epoch 1 batch 2178 loss: 2.4762325286865234\n",
      "epoch 1 batch 2179 loss: 2.9189605712890625\n",
      "epoch 1 batch 2180 loss: 2.4584271907806396\n",
      "epoch 1 batch 2181 loss: 2.4331130981445312\n",
      "epoch 1 batch 2182 loss: 2.674098491668701\n",
      "epoch 1 batch 2183 loss: 2.7346627712249756\n",
      "epoch 1 batch 2184 loss: 2.3883557319641113\n",
      "epoch 1 batch 2185 loss: 2.397772789001465\n",
      "epoch 1 batch 2186 loss: 2.635800361633301\n",
      "epoch 1 batch 2187 loss: 2.6202354431152344\n",
      "epoch 1 batch 2188 loss: 2.5493855476379395\n",
      "epoch 1 batch 2189 loss: 2.569258689880371\n",
      "epoch 1 batch 2190 loss: 2.856790781021118\n",
      "epoch 1 batch 2191 loss: 2.5409607887268066\n",
      "epoch 1 batch 2192 loss: 2.9006335735321045\n",
      "epoch 1 batch 2193 loss: 2.836012363433838\n",
      "epoch 1 batch 2194 loss: 2.779981851577759\n",
      "epoch 1 batch 2195 loss: 2.399688482284546\n",
      "epoch 1 batch 2196 loss: 2.417078733444214\n",
      "epoch 1 batch 2197 loss: 2.443754196166992\n",
      "epoch 1 batch 2198 loss: 2.731752872467041\n",
      "epoch 1 batch 2199 loss: 2.6707887649536133\n",
      "epoch 1 batch 2200 loss: 2.5085902214050293\n",
      "epoch 1 batch 2201 loss: 2.572723388671875\n",
      "epoch 1 batch 2202 loss: 2.7653298377990723\n",
      "epoch 1 batch 2203 loss: 2.4834277629852295\n",
      "epoch 1 batch 2204 loss: 2.3820292949676514\n",
      "epoch 1 batch 2205 loss: 2.32643461227417\n",
      "epoch 1 batch 2206 loss: 2.7233071327209473\n",
      "epoch 1 batch 2207 loss: 2.6096649169921875\n",
      "epoch 1 batch 2208 loss: 2.7290520668029785\n",
      "epoch 1 batch 2209 loss: 2.382449150085449\n",
      "epoch 1 batch 2210 loss: 2.676429271697998\n",
      "epoch 1 batch 2211 loss: 2.335872173309326\n",
      "epoch 1 batch 2212 loss: 2.6154727935791016\n",
      "epoch 1 batch 2213 loss: 2.3187379837036133\n",
      "epoch 1 batch 2214 loss: 2.5578994750976562\n",
      "epoch 1 batch 2215 loss: 2.561016082763672\n",
      "epoch 1 batch 2216 loss: 2.6690170764923096\n",
      "epoch 1 batch 2217 loss: 2.5913166999816895\n",
      "epoch 1 batch 2218 loss: 2.66051983833313\n",
      "epoch 1 batch 2219 loss: 2.560044527053833\n",
      "epoch 1 batch 2220 loss: 2.54874324798584\n",
      "epoch 1 batch 2221 loss: 2.698272705078125\n",
      "epoch 1 batch 2222 loss: 2.6791038513183594\n",
      "epoch 1 batch 2223 loss: 2.4520678520202637\n",
      "epoch 1 batch 2224 loss: 2.446572780609131\n",
      "epoch 1 batch 2225 loss: 2.4896059036254883\n",
      "epoch 1 batch 2226 loss: 2.2416110038757324\n",
      "epoch 1 batch 2227 loss: 2.7479352951049805\n",
      "epoch 1 batch 2228 loss: 2.9387874603271484\n",
      "epoch 1 batch 2229 loss: 2.479560136795044\n",
      "epoch 1 batch 2230 loss: 2.988494634628296\n",
      "epoch 1 batch 2231 loss: 2.671877384185791\n",
      "epoch 1 batch 2232 loss: 2.6014552116394043\n",
      "epoch 1 batch 2233 loss: 2.4750914573669434\n",
      "epoch 1 batch 2234 loss: 2.737398386001587\n",
      "epoch 1 batch 2235 loss: 2.750875234603882\n",
      "epoch 1 batch 2236 loss: 2.4705543518066406\n",
      "epoch 1 batch 2237 loss: 2.641451835632324\n",
      "epoch 1 batch 2238 loss: 3.0546016693115234\n",
      "epoch 1 batch 2239 loss: 2.548025608062744\n",
      "epoch 1 batch 2240 loss: 2.7544636726379395\n",
      "epoch 1 batch 2241 loss: 2.6276309490203857\n",
      "epoch 1 batch 2242 loss: 2.5849497318267822\n",
      "epoch 1 batch 2243 loss: 2.706894874572754\n",
      "epoch 1 batch 2244 loss: 2.465804100036621\n",
      "epoch 1 batch 2245 loss: 2.9148707389831543\n",
      "epoch 1 batch 2246 loss: 2.506080150604248\n",
      "epoch 1 batch 2247 loss: 2.5509212017059326\n",
      "epoch 1 batch 2248 loss: 2.7134504318237305\n",
      "epoch 1 batch 2249 loss: 2.7940168380737305\n",
      "epoch 1 batch 2250 loss: 2.6626527309417725\n",
      "epoch 1 batch 2251 loss: 2.651970386505127\n",
      "epoch 1 batch 2252 loss: 2.7358503341674805\n",
      "epoch 1 batch 2253 loss: 2.6037588119506836\n",
      "epoch 1 batch 2254 loss: 2.6911280155181885\n",
      "epoch 1 batch 2255 loss: 2.4909942150115967\n",
      "epoch 1 batch 2256 loss: 2.5536224842071533\n",
      "epoch 1 batch 2257 loss: 2.276646614074707\n",
      "epoch 1 batch 2258 loss: 2.622321128845215\n",
      "epoch 1 batch 2259 loss: 2.5565977096557617\n",
      "epoch 1 batch 2260 loss: 2.8270018100738525\n",
      "epoch 1 batch 2261 loss: 3.1077778339385986\n",
      "epoch 1 batch 2262 loss: 2.999657154083252\n",
      "epoch 1 batch 2263 loss: 2.931839942932129\n",
      "epoch 1 batch 2264 loss: 2.552572727203369\n",
      "epoch 1 batch 2265 loss: 2.760789155960083\n",
      "epoch 1 batch 2266 loss: 2.7974486351013184\n",
      "epoch 1 batch 2267 loss: 2.610208511352539\n",
      "epoch 1 batch 2268 loss: 2.462873935699463\n",
      "epoch 1 batch 2269 loss: 2.7879698276519775\n",
      "epoch 1 batch 2270 loss: 2.5831117630004883\n",
      "epoch 1 batch 2271 loss: 2.7972729206085205\n",
      "epoch 1 batch 2272 loss: 2.6615734100341797\n",
      "epoch 1 batch 2273 loss: 2.776554584503174\n",
      "epoch 1 batch 2274 loss: 2.4144933223724365\n",
      "epoch 1 batch 2275 loss: 2.468319892883301\n",
      "epoch 1 batch 2276 loss: 2.4456706047058105\n",
      "epoch 1 batch 2277 loss: 2.4302005767822266\n",
      "epoch 1 batch 2278 loss: 2.570042848587036\n",
      "epoch 1 batch 2279 loss: 2.6946535110473633\n",
      "epoch 1 batch 2280 loss: 2.838801145553589\n",
      "epoch 1 batch 2281 loss: 2.673384189605713\n",
      "epoch 1 batch 2282 loss: 3.0676207542419434\n",
      "epoch 1 batch 2283 loss: 2.631349802017212\n",
      "epoch 1 batch 2284 loss: 2.5006232261657715\n",
      "epoch 1 batch 2285 loss: 2.8372437953948975\n",
      "epoch 1 batch 2286 loss: 2.6885573863983154\n",
      "epoch 1 batch 2287 loss: 2.6730751991271973\n",
      "epoch 1 batch 2288 loss: 3.2441656589508057\n",
      "epoch 1 batch 2289 loss: 2.4910101890563965\n",
      "epoch 1 batch 2290 loss: 2.7621235847473145\n",
      "epoch 1 batch 2291 loss: 2.7115135192871094\n",
      "epoch 1 batch 2292 loss: 3.096423387527466\n",
      "epoch 1 batch 2293 loss: 2.4766550064086914\n",
      "epoch 1 batch 2294 loss: 2.603773593902588\n",
      "epoch 1 batch 2295 loss: 2.797921657562256\n",
      "epoch 1 batch 2296 loss: 2.763904571533203\n",
      "epoch 1 batch 2297 loss: 2.52937388420105\n",
      "epoch 1 batch 2298 loss: 2.3869690895080566\n",
      "epoch 1 batch 2299 loss: 2.4790589809417725\n",
      "epoch 1 batch 2300 loss: 2.6332616806030273\n",
      "epoch 1 batch 2301 loss: 2.410501003265381\n",
      "epoch 1 batch 2302 loss: 2.311704158782959\n",
      "epoch 1 batch 2303 loss: 2.7449183464050293\n",
      "epoch 1 batch 2304 loss: 2.3548216819763184\n",
      "epoch 1 batch 2305 loss: 2.4398460388183594\n",
      "epoch 1 batch 2306 loss: 2.933100938796997\n",
      "epoch 1 batch 2307 loss: 2.485581159591675\n",
      "epoch 1 batch 2308 loss: 2.7201828956604004\n",
      "epoch 1 batch 2309 loss: 2.7925806045532227\n",
      "epoch 1 batch 2310 loss: 2.635982036590576\n",
      "epoch 1 batch 2311 loss: 2.685267925262451\n",
      "epoch 1 batch 2312 loss: 2.7485530376434326\n",
      "epoch 1 batch 2313 loss: 2.4055819511413574\n",
      "epoch 1 batch 2314 loss: 2.554823875427246\n",
      "epoch 1 batch 2315 loss: 2.4658279418945312\n",
      "epoch 1 batch 2316 loss: 2.684842109680176\n",
      "epoch 1 batch 2317 loss: 2.426734685897827\n",
      "epoch 1 batch 2318 loss: 2.5754406452178955\n",
      "epoch 1 batch 2319 loss: 2.570798873901367\n",
      "epoch 1 batch 2320 loss: 2.248143196105957\n",
      "epoch 1 batch 2321 loss: 2.7347209453582764\n",
      "epoch 1 batch 2322 loss: 2.6927173137664795\n",
      "epoch 1 batch 2323 loss: 2.476412534713745\n",
      "epoch 1 batch 2324 loss: 2.8090758323669434\n",
      "epoch 1 batch 2325 loss: 2.7460622787475586\n",
      "epoch 1 batch 2326 loss: 2.393314838409424\n",
      "epoch 1 batch 2327 loss: 2.6704540252685547\n",
      "epoch 1 batch 2328 loss: 2.512002468109131\n",
      "epoch 1 batch 2329 loss: 2.8305511474609375\n",
      "epoch 1 batch 2330 loss: 3.2180001735687256\n",
      "epoch 1 batch 2331 loss: 2.7574634552001953\n",
      "epoch 1 batch 2332 loss: 2.4809694290161133\n",
      "epoch 1 batch 2333 loss: 2.693174362182617\n",
      "epoch 1 batch 2334 loss: 2.87461519241333\n",
      "epoch 1 batch 2335 loss: 2.7240467071533203\n",
      "epoch 1 batch 2336 loss: 2.7705066204071045\n",
      "epoch 1 batch 2337 loss: 2.7337775230407715\n",
      "epoch 1 batch 2338 loss: 2.895462989807129\n",
      "epoch 1 batch 2339 loss: 2.6002044677734375\n",
      "epoch 1 batch 2340 loss: 2.553250312805176\n",
      "epoch 1 batch 2341 loss: 2.42795467376709\n",
      "epoch 1 batch 2342 loss: 2.902843952178955\n",
      "epoch 1 batch 2343 loss: 2.6389546394348145\n",
      "epoch 1 batch 2344 loss: 2.8402163982391357\n",
      "epoch 1 batch 2345 loss: 2.576465606689453\n",
      "epoch 1 batch 2346 loss: 3.20888090133667\n",
      "epoch 1 batch 2347 loss: 2.2834999561309814\n",
      "epoch 1 batch 2348 loss: 2.711491823196411\n",
      "epoch 1 batch 2349 loss: 2.7208333015441895\n",
      "epoch 1 batch 2350 loss: 2.320173978805542\n",
      "epoch 1 batch 2351 loss: 2.4651777744293213\n",
      "epoch 1 batch 2352 loss: 2.446808099746704\n",
      "epoch 1 batch 2353 loss: 2.642455577850342\n",
      "epoch 1 batch 2354 loss: 2.4064383506774902\n",
      "epoch 1 batch 2355 loss: 2.6265411376953125\n",
      "epoch 1 batch 2356 loss: 2.5664749145507812\n",
      "epoch 1 batch 2357 loss: 2.3471250534057617\n",
      "epoch 1 batch 2358 loss: 2.5791614055633545\n",
      "epoch 1 batch 2359 loss: 2.4969098567962646\n",
      "epoch 1 batch 2360 loss: 2.5053799152374268\n",
      "epoch 1 batch 2361 loss: 2.8164570331573486\n",
      "epoch 1 batch 2362 loss: 2.75809383392334\n",
      "epoch 1 batch 2363 loss: 2.587799549102783\n",
      "epoch 1 batch 2364 loss: 2.2808244228363037\n",
      "epoch 1 batch 2365 loss: 2.7095894813537598\n",
      "epoch 1 batch 2366 loss: 2.6751480102539062\n",
      "epoch 1 batch 2367 loss: 2.330273151397705\n",
      "epoch 1 batch 2368 loss: 2.8689398765563965\n",
      "epoch 1 batch 2369 loss: 2.509125232696533\n",
      "epoch 1 batch 2370 loss: 2.4108762741088867\n",
      "epoch 1 batch 2371 loss: 2.4490625858306885\n",
      "epoch 1 batch 2372 loss: 2.935237407684326\n",
      "epoch 1 batch 2373 loss: 2.4322376251220703\n",
      "epoch 1 batch 2374 loss: 2.683570146560669\n",
      "epoch 1 batch 2375 loss: 2.887624502182007\n",
      "epoch 1 batch 2376 loss: 2.461629867553711\n",
      "epoch 1 batch 2377 loss: 2.6366701126098633\n",
      "epoch 1 batch 2378 loss: 2.7166433334350586\n",
      "epoch 1 batch 2379 loss: 2.490321397781372\n",
      "epoch 1 batch 2380 loss: 2.65545392036438\n",
      "epoch 1 batch 2381 loss: 2.6841797828674316\n",
      "epoch 1 batch 2382 loss: 2.8277792930603027\n",
      "epoch 1 batch 2383 loss: 2.7535529136657715\n",
      "epoch 1 batch 2384 loss: 2.390719175338745\n",
      "epoch 1 batch 2385 loss: 2.897456645965576\n",
      "epoch 1 batch 2386 loss: 2.5664641857147217\n",
      "epoch 1 batch 2387 loss: 2.784318208694458\n",
      "epoch 1 batch 2388 loss: 2.66827130317688\n",
      "epoch 1 batch 2389 loss: 2.705660581588745\n",
      "epoch 1 batch 2390 loss: 2.7302613258361816\n",
      "epoch 1 batch 2391 loss: 2.7167677879333496\n",
      "epoch 1 batch 2392 loss: 2.2471768856048584\n",
      "epoch 1 batch 2393 loss: 2.7909457683563232\n",
      "epoch 1 batch 2394 loss: 2.88069486618042\n",
      "epoch 1 batch 2395 loss: 2.865633249282837\n",
      "epoch 1 batch 2396 loss: 2.5455265045166016\n",
      "epoch 1 batch 2397 loss: 2.551344871520996\n",
      "epoch 1 batch 2398 loss: 2.6397573947906494\n",
      "epoch 1 batch 2399 loss: 2.7659547328948975\n",
      "epoch 1 batch 2400 loss: 2.516364574432373\n",
      "epoch 1 batch 2401 loss: 2.7208251953125\n",
      "epoch 1 batch 2402 loss: 2.7572503089904785\n",
      "epoch 1 batch 2403 loss: 2.660820960998535\n",
      "epoch 1 batch 2404 loss: 2.875230312347412\n",
      "epoch 1 batch 2405 loss: 2.659355878829956\n",
      "epoch 1 batch 2406 loss: 2.3570194244384766\n",
      "epoch 1 batch 2407 loss: 2.6255240440368652\n",
      "epoch 1 batch 2408 loss: 2.7400808334350586\n",
      "epoch 1 batch 2409 loss: 2.8242268562316895\n",
      "epoch 1 batch 2410 loss: 2.480211019515991\n",
      "epoch 1 batch 2411 loss: 2.7967748641967773\n",
      "epoch 1 batch 2412 loss: 2.3073606491088867\n",
      "epoch 1 batch 2413 loss: 2.6212079524993896\n",
      "epoch 1 batch 2414 loss: 2.2300071716308594\n",
      "epoch 1 batch 2415 loss: 2.6155409812927246\n",
      "epoch 1 batch 2416 loss: 2.6337060928344727\n",
      "epoch 1 batch 2417 loss: 2.5406599044799805\n",
      "epoch 1 batch 2418 loss: 2.2603492736816406\n",
      "epoch 1 batch 2419 loss: 2.651787281036377\n",
      "epoch 1 batch 2420 loss: 2.697763442993164\n",
      "epoch 1 batch 2421 loss: 2.4061689376831055\n",
      "epoch 1 batch 2422 loss: 2.3927083015441895\n",
      "epoch 1 batch 2423 loss: 2.7221593856811523\n",
      "epoch 1 batch 2424 loss: 2.8625411987304688\n",
      "epoch 1 batch 2425 loss: 2.8125123977661133\n",
      "epoch 1 batch 2426 loss: 2.5718436241149902\n",
      "epoch 1 batch 2427 loss: 2.958303928375244\n",
      "epoch 1 batch 2428 loss: 2.473470449447632\n",
      "epoch 1 batch 2429 loss: 2.7444872856140137\n",
      "epoch 1 batch 2430 loss: 2.248997926712036\n",
      "epoch 1 batch 2431 loss: 2.773519992828369\n",
      "epoch 1 batch 2432 loss: 2.66023850440979\n",
      "epoch 1 batch 2433 loss: 2.5868678092956543\n",
      "epoch 1 batch 2434 loss: 2.6111693382263184\n",
      "epoch 1 batch 2435 loss: 2.43040132522583\n",
      "epoch 1 batch 2436 loss: 2.4480862617492676\n",
      "epoch 1 batch 2437 loss: 2.892444372177124\n",
      "epoch 1 batch 2438 loss: 2.4732913970947266\n",
      "epoch 1 batch 2439 loss: 3.2362797260284424\n",
      "epoch 1 batch 2440 loss: 2.704437732696533\n",
      "epoch 1 batch 2441 loss: 2.741751194000244\n",
      "epoch 1 batch 2442 loss: 2.7234725952148438\n",
      "epoch 1 batch 2443 loss: 2.8446145057678223\n",
      "epoch 1 batch 2444 loss: 2.417288064956665\n",
      "epoch 1 batch 2445 loss: 2.671532392501831\n",
      "epoch 1 batch 2446 loss: 2.4970483779907227\n",
      "epoch 1 batch 2447 loss: 2.552926540374756\n",
      "epoch 1 batch 2448 loss: 2.6917636394500732\n",
      "epoch 1 batch 2449 loss: 2.5722193717956543\n",
      "epoch 1 batch 2450 loss: 2.6823105812072754\n",
      "epoch 1 batch 2451 loss: 2.7609546184539795\n",
      "epoch 1 batch 2452 loss: 2.872481346130371\n",
      "epoch 1 batch 2453 loss: 2.9777345657348633\n",
      "epoch 1 batch 2454 loss: 2.798222064971924\n",
      "epoch 1 batch 2455 loss: 2.6795592308044434\n",
      "epoch 1 batch 2456 loss: 2.8119442462921143\n",
      "epoch 1 batch 2457 loss: 2.678445339202881\n",
      "epoch 1 batch 2458 loss: 2.8495430946350098\n",
      "epoch 1 batch 2459 loss: 2.7329845428466797\n",
      "epoch 1 batch 2460 loss: 2.3684043884277344\n",
      "epoch 1 batch 2461 loss: 2.4779629707336426\n",
      "epoch 1 batch 2462 loss: 2.8650286197662354\n",
      "epoch 1 batch 2463 loss: 2.767598867416382\n",
      "epoch 1 batch 2464 loss: 2.5326714515686035\n",
      "epoch 1 batch 2465 loss: 2.4356064796447754\n",
      "epoch 1 batch 2466 loss: 2.500131607055664\n",
      "epoch 1 batch 2467 loss: 2.8545899391174316\n",
      "epoch 1 batch 2468 loss: 2.5913681983947754\n",
      "epoch 1 batch 2469 loss: 2.559684991836548\n",
      "epoch 1 batch 2470 loss: 2.5840630531311035\n",
      "epoch 1 batch 2471 loss: 2.4362316131591797\n",
      "epoch 1 batch 2472 loss: 2.9045522212982178\n",
      "epoch 1 batch 2473 loss: 2.453646659851074\n",
      "epoch 1 batch 2474 loss: 2.6672067642211914\n",
      "epoch 1 batch 2475 loss: 2.543051242828369\n",
      "epoch 1 batch 2476 loss: 2.584315299987793\n",
      "epoch 1 batch 2477 loss: 2.528797149658203\n",
      "epoch 1 batch 2478 loss: 2.7549662590026855\n",
      "epoch 1 batch 2479 loss: 2.87859845161438\n",
      "epoch 1 batch 2480 loss: 2.8059587478637695\n",
      "epoch 1 batch 2481 loss: 2.6852827072143555\n",
      "epoch 1 batch 2482 loss: 2.5357584953308105\n",
      "epoch 1 batch 2483 loss: 2.3906126022338867\n",
      "epoch 1 batch 2484 loss: 2.529033660888672\n",
      "epoch 1 batch 2485 loss: 2.4179627895355225\n",
      "epoch 1 batch 2486 loss: 2.5425515174865723\n",
      "epoch 1 batch 2487 loss: 2.5534143447875977\n",
      "epoch 1 batch 2488 loss: 2.383392810821533\n",
      "epoch 1 batch 2489 loss: 2.7735238075256348\n",
      "epoch 1 batch 2490 loss: 2.67574143409729\n",
      "epoch 1 batch 2491 loss: 2.7123513221740723\n",
      "epoch 1 batch 2492 loss: 2.713047981262207\n",
      "epoch 1 batch 2493 loss: 2.4970040321350098\n",
      "epoch 1 batch 2494 loss: 2.5132358074188232\n",
      "epoch 1 batch 2495 loss: 2.8024063110351562\n",
      "epoch 1 batch 2496 loss: 2.6772942543029785\n",
      "epoch 1 batch 2497 loss: 2.665283679962158\n",
      "epoch 1 batch 2498 loss: 2.6910462379455566\n",
      "epoch 1 batch 2499 loss: 2.652698516845703\n",
      "epoch 1 batch 2500 loss: 2.7123656272888184\n",
      "epoch 1 batch 2501 loss: 2.63698148727417\n",
      "epoch 1 batch 2502 loss: 2.521461009979248\n",
      "epoch 1 batch 2503 loss: 2.775498151779175\n",
      "epoch 1 batch 2504 loss: 2.8017044067382812\n",
      "epoch 1 batch 2505 loss: 2.7825546264648438\n",
      "epoch 1 batch 2506 loss: 2.5784904956817627\n",
      "epoch 1 batch 2507 loss: 2.642716884613037\n",
      "epoch 1 batch 2508 loss: 2.7163567543029785\n",
      "epoch 1 batch 2509 loss: 2.500941514968872\n",
      "epoch 1 batch 2510 loss: 2.273831844329834\n",
      "epoch 1 batch 2511 loss: 2.820098400115967\n",
      "epoch 1 batch 2512 loss: 2.427521228790283\n",
      "epoch 1 batch 2513 loss: 3.260227680206299\n",
      "epoch 1 batch 2514 loss: 2.912656307220459\n",
      "epoch 1 batch 2515 loss: 2.5539498329162598\n",
      "epoch 1 batch 2516 loss: 3.0972914695739746\n",
      "epoch 1 batch 2517 loss: 2.7695837020874023\n",
      "epoch 1 batch 2518 loss: 2.8981575965881348\n",
      "epoch 1 batch 2519 loss: 2.8364791870117188\n",
      "epoch 1 batch 2520 loss: 3.138252019882202\n",
      "epoch 1 batch 2521 loss: 2.5598955154418945\n",
      "epoch 1 batch 2522 loss: 2.756387710571289\n",
      "epoch 1 batch 2523 loss: 2.3785388469696045\n",
      "epoch 1 batch 2524 loss: 2.3255820274353027\n",
      "epoch 1 batch 2525 loss: 2.454990863800049\n",
      "epoch 1 batch 2526 loss: 2.4182467460632324\n",
      "epoch 1 batch 2527 loss: 2.7850332260131836\n",
      "epoch 1 batch 2528 loss: 2.429239273071289\n",
      "epoch 1 batch 2529 loss: 2.8958818912506104\n",
      "epoch 1 batch 2530 loss: 2.757045269012451\n",
      "epoch 1 batch 2531 loss: 2.724513292312622\n",
      "epoch 1 batch 2532 loss: 2.8026175498962402\n",
      "epoch 1 batch 2533 loss: 2.3318026065826416\n",
      "epoch 1 batch 2534 loss: 2.619021415710449\n",
      "epoch 1 batch 2535 loss: 3.061602830886841\n",
      "epoch 1 batch 2536 loss: 2.749049186706543\n",
      "epoch 1 batch 2537 loss: 2.555971622467041\n",
      "epoch 1 batch 2538 loss: 2.8478732109069824\n",
      "epoch 1 batch 2539 loss: 2.8889198303222656\n",
      "epoch 1 batch 2540 loss: 2.7193431854248047\n",
      "epoch 1 batch 2541 loss: 2.4409542083740234\n",
      "epoch 1 batch 2542 loss: 2.591461658477783\n",
      "epoch 1 batch 2543 loss: 2.9957895278930664\n",
      "epoch 1 batch 2544 loss: 2.7123069763183594\n",
      "epoch 1 batch 2545 loss: 3.1788668632507324\n",
      "epoch 1 batch 2546 loss: 2.7078404426574707\n",
      "epoch 1 batch 2547 loss: 2.507692813873291\n",
      "epoch 1 batch 2548 loss: 2.816315174102783\n",
      "epoch 1 batch 2549 loss: 2.6188230514526367\n",
      "epoch 1 batch 2550 loss: 2.6704041957855225\n",
      "epoch 1 batch 2551 loss: 2.4747447967529297\n",
      "epoch 1 batch 2552 loss: 2.736128807067871\n",
      "epoch 1 batch 2553 loss: 2.5128684043884277\n",
      "epoch 1 batch 2554 loss: 3.0575575828552246\n",
      "epoch 1 batch 2555 loss: 2.6403322219848633\n",
      "epoch 1 batch 2556 loss: 2.4839770793914795\n",
      "epoch 1 batch 2557 loss: 3.024784564971924\n",
      "epoch 1 batch 2558 loss: 2.579824924468994\n",
      "epoch 1 batch 2559 loss: 2.734865665435791\n",
      "epoch 1 batch 2560 loss: 2.447173595428467\n",
      "epoch 1 batch 2561 loss: 2.764324188232422\n",
      "epoch 1 batch 2562 loss: 2.2607555389404297\n",
      "epoch 1 batch 2563 loss: 2.832728862762451\n",
      "epoch 1 batch 2564 loss: 2.854146957397461\n",
      "epoch 1 batch 2565 loss: 2.2507200241088867\n",
      "epoch 1 batch 2566 loss: 2.4487380981445312\n",
      "epoch 1 batch 2567 loss: 2.7357869148254395\n",
      "epoch 1 batch 2568 loss: 2.5018882751464844\n",
      "epoch 1 batch 2569 loss: 2.8090426921844482\n",
      "epoch 1 batch 2570 loss: 2.661616802215576\n",
      "epoch 1 batch 2571 loss: 2.747002124786377\n",
      "epoch 1 batch 2572 loss: 2.556030750274658\n",
      "epoch 1 batch 2573 loss: 2.8738279342651367\n",
      "epoch 1 batch 2574 loss: 2.8864359855651855\n",
      "epoch 1 batch 2575 loss: 2.4604439735412598\n",
      "epoch 1 batch 2576 loss: 2.8711042404174805\n",
      "epoch 1 batch 2577 loss: 2.8638153076171875\n",
      "epoch 1 batch 2578 loss: 3.1605148315429688\n",
      "epoch 1 batch 2579 loss: 2.8203155994415283\n",
      "epoch 1 batch 2580 loss: 2.629624366760254\n",
      "epoch 1 batch 2581 loss: 2.560990571975708\n",
      "epoch 1 batch 2582 loss: 2.793255090713501\n",
      "epoch 1 batch 2583 loss: 2.4528427124023438\n",
      "epoch 1 batch 2584 loss: 2.7159032821655273\n",
      "epoch 1 batch 2585 loss: 2.9223263263702393\n",
      "epoch 1 batch 2586 loss: 2.654654026031494\n",
      "epoch 1 batch 2587 loss: 2.807553768157959\n",
      "epoch 1 batch 2588 loss: 2.4922924041748047\n",
      "epoch 1 batch 2589 loss: 2.47279953956604\n",
      "epoch 1 batch 2590 loss: 2.693230628967285\n",
      "epoch 1 batch 2591 loss: 2.4338021278381348\n",
      "epoch 1 batch 2592 loss: 2.831713914871216\n",
      "epoch 1 batch 2593 loss: 2.4593119621276855\n",
      "epoch 1 batch 2594 loss: 2.937483310699463\n",
      "epoch 1 batch 2595 loss: 2.922539472579956\n",
      "epoch 1 batch 2596 loss: 2.482900381088257\n",
      "epoch 1 batch 2597 loss: 2.535325527191162\n",
      "epoch 1 batch 2598 loss: 2.8769426345825195\n",
      "epoch 1 batch 2599 loss: 2.4073925018310547\n",
      "epoch 1 batch 2600 loss: 2.6392552852630615\n",
      "epoch 1 batch 2601 loss: 2.669170618057251\n",
      "epoch 1 batch 2602 loss: 2.716620445251465\n",
      "epoch 1 batch 2603 loss: 2.803530216217041\n",
      "epoch 1 batch 2604 loss: 2.647904872894287\n",
      "epoch 1 batch 2605 loss: 2.447117328643799\n",
      "epoch 1 batch 2606 loss: 2.351017475128174\n",
      "epoch 1 batch 2607 loss: 2.452660083770752\n",
      "epoch 1 batch 2608 loss: 2.511185646057129\n",
      "epoch 1 batch 2609 loss: 2.5406603813171387\n",
      "epoch 1 batch 2610 loss: 2.7491466999053955\n",
      "epoch 1 batch 2611 loss: 2.9692797660827637\n",
      "epoch 1 batch 2612 loss: 2.4706859588623047\n",
      "epoch 1 batch 2613 loss: 2.3591957092285156\n",
      "epoch 1 batch 2614 loss: 2.7357921600341797\n",
      "epoch 1 batch 2615 loss: 2.8451194763183594\n",
      "epoch 1 batch 2616 loss: 2.5260026454925537\n",
      "epoch 1 batch 2617 loss: 2.450697898864746\n",
      "epoch 1 batch 2618 loss: 2.8671631813049316\n",
      "epoch 1 batch 2619 loss: 2.804750442504883\n",
      "epoch 1 batch 2620 loss: 2.5991907119750977\n",
      "epoch 1 batch 2621 loss: 2.481898307800293\n",
      "epoch 1 batch 2622 loss: 2.3823165893554688\n",
      "epoch 1 batch 2623 loss: 2.4362120628356934\n",
      "epoch 1 batch 2624 loss: 2.6133217811584473\n",
      "epoch 1 batch 2625 loss: 2.6087257862091064\n",
      "epoch 1 batch 2626 loss: 2.513216972351074\n",
      "epoch 1 batch 2627 loss: 2.5198259353637695\n",
      "epoch 1 batch 2628 loss: 2.600368022918701\n",
      "epoch 1 batch 2629 loss: 2.773662805557251\n",
      "epoch 1 batch 2630 loss: 2.363607406616211\n",
      "epoch 1 batch 2631 loss: 2.2859673500061035\n",
      "epoch 1 batch 2632 loss: 2.796372890472412\n",
      "epoch 1 batch 2633 loss: 2.6070733070373535\n",
      "epoch 1 batch 2634 loss: 2.723820924758911\n",
      "epoch 1 batch 2635 loss: 2.67299222946167\n",
      "epoch 1 batch 2636 loss: 2.3530354499816895\n",
      "epoch 1 batch 2637 loss: 2.597708225250244\n",
      "epoch 1 batch 2638 loss: 2.5732667446136475\n",
      "epoch 1 batch 2639 loss: 2.6342785358428955\n",
      "epoch 1 batch 2640 loss: 2.9013471603393555\n",
      "epoch 1 batch 2641 loss: 2.8858985900878906\n",
      "epoch 1 batch 2642 loss: 2.4078173637390137\n",
      "epoch 1 batch 2643 loss: 2.5409388542175293\n",
      "epoch 1 batch 2644 loss: 2.4981000423431396\n",
      "epoch 1 batch 2645 loss: 2.4430973529815674\n",
      "epoch 1 batch 2646 loss: 2.5461976528167725\n",
      "epoch 1 batch 2647 loss: 2.6706719398498535\n",
      "epoch 1 batch 2648 loss: 2.5258073806762695\n",
      "epoch 1 batch 2649 loss: 2.6126909255981445\n",
      "epoch 1 batch 2650 loss: 2.3672146797180176\n",
      "epoch 1 batch 2651 loss: 2.522319793701172\n",
      "epoch 1 batch 2652 loss: 2.6185505390167236\n",
      "epoch 1 batch 2653 loss: 2.453458309173584\n",
      "epoch 1 batch 2654 loss: 2.832667350769043\n",
      "epoch 1 batch 2655 loss: 2.564182758331299\n",
      "epoch 1 batch 2656 loss: 2.6447677612304688\n",
      "epoch 1 batch 2657 loss: 2.6229395866394043\n",
      "epoch 1 batch 2658 loss: 2.7355690002441406\n",
      "epoch 1 batch 2659 loss: 2.896479368209839\n",
      "epoch 1 batch 2660 loss: 2.994025230407715\n",
      "epoch 1 batch 2661 loss: 3.2113733291625977\n",
      "epoch 1 batch 2662 loss: 2.52789568901062\n",
      "epoch 1 batch 2663 loss: 2.474424123764038\n",
      "epoch 1 batch 2664 loss: 2.4945895671844482\n",
      "epoch 1 batch 2665 loss: 2.909909725189209\n",
      "epoch 1 batch 2666 loss: 2.7197046279907227\n",
      "epoch 1 batch 2667 loss: 2.486896514892578\n",
      "epoch 1 batch 2668 loss: 2.5917110443115234\n",
      "epoch 1 batch 2669 loss: 2.8311381340026855\n",
      "epoch 1 batch 2670 loss: 2.6077566146850586\n",
      "epoch 1 batch 2671 loss: 2.526113510131836\n",
      "epoch 1 batch 2672 loss: 2.4883198738098145\n",
      "epoch 1 batch 2673 loss: 2.4142627716064453\n",
      "epoch 1 batch 2674 loss: 2.791088581085205\n",
      "epoch 1 batch 2675 loss: 2.3566091060638428\n",
      "epoch 1 batch 2676 loss: 2.863206624984741\n",
      "epoch 1 batch 2677 loss: 2.6379263401031494\n",
      "epoch 1 batch 2678 loss: 2.6506214141845703\n",
      "epoch 1 batch 2679 loss: 2.6865527629852295\n",
      "epoch 1 batch 2680 loss: 2.7817282676696777\n",
      "epoch 1 batch 2681 loss: 2.3890833854675293\n",
      "epoch 1 batch 2682 loss: 2.6981139183044434\n",
      "epoch 1 batch 2683 loss: 2.3050689697265625\n",
      "epoch 1 batch 2684 loss: 2.4894142150878906\n",
      "epoch 1 batch 2685 loss: 2.7324938774108887\n",
      "epoch 1 batch 2686 loss: 2.9083545207977295\n",
      "epoch 1 batch 2687 loss: 2.4428625106811523\n",
      "epoch 1 batch 2688 loss: 2.7072665691375732\n",
      "epoch 1 batch 2689 loss: 2.7585866451263428\n",
      "epoch 1 batch 2690 loss: 2.1835670471191406\n",
      "epoch 1 batch 2691 loss: 2.6034345626831055\n",
      "epoch 1 batch 2692 loss: 2.7819085121154785\n",
      "epoch 1 batch 2693 loss: 2.7403314113616943\n",
      "epoch 1 batch 2694 loss: 2.4784934520721436\n",
      "epoch 1 batch 2695 loss: 2.67313814163208\n",
      "epoch 1 batch 2696 loss: 2.3180088996887207\n",
      "epoch 1 batch 2697 loss: 2.578362464904785\n",
      "epoch 1 batch 2698 loss: 2.677797794342041\n",
      "epoch 1 batch 2699 loss: 2.4798777103424072\n",
      "epoch 1 batch 2700 loss: 2.8200571537017822\n",
      "epoch 1 batch 2701 loss: 2.6247928142547607\n",
      "epoch 1 batch 2702 loss: 2.6107070446014404\n",
      "epoch 1 batch 2703 loss: 2.274423122406006\n",
      "epoch 1 batch 2704 loss: 2.578993558883667\n",
      "epoch 1 batch 2705 loss: 2.6168861389160156\n",
      "epoch 1 batch 2706 loss: 2.6861703395843506\n",
      "epoch 1 batch 2707 loss: 2.6983237266540527\n",
      "epoch 1 batch 2708 loss: 2.569706916809082\n",
      "epoch 1 batch 2709 loss: 2.456686019897461\n",
      "epoch 1 batch 2710 loss: 2.7920520305633545\n",
      "epoch 1 batch 2711 loss: 2.5420145988464355\n",
      "epoch 1 batch 2712 loss: 2.5934436321258545\n",
      "epoch 1 batch 2713 loss: 2.349113702774048\n",
      "epoch 1 batch 2714 loss: 2.659130096435547\n",
      "epoch 1 batch 2715 loss: 2.443408250808716\n",
      "epoch 1 batch 2716 loss: 2.5568957328796387\n",
      "epoch 1 batch 2717 loss: 2.556593179702759\n",
      "epoch 1 batch 2718 loss: 3.1664493083953857\n",
      "epoch 1 batch 2719 loss: 2.575920581817627\n",
      "epoch 1 batch 2720 loss: 2.7939743995666504\n",
      "epoch 1 batch 2721 loss: 2.545745849609375\n",
      "epoch 1 batch 2722 loss: 2.280398368835449\n",
      "epoch 1 batch 2723 loss: 2.6864795684814453\n",
      "epoch 1 batch 2724 loss: 2.760350227355957\n",
      "epoch 1 batch 2725 loss: 2.813446044921875\n",
      "epoch 1 batch 2726 loss: 2.5465569496154785\n",
      "epoch 1 batch 2727 loss: 2.924158811569214\n",
      "epoch 1 batch 2728 loss: 2.531208038330078\n",
      "epoch 1 batch 2729 loss: 2.690014123916626\n",
      "epoch 1 batch 2730 loss: 3.010092258453369\n",
      "epoch 1 batch 2731 loss: 2.6448705196380615\n",
      "epoch 1 batch 2732 loss: 2.3844690322875977\n",
      "epoch 1 batch 2733 loss: 2.4910659790039062\n",
      "epoch 1 batch 2734 loss: 2.4658498764038086\n",
      "epoch 1 batch 2735 loss: 2.4999234676361084\n",
      "epoch 1 batch 2736 loss: 2.760713577270508\n",
      "epoch 1 batch 2737 loss: 2.564237594604492\n",
      "epoch 1 batch 2738 loss: 2.6894006729125977\n",
      "epoch 1 batch 2739 loss: 2.6158366203308105\n",
      "epoch 1 batch 2740 loss: 2.6130452156066895\n",
      "epoch 1 batch 2741 loss: 2.6624364852905273\n",
      "epoch 1 batch 2742 loss: 2.6157326698303223\n",
      "epoch 1 batch 2743 loss: 2.7367124557495117\n",
      "epoch 1 batch 2744 loss: 2.840552806854248\n",
      "epoch 1 batch 2745 loss: 2.698251724243164\n",
      "epoch 1 batch 2746 loss: 2.5906381607055664\n",
      "epoch 1 batch 2747 loss: 2.4969403743743896\n",
      "epoch 1 batch 2748 loss: 2.7388510704040527\n",
      "epoch 1 batch 2749 loss: 2.4708003997802734\n",
      "epoch 1 batch 2750 loss: 2.5552563667297363\n",
      "epoch 1 batch 2751 loss: 2.5526864528656006\n",
      "epoch 1 batch 2752 loss: 2.3739466667175293\n",
      "epoch 1 batch 2753 loss: 2.3280599117279053\n",
      "epoch 1 batch 2754 loss: 2.778991222381592\n",
      "epoch 1 batch 2755 loss: 2.604456901550293\n",
      "epoch 1 batch 2756 loss: 3.0964460372924805\n",
      "epoch 1 batch 2757 loss: 2.788853406906128\n",
      "epoch 1 batch 2758 loss: 2.7522854804992676\n",
      "epoch 1 batch 2759 loss: 2.2804203033447266\n",
      "epoch 1 batch 2760 loss: 2.613649606704712\n",
      "epoch 1 batch 2761 loss: 2.718083381652832\n",
      "epoch 1 batch 2762 loss: 2.6595044136047363\n",
      "epoch 1 batch 2763 loss: 2.4822707176208496\n",
      "epoch 1 batch 2764 loss: 2.7436556816101074\n",
      "epoch 1 batch 2765 loss: 2.5444693565368652\n",
      "epoch 1 batch 2766 loss: 2.346390724182129\n",
      "epoch 1 batch 2767 loss: 2.6014456748962402\n",
      "epoch 1 batch 2768 loss: 2.9544408321380615\n",
      "epoch 1 batch 2769 loss: 2.467264413833618\n",
      "epoch 1 batch 2770 loss: 2.9506146907806396\n",
      "epoch 1 batch 2771 loss: 2.822935104370117\n",
      "epoch 1 batch 2772 loss: 2.743220806121826\n",
      "epoch 1 batch 2773 loss: 2.4845705032348633\n",
      "epoch 1 batch 2774 loss: 3.0933516025543213\n",
      "epoch 1 batch 2775 loss: 2.549525737762451\n",
      "epoch 1 batch 2776 loss: 2.7305383682250977\n",
      "epoch 1 batch 2777 loss: 2.8863353729248047\n",
      "epoch 1 batch 2778 loss: 2.7585110664367676\n",
      "epoch 1 batch 2779 loss: 2.7447681427001953\n",
      "epoch 1 batch 2780 loss: 2.576449394226074\n",
      "epoch 1 batch 2781 loss: 3.442751407623291\n",
      "epoch 1 batch 2782 loss: 2.6065611839294434\n",
      "epoch 1 batch 2783 loss: 2.4843969345092773\n",
      "epoch 1 batch 2784 loss: 2.54091477394104\n",
      "epoch 1 batch 2785 loss: 2.4934232234954834\n",
      "epoch 1 batch 2786 loss: 2.5299618244171143\n",
      "epoch 1 batch 2787 loss: 2.257388114929199\n",
      "epoch 1 batch 2788 loss: 2.5764567852020264\n",
      "epoch 1 batch 2789 loss: 2.3550891876220703\n",
      "epoch 1 batch 2790 loss: 2.792416572570801\n",
      "epoch 1 batch 2791 loss: 2.341827869415283\n",
      "epoch 1 batch 2792 loss: 2.4846725463867188\n",
      "epoch 1 batch 2793 loss: 2.7200939655303955\n",
      "epoch 1 batch 2794 loss: 2.452780246734619\n",
      "epoch 1 batch 2795 loss: 2.4999866485595703\n",
      "epoch 1 batch 2796 loss: 2.3171749114990234\n",
      "epoch 1 batch 2797 loss: 2.5816078186035156\n",
      "epoch 1 batch 2798 loss: 2.5961384773254395\n",
      "epoch 1 batch 2799 loss: 2.4303994178771973\n",
      "epoch 1 batch 2800 loss: 2.5200910568237305\n",
      "epoch 1 batch 2801 loss: 2.5234873294830322\n",
      "epoch 1 batch 2802 loss: 2.5898656845092773\n",
      "epoch 1 batch 2803 loss: 2.4621171951293945\n",
      "epoch 1 batch 2804 loss: 2.988407611846924\n",
      "epoch 1 batch 2805 loss: 2.731487512588501\n",
      "epoch 1 batch 2806 loss: 2.8303346633911133\n",
      "epoch 1 batch 2807 loss: 2.6601181030273438\n",
      "epoch 1 batch 2808 loss: 2.5807912349700928\n",
      "epoch 1 batch 2809 loss: 2.4641339778900146\n",
      "epoch 1 batch 2810 loss: 2.3833155632019043\n",
      "epoch 1 batch 2811 loss: 2.651864528656006\n",
      "epoch 1 batch 2812 loss: 2.7330141067504883\n",
      "epoch 1 batch 2813 loss: 2.6116302013397217\n",
      "epoch 1 batch 2814 loss: 2.513692855834961\n",
      "epoch 1 batch 2815 loss: 2.4421486854553223\n",
      "epoch 1 batch 2816 loss: 2.8758153915405273\n",
      "epoch 1 batch 2817 loss: 2.4979352951049805\n",
      "epoch 1 batch 2818 loss: 2.760529041290283\n",
      "epoch 1 batch 2819 loss: 2.6930716037750244\n",
      "epoch 1 batch 2820 loss: 2.872593879699707\n",
      "epoch 1 batch 2821 loss: 2.5396785736083984\n",
      "epoch 1 batch 2822 loss: 2.8776187896728516\n",
      "epoch 1 batch 2823 loss: 2.5700340270996094\n",
      "epoch 1 batch 2824 loss: 2.743882894515991\n",
      "epoch 1 batch 2825 loss: 2.789491653442383\n",
      "epoch 1 batch 2826 loss: 2.613131046295166\n",
      "epoch 1 batch 2827 loss: 3.060549259185791\n",
      "epoch 1 batch 2828 loss: 3.0513253211975098\n",
      "epoch 1 batch 2829 loss: 2.7998197078704834\n",
      "epoch 1 batch 2830 loss: 2.6147730350494385\n",
      "epoch 1 batch 2831 loss: 2.589686393737793\n",
      "epoch 1 batch 2832 loss: 2.5331296920776367\n",
      "epoch 1 batch 2833 loss: 2.886728525161743\n",
      "epoch 1 batch 2834 loss: 2.609706401824951\n",
      "epoch 1 batch 2835 loss: 2.3053207397460938\n",
      "epoch 1 batch 2836 loss: 2.821134567260742\n",
      "epoch 1 batch 2837 loss: 2.5179858207702637\n",
      "epoch 1 batch 2838 loss: 2.679835557937622\n",
      "epoch 1 batch 2839 loss: 2.415437936782837\n",
      "epoch 1 batch 2840 loss: 2.486196994781494\n",
      "epoch 1 batch 2841 loss: 2.4733338356018066\n",
      "epoch 1 batch 2842 loss: 2.4824273586273193\n",
      "epoch 1 batch 2843 loss: 2.7889199256896973\n",
      "epoch 1 batch 2844 loss: 2.2916970252990723\n",
      "epoch 1 batch 2845 loss: 2.6098642349243164\n",
      "epoch 1 batch 2846 loss: 2.776278257369995\n",
      "epoch 1 batch 2847 loss: 2.720526695251465\n",
      "epoch 1 batch 2848 loss: 2.5528900623321533\n",
      "epoch 1 batch 2849 loss: 2.853691816329956\n",
      "epoch 1 batch 2850 loss: 2.8564350605010986\n",
      "epoch 1 batch 2851 loss: 2.802689552307129\n",
      "epoch 1 batch 2852 loss: 2.4324045181274414\n",
      "epoch 1 batch 2853 loss: 2.6795132160186768\n",
      "epoch 1 batch 2854 loss: 2.7291154861450195\n",
      "epoch 1 batch 2855 loss: 2.5307602882385254\n",
      "epoch 1 batch 2856 loss: 2.8081250190734863\n",
      "epoch 1 batch 2857 loss: 3.0012757778167725\n",
      "epoch 1 batch 2858 loss: 2.9827094078063965\n",
      "epoch 1 batch 2859 loss: 2.4448256492614746\n",
      "epoch 1 batch 2860 loss: 2.626218318939209\n",
      "epoch 1 batch 2861 loss: 2.5411713123321533\n",
      "epoch 1 batch 2862 loss: 2.2354421615600586\n",
      "epoch 1 batch 2863 loss: 2.5237340927124023\n",
      "epoch 1 batch 2864 loss: 2.69435977935791\n",
      "epoch 1 batch 2865 loss: 2.3197646141052246\n",
      "epoch 1 batch 2866 loss: 2.531506299972534\n",
      "epoch 1 batch 2867 loss: 2.8074424266815186\n",
      "epoch 1 batch 2868 loss: 2.1950247287750244\n",
      "epoch 1 batch 2869 loss: 2.6379587650299072\n",
      "epoch 1 batch 2870 loss: 2.9782021045684814\n",
      "epoch 1 batch 2871 loss: 2.5150632858276367\n",
      "epoch 1 batch 2872 loss: 2.4664785861968994\n",
      "epoch 1 batch 2873 loss: 2.5798773765563965\n",
      "epoch 1 batch 2874 loss: 2.6797189712524414\n",
      "epoch 1 batch 2875 loss: 2.76178240776062\n",
      "epoch 1 batch 2876 loss: 2.5400588512420654\n",
      "epoch 1 batch 2877 loss: 2.361239194869995\n",
      "epoch 1 batch 2878 loss: 2.4866583347320557\n",
      "epoch 1 batch 2879 loss: 2.6871774196624756\n",
      "epoch 1 batch 2880 loss: 2.627798557281494\n",
      "epoch 1 batch 2881 loss: 2.3702878952026367\n",
      "epoch 1 batch 2882 loss: 2.4523391723632812\n",
      "epoch 1 batch 2883 loss: 2.6760241985321045\n",
      "epoch 1 batch 2884 loss: 2.522433280944824\n",
      "epoch 1 batch 2885 loss: 2.6869308948516846\n",
      "epoch 1 batch 2886 loss: 3.0762574672698975\n",
      "epoch 1 batch 2887 loss: 2.5042288303375244\n",
      "epoch 1 batch 2888 loss: 2.3271472454071045\n",
      "epoch 1 batch 2889 loss: 2.7988781929016113\n",
      "epoch 1 batch 2890 loss: 2.472973346710205\n",
      "epoch 1 batch 2891 loss: 2.5068812370300293\n",
      "epoch 1 batch 2892 loss: 2.459578514099121\n",
      "epoch 1 batch 2893 loss: 2.647331714630127\n",
      "epoch 1 batch 2894 loss: 2.6264939308166504\n",
      "epoch 1 batch 2895 loss: 2.376217842102051\n",
      "epoch 1 batch 2896 loss: 2.378542184829712\n",
      "epoch 1 batch 2897 loss: 2.2751598358154297\n",
      "epoch 1 batch 2898 loss: 2.539273500442505\n",
      "epoch 1 batch 2899 loss: 2.5178403854370117\n",
      "epoch 1 batch 2900 loss: 2.766404151916504\n",
      "epoch 1 batch 2901 loss: 2.622230052947998\n",
      "epoch 1 batch 2902 loss: 2.3913071155548096\n",
      "epoch 1 batch 2903 loss: 2.450277805328369\n",
      "epoch 1 batch 2904 loss: 2.507845401763916\n",
      "epoch 1 batch 2905 loss: 2.767810821533203\n",
      "epoch 1 batch 2906 loss: 2.7113122940063477\n",
      "epoch 1 batch 2907 loss: 2.384702682495117\n",
      "epoch 1 batch 2908 loss: 3.184915781021118\n",
      "epoch 1 batch 2909 loss: 2.3520872592926025\n",
      "epoch 1 batch 2910 loss: 2.6586389541625977\n",
      "epoch 1 batch 2911 loss: 2.3792505264282227\n",
      "epoch 1 batch 2912 loss: 2.7281665802001953\n",
      "epoch 1 batch 2913 loss: 2.444387197494507\n",
      "epoch 1 batch 2914 loss: 2.5414371490478516\n",
      "epoch 1 batch 2915 loss: 2.905971050262451\n",
      "epoch 1 batch 2916 loss: 2.608447551727295\n",
      "epoch 1 batch 2917 loss: 2.9711780548095703\n",
      "epoch 1 batch 2918 loss: 2.5299298763275146\n",
      "epoch 1 batch 2919 loss: 2.3756935596466064\n",
      "epoch 1 batch 2920 loss: 2.8063719272613525\n",
      "epoch 1 batch 2921 loss: 2.646845817565918\n",
      "epoch 1 batch 2922 loss: 2.517672538757324\n",
      "epoch 1 batch 2923 loss: 2.7356247901916504\n",
      "epoch 1 batch 2924 loss: 2.5773870944976807\n",
      "epoch 1 batch 2925 loss: 2.4998738765716553\n",
      "epoch 1 batch 2926 loss: 2.614682197570801\n",
      "epoch 1 batch 2927 loss: 2.3713316917419434\n",
      "epoch 1 batch 2928 loss: 2.5535073280334473\n",
      "epoch 1 batch 2929 loss: 2.8665823936462402\n",
      "epoch 1 batch 2930 loss: 2.5164341926574707\n",
      "epoch 1 batch 2931 loss: 2.761129856109619\n",
      "epoch 1 batch 2932 loss: 2.4255623817443848\n",
      "epoch 1 batch 2933 loss: 2.607722759246826\n",
      "epoch 1 batch 2934 loss: 2.417102336883545\n",
      "epoch 1 batch 2935 loss: 2.618239164352417\n",
      "epoch 1 batch 2936 loss: 2.4135923385620117\n",
      "epoch 1 batch 2937 loss: 2.572387218475342\n",
      "epoch 1 batch 2938 loss: 2.43890380859375\n",
      "epoch 1 batch 2939 loss: 2.8418822288513184\n",
      "epoch 1 batch 2940 loss: 2.4273691177368164\n",
      "epoch 1 batch 2941 loss: 2.962794065475464\n",
      "epoch 1 batch 2942 loss: 2.8985607624053955\n",
      "epoch 1 batch 2943 loss: 2.378995895385742\n",
      "epoch 1 batch 2944 loss: 2.6944379806518555\n",
      "epoch 1 batch 2945 loss: 2.5872225761413574\n",
      "epoch 1 batch 2946 loss: 2.6593916416168213\n",
      "epoch 1 batch 2947 loss: 2.6719467639923096\n",
      "epoch 1 batch 2948 loss: 2.6897482872009277\n",
      "epoch 1 batch 2949 loss: 2.626546859741211\n",
      "epoch 1 batch 2950 loss: 2.663017511367798\n",
      "epoch 1 batch 2951 loss: 2.801879644393921\n",
      "epoch 1 batch 2952 loss: 2.754422664642334\n",
      "epoch 1 batch 2953 loss: 2.700517177581787\n",
      "epoch 1 batch 2954 loss: 2.542025089263916\n",
      "epoch 1 batch 2955 loss: 2.69742488861084\n",
      "epoch 1 batch 2956 loss: 2.9095213413238525\n",
      "epoch 1 batch 2957 loss: 2.5095880031585693\n",
      "epoch 1 batch 2958 loss: 2.7050368785858154\n",
      "epoch 1 batch 2959 loss: 2.6429781913757324\n",
      "epoch 1 batch 2960 loss: 2.8321692943573\n",
      "epoch 1 batch 2961 loss: 2.3548948764801025\n",
      "epoch 1 batch 2962 loss: 2.637885570526123\n",
      "epoch 1 batch 2963 loss: 3.0180165767669678\n",
      "epoch 1 batch 2964 loss: 2.9947218894958496\n",
      "epoch 1 batch 2965 loss: 2.546257495880127\n",
      "epoch 1 batch 2966 loss: 2.746450901031494\n",
      "epoch 1 batch 2967 loss: 2.882469415664673\n",
      "epoch 1 batch 2968 loss: 2.393367290496826\n",
      "epoch 1 batch 2969 loss: 2.540753126144409\n",
      "epoch 1 batch 2970 loss: 2.384972095489502\n",
      "epoch 1 batch 2971 loss: 2.922802686691284\n",
      "epoch 1 batch 2972 loss: 2.52685284614563\n",
      "epoch 1 batch 2973 loss: 2.522918701171875\n",
      "epoch 1 batch 2974 loss: 2.744335174560547\n",
      "epoch 1 batch 2975 loss: 2.6802420616149902\n",
      "epoch 1 batch 2976 loss: 2.8262267112731934\n",
      "epoch 1 batch 2977 loss: 2.9199106693267822\n",
      "epoch 1 batch 2978 loss: 2.6830639839172363\n",
      "epoch 1 batch 2979 loss: 2.9471049308776855\n",
      "epoch 1 batch 2980 loss: 2.4728713035583496\n",
      "epoch 1 batch 2981 loss: 2.7269632816314697\n",
      "epoch 1 batch 2982 loss: 2.7553200721740723\n",
      "epoch 1 batch 2983 loss: 2.6571013927459717\n",
      "epoch 1 batch 2984 loss: 2.4040255546569824\n",
      "epoch 1 batch 2985 loss: 2.3914427757263184\n",
      "epoch 1 batch 2986 loss: 2.5090153217315674\n",
      "epoch 1 batch 2987 loss: 2.5632705688476562\n",
      "epoch 1 batch 2988 loss: 2.641599178314209\n",
      "epoch 1 batch 2989 loss: 2.8736047744750977\n",
      "epoch 1 batch 2990 loss: 2.7129976749420166\n",
      "epoch 1 batch 2991 loss: 2.39749813079834\n",
      "epoch 1 batch 2992 loss: 2.6382131576538086\n",
      "epoch 1 batch 2993 loss: 2.412153720855713\n",
      "epoch 1 batch 2994 loss: 2.693502426147461\n",
      "epoch 1 batch 2995 loss: 2.7892301082611084\n",
      "epoch 1 batch 2996 loss: 2.8390064239501953\n",
      "epoch 1 batch 2997 loss: 2.6938552856445312\n",
      "epoch 1 batch 2998 loss: 2.487696647644043\n",
      "epoch 1 batch 2999 loss: 2.834807872772217\n",
      "epoch 1 batch 3000 loss: 2.7054548263549805\n",
      "epoch 1 batch 3001 loss: 2.7634284496307373\n",
      "epoch 1 batch 3002 loss: 2.5579237937927246\n",
      "epoch 1 batch 3003 loss: 2.6549696922302246\n",
      "epoch 1 batch 3004 loss: 2.4597525596618652\n",
      "epoch 1 batch 3005 loss: 2.461977958679199\n",
      "epoch 1 batch 3006 loss: 2.461418867111206\n",
      "epoch 1 batch 3007 loss: 2.40822172164917\n",
      "epoch 1 batch 3008 loss: 2.7182178497314453\n",
      "epoch 1 batch 3009 loss: 2.5501389503479004\n",
      "epoch 1 batch 3010 loss: 2.5240471363067627\n",
      "epoch 1 batch 3011 loss: 2.5434718132019043\n",
      "epoch 1 batch 3012 loss: 2.6202268600463867\n",
      "epoch 1 batch 3013 loss: 2.398869752883911\n",
      "epoch 1 batch 3014 loss: 2.6736602783203125\n",
      "epoch 1 batch 3015 loss: 2.6310672760009766\n",
      "epoch 1 batch 3016 loss: 2.4955203533172607\n",
      "epoch 1 batch 3017 loss: 2.589388370513916\n",
      "epoch 1 batch 3018 loss: 2.6838009357452393\n",
      "epoch 1 batch 3019 loss: 2.757732629776001\n",
      "epoch 1 batch 3020 loss: 3.1400506496429443\n",
      "epoch 1 batch 3021 loss: 2.7750489711761475\n",
      "epoch 1 batch 3022 loss: 2.914693832397461\n",
      "epoch 1 batch 3023 loss: 2.8001585006713867\n",
      "epoch 1 batch 3024 loss: 2.8997206687927246\n",
      "epoch 1 batch 3025 loss: 3.038316249847412\n",
      "epoch 1 batch 3026 loss: 2.494370460510254\n",
      "epoch 1 batch 3027 loss: 2.717853546142578\n",
      "epoch 1 batch 3028 loss: 2.823943853378296\n",
      "epoch 1 batch 3029 loss: 2.268113613128662\n",
      "epoch 1 batch 3030 loss: 2.401362895965576\n",
      "epoch 1 batch 3031 loss: 2.481330394744873\n",
      "epoch 1 batch 3032 loss: 2.57765531539917\n",
      "epoch 1 batch 3033 loss: 3.153648614883423\n",
      "epoch 1 batch 3034 loss: 2.8652424812316895\n",
      "epoch 1 batch 3035 loss: 2.7469472885131836\n",
      "epoch 1 batch 3036 loss: 3.070101022720337\n",
      "epoch 1 batch 3037 loss: 2.4966256618499756\n",
      "epoch 1 batch 3038 loss: 2.9551589488983154\n",
      "epoch 1 batch 3039 loss: 2.7631800174713135\n",
      "epoch 1 batch 3040 loss: 2.9214324951171875\n",
      "epoch 1 batch 3041 loss: 2.4412145614624023\n",
      "epoch 1 batch 3042 loss: 2.6666924953460693\n",
      "epoch 1 batch 3043 loss: 2.2171013355255127\n",
      "epoch 1 batch 3044 loss: 2.4230127334594727\n",
      "epoch 1 batch 3045 loss: 2.667633056640625\n",
      "epoch 1 batch 3046 loss: 2.6614432334899902\n",
      "epoch 1 batch 3047 loss: 2.3504226207733154\n",
      "epoch 1 batch 3048 loss: 2.409888744354248\n",
      "epoch 1 batch 3049 loss: 2.3873448371887207\n",
      "epoch 1 batch 3050 loss: 2.5602593421936035\n",
      "epoch 1 batch 3051 loss: 2.5885708332061768\n",
      "epoch 1 batch 3052 loss: 2.5233707427978516\n",
      "epoch 1 batch 3053 loss: 2.6847946643829346\n",
      "epoch 1 batch 3054 loss: 2.5949089527130127\n",
      "epoch 1 batch 3055 loss: 2.844209671020508\n",
      "epoch 1 batch 3056 loss: 2.310197114944458\n",
      "epoch 1 batch 3057 loss: 2.5688891410827637\n",
      "epoch 1 batch 3058 loss: 3.0162062644958496\n",
      "epoch 1 batch 3059 loss: 2.8158745765686035\n",
      "epoch 1 batch 3060 loss: 2.6440489292144775\n",
      "epoch 1 batch 3061 loss: 2.673042058944702\n",
      "epoch 1 batch 3062 loss: 2.547518253326416\n",
      "epoch 1 batch 3063 loss: 2.952259063720703\n",
      "epoch 1 batch 3064 loss: 2.227612018585205\n",
      "epoch 1 batch 3065 loss: 2.6673355102539062\n",
      "epoch 1 batch 3066 loss: 2.533021926879883\n",
      "epoch 1 batch 3067 loss: 2.4671342372894287\n",
      "epoch 1 batch 3068 loss: 2.6729862689971924\n",
      "epoch 1 batch 3069 loss: 2.667940855026245\n",
      "epoch 1 batch 3070 loss: 2.521641731262207\n",
      "epoch 1 batch 3071 loss: 2.516176700592041\n",
      "epoch 1 batch 3072 loss: 2.5562548637390137\n",
      "epoch 1 batch 3073 loss: 2.660475730895996\n",
      "epoch 1 batch 3074 loss: 3.2457222938537598\n",
      "epoch 1 batch 3075 loss: 2.574303150177002\n",
      "epoch 1 batch 3076 loss: 2.7695870399475098\n",
      "epoch 1 batch 3077 loss: 2.425689935684204\n",
      "epoch 1 batch 3078 loss: 2.9186134338378906\n",
      "epoch 1 batch 3079 loss: 2.549534320831299\n",
      "epoch 1 batch 3080 loss: 2.3281407356262207\n",
      "epoch 1 batch 3081 loss: 2.7318053245544434\n",
      "epoch 1 batch 3082 loss: 2.512539863586426\n",
      "epoch 1 batch 3083 loss: 2.7232608795166016\n",
      "epoch 1 batch 3084 loss: 2.9296841621398926\n",
      "epoch 1 batch 3085 loss: 2.468479633331299\n",
      "epoch 1 batch 3086 loss: 2.6579792499542236\n",
      "epoch 1 batch 3087 loss: 2.5782299041748047\n",
      "epoch 1 batch 3088 loss: 2.622345447540283\n",
      "epoch 1 batch 3089 loss: 2.389807939529419\n",
      "epoch 1 batch 3090 loss: 2.7976880073547363\n",
      "epoch 1 batch 3091 loss: 2.831244468688965\n",
      "epoch 1 batch 3092 loss: 2.831885576248169\n",
      "epoch 1 batch 3093 loss: 2.922079563140869\n",
      "epoch 1 batch 3094 loss: 2.4462406635284424\n",
      "epoch 1 batch 3095 loss: 2.4419074058532715\n",
      "epoch 1 batch 3096 loss: 2.514131546020508\n",
      "epoch 1 batch 3097 loss: 2.8572964668273926\n",
      "epoch 1 batch 3098 loss: 2.824798583984375\n",
      "epoch 1 batch 3099 loss: 2.7380623817443848\n",
      "epoch 1 batch 3100 loss: 2.8793816566467285\n",
      "epoch 1 batch 3101 loss: 2.4601399898529053\n",
      "epoch 1 batch 3102 loss: 2.8132128715515137\n",
      "epoch 1 batch 3103 loss: 2.5311827659606934\n",
      "epoch 1 batch 3104 loss: 2.96287202835083\n",
      "epoch 1 batch 3105 loss: 2.46201229095459\n",
      "epoch 1 batch 3106 loss: 3.009152412414551\n",
      "epoch 1 batch 3107 loss: 2.7550525665283203\n",
      "epoch 1 batch 3108 loss: 2.596390724182129\n",
      "epoch 1 batch 3109 loss: 2.8240294456481934\n",
      "epoch 1 batch 3110 loss: 2.602564573287964\n",
      "epoch 1 batch 3111 loss: 2.5643463134765625\n",
      "epoch 1 batch 3112 loss: 2.5448157787323\n",
      "epoch 1 batch 3113 loss: 2.5539121627807617\n",
      "epoch 1 batch 3114 loss: 2.6402769088745117\n",
      "epoch 1 batch 3115 loss: 2.783322811126709\n",
      "epoch 1 batch 3116 loss: 2.423891067504883\n",
      "epoch 1 batch 3117 loss: 2.685025215148926\n",
      "epoch 1 batch 3118 loss: 2.6155247688293457\n",
      "epoch 1 batch 3119 loss: 2.562420129776001\n",
      "epoch 1 batch 3120 loss: 2.71600341796875\n",
      "epoch 1 batch 3121 loss: 3.016713857650757\n",
      "epoch 1 batch 3122 loss: 2.699157238006592\n",
      "epoch 1 batch 3123 loss: 2.7103958129882812\n",
      "epoch 1 batch 3124 loss: 2.461818218231201\n",
      "epoch loss: 2.816603406829834\n",
      "epoch 2 batch 0 loss: 2.446789026260376\n",
      "epoch 2 batch 1 loss: 2.417048931121826\n",
      "epoch 2 batch 2 loss: 2.6122303009033203\n",
      "epoch 2 batch 3 loss: 3.2924671173095703\n",
      "epoch 2 batch 4 loss: 2.981538772583008\n",
      "epoch 2 batch 5 loss: 2.879589557647705\n",
      "epoch 2 batch 6 loss: 2.751469135284424\n",
      "epoch 2 batch 7 loss: 2.7538259029388428\n",
      "epoch 2 batch 8 loss: 2.3817379474639893\n",
      "epoch 2 batch 9 loss: 2.304388999938965\n",
      "epoch 2 batch 10 loss: 2.6691365242004395\n",
      "epoch 2 batch 11 loss: 2.529465436935425\n",
      "epoch 2 batch 12 loss: 2.5362422466278076\n",
      "epoch 2 batch 13 loss: 2.8049426078796387\n",
      "epoch 2 batch 14 loss: 2.8086633682250977\n",
      "epoch 2 batch 15 loss: 2.2698044776916504\n",
      "epoch 2 batch 16 loss: 2.4028735160827637\n",
      "epoch 2 batch 17 loss: 3.0237138271331787\n",
      "epoch 2 batch 18 loss: 2.746253490447998\n",
      "epoch 2 batch 19 loss: 2.843155860900879\n",
      "epoch 2 batch 20 loss: 2.344580888748169\n",
      "epoch 2 batch 21 loss: 2.3625588417053223\n",
      "epoch 2 batch 22 loss: 2.7515087127685547\n",
      "epoch 2 batch 23 loss: 2.5984625816345215\n",
      "epoch 2 batch 24 loss: 2.5904741287231445\n",
      "epoch 2 batch 25 loss: 2.8697853088378906\n",
      "epoch 2 batch 26 loss: 2.887601852416992\n",
      "epoch 2 batch 27 loss: 2.9470930099487305\n",
      "epoch 2 batch 28 loss: 2.327756404876709\n",
      "epoch 2 batch 29 loss: 2.6446399688720703\n",
      "epoch 2 batch 30 loss: 2.6752703189849854\n",
      "epoch 2 batch 31 loss: 2.6472251415252686\n",
      "epoch 2 batch 32 loss: 2.6692750453948975\n",
      "epoch 2 batch 33 loss: 2.670711040496826\n",
      "epoch 2 batch 34 loss: 3.0498225688934326\n",
      "epoch 2 batch 35 loss: 2.7922897338867188\n",
      "epoch 2 batch 36 loss: 2.5835938453674316\n",
      "epoch 2 batch 37 loss: 2.5672194957733154\n",
      "epoch 2 batch 38 loss: 2.440218687057495\n",
      "epoch 2 batch 39 loss: 2.303185224533081\n",
      "epoch 2 batch 40 loss: 2.602271556854248\n",
      "epoch 2 batch 41 loss: 2.5803580284118652\n",
      "epoch 2 batch 42 loss: 2.560948610305786\n",
      "epoch 2 batch 43 loss: 2.4723312854766846\n",
      "epoch 2 batch 44 loss: 2.601473093032837\n",
      "epoch 2 batch 45 loss: 2.999159812927246\n",
      "epoch 2 batch 46 loss: 2.4285099506378174\n",
      "epoch 2 batch 47 loss: 2.4722423553466797\n",
      "epoch 2 batch 48 loss: 2.7564239501953125\n",
      "epoch 2 batch 49 loss: 2.7240817546844482\n",
      "epoch 2 batch 50 loss: 2.481311082839966\n",
      "epoch 2 batch 51 loss: 2.435905933380127\n",
      "epoch 2 batch 52 loss: 2.4827167987823486\n",
      "epoch 2 batch 53 loss: 2.858889579772949\n",
      "epoch 2 batch 54 loss: 2.4323549270629883\n",
      "epoch 2 batch 55 loss: 2.465958595275879\n",
      "epoch 2 batch 56 loss: 2.9253833293914795\n",
      "epoch 2 batch 57 loss: 2.663865566253662\n",
      "epoch 2 batch 58 loss: 2.389457941055298\n",
      "epoch 2 batch 59 loss: 2.723548412322998\n",
      "epoch 2 batch 60 loss: 2.992910385131836\n",
      "epoch 2 batch 61 loss: 2.612442970275879\n",
      "epoch 2 batch 62 loss: 2.3212506771087646\n",
      "epoch 2 batch 63 loss: 2.6087727546691895\n",
      "epoch 2 batch 64 loss: 2.76391339302063\n",
      "epoch 2 batch 65 loss: 3.004970073699951\n",
      "epoch 2 batch 66 loss: 2.748812675476074\n",
      "epoch 2 batch 67 loss: 2.8117458820343018\n",
      "epoch 2 batch 68 loss: 2.628751754760742\n",
      "epoch 2 batch 69 loss: 2.629478931427002\n",
      "epoch 2 batch 70 loss: 2.8624653816223145\n",
      "epoch 2 batch 71 loss: 2.583873987197876\n",
      "epoch 2 batch 72 loss: 2.8399851322174072\n",
      "epoch 2 batch 73 loss: 2.292820692062378\n",
      "epoch 2 batch 74 loss: 2.7968852519989014\n",
      "epoch 2 batch 75 loss: 2.304441452026367\n",
      "epoch 2 batch 76 loss: 2.7376646995544434\n",
      "epoch 2 batch 77 loss: 2.4948623180389404\n",
      "epoch 2 batch 78 loss: 2.907815933227539\n",
      "epoch 2 batch 79 loss: 2.6740968227386475\n",
      "epoch 2 batch 80 loss: 2.5741727352142334\n",
      "epoch 2 batch 81 loss: 2.5413873195648193\n",
      "epoch 2 batch 82 loss: 2.718331813812256\n",
      "epoch 2 batch 83 loss: 2.460897445678711\n",
      "epoch 2 batch 84 loss: 3.1654672622680664\n",
      "epoch 2 batch 85 loss: 2.527834177017212\n",
      "epoch 2 batch 86 loss: 2.7301740646362305\n",
      "epoch 2 batch 87 loss: 2.600456714630127\n",
      "epoch 2 batch 88 loss: 2.8466644287109375\n",
      "epoch 2 batch 89 loss: 2.4922797679901123\n",
      "epoch 2 batch 90 loss: 2.822573184967041\n",
      "epoch 2 batch 91 loss: 2.7908949851989746\n",
      "epoch 2 batch 92 loss: 2.5286407470703125\n",
      "epoch 2 batch 93 loss: 2.473132610321045\n",
      "epoch 2 batch 94 loss: 2.5630500316619873\n",
      "epoch 2 batch 95 loss: 2.611999034881592\n",
      "epoch 2 batch 96 loss: 2.391836404800415\n",
      "epoch 2 batch 97 loss: 2.5297160148620605\n",
      "epoch 2 batch 98 loss: 2.8563737869262695\n",
      "epoch 2 batch 99 loss: 2.709517478942871\n",
      "epoch 2 batch 100 loss: 2.783473491668701\n",
      "epoch 2 batch 101 loss: 2.567904233932495\n",
      "epoch 2 batch 102 loss: 2.4710001945495605\n",
      "epoch 2 batch 103 loss: 2.3173773288726807\n",
      "epoch 2 batch 104 loss: 2.588329315185547\n",
      "epoch 2 batch 105 loss: 2.3822946548461914\n",
      "epoch 2 batch 106 loss: 2.58463191986084\n",
      "epoch 2 batch 107 loss: 2.9774844646453857\n",
      "epoch 2 batch 108 loss: 2.8104143142700195\n",
      "epoch 2 batch 109 loss: 2.3187127113342285\n",
      "epoch 2 batch 110 loss: 2.5532312393188477\n",
      "epoch 2 batch 111 loss: 2.637065887451172\n",
      "epoch 2 batch 112 loss: 2.7845189571380615\n",
      "epoch 2 batch 113 loss: 2.7072699069976807\n",
      "epoch 2 batch 114 loss: 2.617614507675171\n",
      "epoch 2 batch 115 loss: 2.9088077545166016\n",
      "epoch 2 batch 116 loss: 2.459651470184326\n",
      "epoch 2 batch 117 loss: 2.975778818130493\n",
      "epoch 2 batch 118 loss: 2.631608009338379\n",
      "epoch 2 batch 119 loss: 2.7868378162384033\n",
      "epoch 2 batch 120 loss: 2.7692618370056152\n",
      "epoch 2 batch 121 loss: 2.5976245403289795\n",
      "epoch 2 batch 122 loss: 2.7318663597106934\n",
      "epoch 2 batch 123 loss: 2.1626219749450684\n",
      "epoch 2 batch 124 loss: 2.930997610092163\n",
      "epoch 2 batch 125 loss: 2.578204393386841\n",
      "epoch 2 batch 126 loss: 2.5461692810058594\n",
      "epoch 2 batch 127 loss: 2.6202452182769775\n",
      "epoch 2 batch 128 loss: 2.773914337158203\n",
      "epoch 2 batch 129 loss: 2.682034969329834\n",
      "epoch 2 batch 130 loss: 2.434748649597168\n",
      "epoch 2 batch 131 loss: 2.609462022781372\n",
      "epoch 2 batch 132 loss: 2.782386302947998\n",
      "epoch 2 batch 133 loss: 2.592618227005005\n",
      "epoch 2 batch 134 loss: 2.6338963508605957\n",
      "epoch 2 batch 135 loss: 2.6942338943481445\n",
      "epoch 2 batch 136 loss: 2.818056583404541\n",
      "epoch 2 batch 137 loss: 2.6625585556030273\n",
      "epoch 2 batch 138 loss: 2.7789175510406494\n",
      "epoch 2 batch 139 loss: 2.382906436920166\n",
      "epoch 2 batch 140 loss: 2.662382125854492\n",
      "epoch 2 batch 141 loss: 2.3934531211853027\n",
      "epoch 2 batch 142 loss: 2.5070152282714844\n",
      "epoch 2 batch 143 loss: 2.90622615814209\n",
      "epoch 2 batch 144 loss: 2.1736207008361816\n",
      "epoch 2 batch 145 loss: 2.734753131866455\n",
      "epoch 2 batch 146 loss: 2.864009141921997\n",
      "epoch 2 batch 147 loss: 2.875582456588745\n",
      "epoch 2 batch 148 loss: 2.3347034454345703\n",
      "epoch 2 batch 149 loss: 2.242063045501709\n",
      "epoch 2 batch 150 loss: 2.3011555671691895\n",
      "epoch 2 batch 151 loss: 2.5618245601654053\n",
      "epoch 2 batch 152 loss: 2.486701011657715\n",
      "epoch 2 batch 153 loss: 2.805659770965576\n",
      "epoch 2 batch 154 loss: 2.519387722015381\n",
      "epoch 2 batch 155 loss: 2.796816825866699\n",
      "epoch 2 batch 156 loss: 2.668123245239258\n",
      "epoch 2 batch 157 loss: 2.5970358848571777\n",
      "epoch 2 batch 158 loss: 2.7298827171325684\n",
      "epoch 2 batch 159 loss: 2.405797243118286\n",
      "epoch 2 batch 160 loss: 2.575640916824341\n",
      "epoch 2 batch 161 loss: 2.4097952842712402\n",
      "epoch 2 batch 162 loss: 2.694490909576416\n",
      "epoch 2 batch 163 loss: 2.58797025680542\n",
      "epoch 2 batch 164 loss: 2.751767635345459\n",
      "epoch 2 batch 165 loss: 2.8817591667175293\n",
      "epoch 2 batch 166 loss: 2.7790379524230957\n",
      "epoch 2 batch 167 loss: 2.7088708877563477\n",
      "epoch 2 batch 168 loss: 2.8383026123046875\n",
      "epoch 2 batch 169 loss: 2.6680893898010254\n",
      "epoch 2 batch 170 loss: 2.4820382595062256\n",
      "epoch 2 batch 171 loss: 2.9243199825286865\n",
      "epoch 2 batch 172 loss: 2.4273524284362793\n",
      "epoch 2 batch 173 loss: 2.5266666412353516\n",
      "epoch 2 batch 174 loss: 2.6155409812927246\n",
      "epoch 2 batch 175 loss: 2.6846823692321777\n",
      "epoch 2 batch 176 loss: 2.7359113693237305\n",
      "epoch 2 batch 177 loss: 2.7584290504455566\n",
      "epoch 2 batch 178 loss: 2.5498034954071045\n",
      "epoch 2 batch 179 loss: 2.551732301712036\n",
      "epoch 2 batch 180 loss: 2.345458507537842\n",
      "epoch 2 batch 181 loss: 2.249479055404663\n",
      "epoch 2 batch 182 loss: 2.243788719177246\n",
      "epoch 2 batch 183 loss: 2.735055446624756\n",
      "epoch 2 batch 184 loss: 2.5192887783050537\n",
      "epoch 2 batch 185 loss: 2.590394973754883\n",
      "epoch 2 batch 186 loss: 2.914588689804077\n",
      "epoch 2 batch 187 loss: 2.8309683799743652\n",
      "epoch 2 batch 188 loss: 2.665700912475586\n",
      "epoch 2 batch 189 loss: 2.3875832557678223\n",
      "epoch 2 batch 190 loss: 2.3626136779785156\n",
      "epoch 2 batch 191 loss: 2.7534003257751465\n",
      "epoch 2 batch 192 loss: 2.5336241722106934\n",
      "epoch 2 batch 193 loss: 2.647106647491455\n",
      "epoch 2 batch 194 loss: 2.230842113494873\n",
      "epoch 2 batch 195 loss: 2.3570704460144043\n",
      "epoch 2 batch 196 loss: 2.652066469192505\n",
      "epoch 2 batch 197 loss: 2.4663352966308594\n",
      "epoch 2 batch 198 loss: 2.4481258392333984\n",
      "epoch 2 batch 199 loss: 2.363002300262451\n",
      "epoch 2 batch 200 loss: 2.4807729721069336\n",
      "epoch 2 batch 201 loss: 2.5763916969299316\n",
      "epoch 2 batch 202 loss: 2.6136083602905273\n",
      "epoch 2 batch 203 loss: 2.3716444969177246\n",
      "epoch 2 batch 204 loss: 2.2498795986175537\n",
      "epoch 2 batch 205 loss: 2.4158668518066406\n",
      "epoch 2 batch 206 loss: 2.8063762187957764\n",
      "epoch 2 batch 207 loss: 2.440408945083618\n",
      "epoch 2 batch 208 loss: 2.818723440170288\n",
      "epoch 2 batch 209 loss: 2.5687990188598633\n",
      "epoch 2 batch 210 loss: 2.304826498031616\n",
      "epoch 2 batch 211 loss: 2.7420291900634766\n",
      "epoch 2 batch 212 loss: 2.5489230155944824\n",
      "epoch 2 batch 213 loss: 2.5549888610839844\n",
      "epoch 2 batch 214 loss: 2.8416824340820312\n",
      "epoch 2 batch 215 loss: 2.7958531379699707\n",
      "epoch 2 batch 216 loss: 2.545163154602051\n",
      "epoch 2 batch 217 loss: 2.693828582763672\n",
      "epoch 2 batch 218 loss: 2.492415428161621\n",
      "epoch 2 batch 219 loss: 2.5263826847076416\n",
      "epoch 2 batch 220 loss: 2.7243473529815674\n",
      "epoch 2 batch 221 loss: 2.317774772644043\n",
      "epoch 2 batch 222 loss: 2.567213296890259\n",
      "epoch 2 batch 223 loss: 2.3550829887390137\n",
      "epoch 2 batch 224 loss: 2.521073341369629\n",
      "epoch 2 batch 225 loss: 2.2294278144836426\n",
      "epoch 2 batch 226 loss: 2.5205321311950684\n",
      "epoch 2 batch 227 loss: 2.399303674697876\n",
      "epoch 2 batch 228 loss: 2.718477725982666\n",
      "epoch 2 batch 229 loss: 2.2629685401916504\n",
      "epoch 2 batch 230 loss: 2.731281280517578\n",
      "epoch 2 batch 231 loss: 2.624685764312744\n",
      "epoch 2 batch 232 loss: 2.5312561988830566\n",
      "epoch 2 batch 233 loss: 2.7664971351623535\n",
      "epoch 2 batch 234 loss: 2.474120616912842\n",
      "epoch 2 batch 235 loss: 2.9332072734832764\n",
      "epoch 2 batch 236 loss: 2.5098934173583984\n",
      "epoch 2 batch 237 loss: 2.5017852783203125\n",
      "epoch 2 batch 238 loss: 2.620953321456909\n",
      "epoch 2 batch 239 loss: 2.4369654655456543\n",
      "epoch 2 batch 240 loss: 2.7305490970611572\n",
      "epoch 2 batch 241 loss: 2.5954418182373047\n",
      "epoch 2 batch 242 loss: 2.4840123653411865\n",
      "epoch 2 batch 243 loss: 2.70914363861084\n",
      "epoch 2 batch 244 loss: 2.585604190826416\n",
      "epoch 2 batch 245 loss: 2.64333176612854\n",
      "epoch 2 batch 246 loss: 2.496593475341797\n",
      "epoch 2 batch 247 loss: 2.303860664367676\n",
      "epoch 2 batch 248 loss: 2.750663995742798\n",
      "epoch 2 batch 249 loss: 2.68876576423645\n",
      "epoch 2 batch 250 loss: 2.6972527503967285\n",
      "epoch 2 batch 251 loss: 2.3466668128967285\n",
      "epoch 2 batch 252 loss: 2.7566614151000977\n",
      "epoch 2 batch 253 loss: 2.3933820724487305\n",
      "epoch 2 batch 254 loss: 2.4400570392608643\n",
      "epoch 2 batch 255 loss: 2.309562921524048\n",
      "epoch 2 batch 256 loss: 2.4045774936676025\n",
      "epoch 2 batch 257 loss: 2.585871458053589\n",
      "epoch 2 batch 258 loss: 2.5262417793273926\n",
      "epoch 2 batch 259 loss: 2.4825148582458496\n",
      "epoch 2 batch 260 loss: 2.606092929840088\n",
      "epoch 2 batch 261 loss: 2.313748359680176\n",
      "epoch 2 batch 262 loss: 2.6679036617279053\n",
      "epoch 2 batch 263 loss: 2.5920238494873047\n",
      "epoch 2 batch 264 loss: 2.4551188945770264\n",
      "epoch 2 batch 265 loss: 2.778071165084839\n",
      "epoch 2 batch 266 loss: 2.650200366973877\n",
      "epoch 2 batch 267 loss: 2.4434432983398438\n",
      "epoch 2 batch 268 loss: 2.404017448425293\n",
      "epoch 2 batch 269 loss: 2.3980536460876465\n",
      "epoch 2 batch 270 loss: 2.241922378540039\n",
      "epoch 2 batch 271 loss: 2.394423007965088\n",
      "epoch 2 batch 272 loss: 2.7781944274902344\n",
      "epoch 2 batch 273 loss: 2.7203283309936523\n",
      "epoch 2 batch 274 loss: 3.027817726135254\n",
      "epoch 2 batch 275 loss: 2.6348443031311035\n",
      "epoch 2 batch 276 loss: 2.2971792221069336\n",
      "epoch 2 batch 277 loss: 2.5401554107666016\n",
      "epoch 2 batch 278 loss: 2.6041622161865234\n",
      "epoch 2 batch 279 loss: 2.1743927001953125\n",
      "epoch 2 batch 280 loss: 2.6486294269561768\n",
      "epoch 2 batch 281 loss: 2.4973485469818115\n",
      "epoch 2 batch 282 loss: 2.515252113342285\n",
      "epoch 2 batch 283 loss: 2.637660026550293\n",
      "epoch 2 batch 284 loss: 3.3036842346191406\n",
      "epoch 2 batch 285 loss: 2.762939453125\n",
      "epoch 2 batch 286 loss: 2.870687246322632\n",
      "epoch 2 batch 287 loss: 2.446176528930664\n",
      "epoch 2 batch 288 loss: 2.2396488189697266\n",
      "epoch 2 batch 289 loss: 2.5281524658203125\n",
      "epoch 2 batch 290 loss: 3.0036778450012207\n",
      "epoch 2 batch 291 loss: 2.5155811309814453\n",
      "epoch 2 batch 292 loss: 2.4978890419006348\n",
      "epoch 2 batch 293 loss: 2.5429604053497314\n",
      "epoch 2 batch 294 loss: 2.2430529594421387\n",
      "epoch 2 batch 295 loss: 2.5880746841430664\n",
      "epoch 2 batch 296 loss: 2.7834296226501465\n",
      "epoch 2 batch 297 loss: 2.6602537631988525\n",
      "epoch 2 batch 298 loss: 3.0402214527130127\n",
      "epoch 2 batch 299 loss: 2.5077767372131348\n",
      "epoch 2 batch 300 loss: 3.049086093902588\n",
      "epoch 2 batch 301 loss: 2.7519381046295166\n",
      "epoch 2 batch 302 loss: 2.722588062286377\n",
      "epoch 2 batch 303 loss: 2.6649436950683594\n",
      "epoch 2 batch 304 loss: 2.8819193840026855\n",
      "epoch 2 batch 305 loss: 2.3417420387268066\n",
      "epoch 2 batch 306 loss: 2.5736031532287598\n",
      "epoch 2 batch 307 loss: 2.3961687088012695\n",
      "epoch 2 batch 308 loss: 2.28971791267395\n",
      "epoch 2 batch 309 loss: 2.6288719177246094\n",
      "epoch 2 batch 310 loss: 2.8771584033966064\n",
      "epoch 2 batch 311 loss: 2.611269235610962\n",
      "epoch 2 batch 312 loss: 2.6050214767456055\n",
      "epoch 2 batch 313 loss: 2.3719520568847656\n",
      "epoch 2 batch 314 loss: 2.457371950149536\n",
      "epoch 2 batch 315 loss: 2.181100845336914\n",
      "epoch 2 batch 316 loss: 2.2777199745178223\n",
      "epoch 2 batch 317 loss: 2.5441341400146484\n",
      "epoch 2 batch 318 loss: 2.838606119155884\n",
      "epoch 2 batch 319 loss: 2.7835464477539062\n",
      "epoch 2 batch 320 loss: 2.6004111766815186\n",
      "epoch 2 batch 321 loss: 2.6886954307556152\n",
      "epoch 2 batch 322 loss: 2.413423538208008\n",
      "epoch 2 batch 323 loss: 2.5075855255126953\n",
      "epoch 2 batch 324 loss: 2.6540026664733887\n",
      "epoch 2 batch 325 loss: 2.7995197772979736\n",
      "epoch 2 batch 326 loss: 2.7512969970703125\n",
      "epoch 2 batch 327 loss: 2.529566764831543\n",
      "epoch 2 batch 328 loss: 2.654726982116699\n",
      "epoch 2 batch 329 loss: 2.6334004402160645\n",
      "epoch 2 batch 330 loss: 2.5607402324676514\n",
      "epoch 2 batch 331 loss: 2.203477621078491\n",
      "epoch 2 batch 332 loss: 2.5352373123168945\n",
      "epoch 2 batch 333 loss: 2.6084518432617188\n",
      "epoch 2 batch 334 loss: 3.001861572265625\n",
      "epoch 2 batch 335 loss: 2.721736431121826\n",
      "epoch 2 batch 336 loss: 2.436244010925293\n",
      "epoch 2 batch 337 loss: 2.5154926776885986\n",
      "epoch 2 batch 338 loss: 2.5425117015838623\n",
      "epoch 2 batch 339 loss: 2.8532979488372803\n",
      "epoch 2 batch 340 loss: 2.6263554096221924\n",
      "epoch 2 batch 341 loss: 2.6948232650756836\n",
      "epoch 2 batch 342 loss: 2.6548025608062744\n",
      "epoch 2 batch 343 loss: 2.557858467102051\n",
      "epoch 2 batch 344 loss: 2.598294734954834\n",
      "epoch 2 batch 345 loss: 2.698124647140503\n",
      "epoch 2 batch 346 loss: 2.69309139251709\n",
      "epoch 2 batch 347 loss: 2.533984422683716\n",
      "epoch 2 batch 348 loss: 2.454089403152466\n",
      "epoch 2 batch 349 loss: 2.979832649230957\n",
      "epoch 2 batch 350 loss: 2.6570065021514893\n",
      "epoch 2 batch 351 loss: 2.2823314666748047\n",
      "epoch 2 batch 352 loss: 2.753312587738037\n",
      "epoch 2 batch 353 loss: 2.589318037033081\n",
      "epoch 2 batch 354 loss: 2.436877727508545\n",
      "epoch 2 batch 355 loss: 2.4366679191589355\n",
      "epoch 2 batch 356 loss: 2.583373546600342\n",
      "epoch 2 batch 357 loss: 2.537325143814087\n",
      "epoch 2 batch 358 loss: 2.6792333126068115\n",
      "epoch 2 batch 359 loss: 2.7171716690063477\n",
      "epoch 2 batch 360 loss: 2.692093849182129\n",
      "epoch 2 batch 361 loss: 2.5063681602478027\n",
      "epoch 2 batch 362 loss: 2.59140682220459\n",
      "epoch 2 batch 363 loss: 2.313547134399414\n",
      "epoch 2 batch 364 loss: 2.6741251945495605\n",
      "epoch 2 batch 365 loss: 2.880913257598877\n",
      "epoch 2 batch 366 loss: 3.0900840759277344\n",
      "epoch 2 batch 367 loss: 2.3317811489105225\n",
      "epoch 2 batch 368 loss: 2.6399407386779785\n",
      "epoch 2 batch 369 loss: 2.5333826541900635\n",
      "epoch 2 batch 370 loss: 2.5236849784851074\n",
      "epoch 2 batch 371 loss: 2.4852566719055176\n",
      "epoch 2 batch 372 loss: 2.602047920227051\n",
      "epoch 2 batch 373 loss: 2.571556568145752\n",
      "epoch 2 batch 374 loss: 2.4683074951171875\n",
      "epoch 2 batch 375 loss: 2.6443395614624023\n",
      "epoch 2 batch 376 loss: 2.5410754680633545\n",
      "epoch 2 batch 377 loss: 2.571932315826416\n",
      "epoch 2 batch 378 loss: 2.6953253746032715\n",
      "epoch 2 batch 379 loss: 2.4579224586486816\n",
      "epoch 2 batch 380 loss: 2.5378551483154297\n",
      "epoch 2 batch 381 loss: 2.373906135559082\n",
      "epoch 2 batch 382 loss: 2.560727596282959\n",
      "epoch 2 batch 383 loss: 2.852402687072754\n",
      "epoch 2 batch 384 loss: 2.5182456970214844\n",
      "epoch 2 batch 385 loss: 2.791452407836914\n",
      "epoch 2 batch 386 loss: 2.372640609741211\n",
      "epoch 2 batch 387 loss: 2.5387516021728516\n",
      "epoch 2 batch 388 loss: 2.633413314819336\n",
      "epoch 2 batch 389 loss: 2.8401756286621094\n",
      "epoch 2 batch 390 loss: 2.344681739807129\n",
      "epoch 2 batch 391 loss: 2.6427056789398193\n",
      "epoch 2 batch 392 loss: 2.8208324909210205\n",
      "epoch 2 batch 393 loss: 2.550931930541992\n",
      "epoch 2 batch 394 loss: 2.4309492111206055\n",
      "epoch 2 batch 395 loss: 2.6618003845214844\n",
      "epoch 2 batch 396 loss: 2.6866869926452637\n",
      "epoch 2 batch 397 loss: 2.4500844478607178\n",
      "epoch 2 batch 398 loss: 2.668435573577881\n",
      "epoch 2 batch 399 loss: 2.6318488121032715\n",
      "epoch 2 batch 400 loss: 2.90476655960083\n",
      "epoch 2 batch 401 loss: 2.6466803550720215\n",
      "epoch 2 batch 402 loss: 2.245408535003662\n",
      "epoch 2 batch 403 loss: 2.347215175628662\n",
      "epoch 2 batch 404 loss: 2.628878593444824\n",
      "epoch 2 batch 405 loss: 2.4476897716522217\n",
      "epoch 2 batch 406 loss: 2.8907294273376465\n",
      "epoch 2 batch 407 loss: 2.637115955352783\n",
      "epoch 2 batch 408 loss: 2.47434139251709\n",
      "epoch 2 batch 409 loss: 2.762537956237793\n",
      "epoch 2 batch 410 loss: 2.8718838691711426\n",
      "epoch 2 batch 411 loss: 2.5722532272338867\n",
      "epoch 2 batch 412 loss: 2.626425266265869\n",
      "epoch 2 batch 413 loss: 2.7104671001434326\n",
      "epoch 2 batch 414 loss: 2.9116673469543457\n",
      "epoch 2 batch 415 loss: 2.699387550354004\n",
      "epoch 2 batch 416 loss: 2.7624692916870117\n",
      "epoch 2 batch 417 loss: 2.6364288330078125\n",
      "epoch 2 batch 418 loss: 2.5862674713134766\n",
      "epoch 2 batch 419 loss: 2.4626288414001465\n",
      "epoch 2 batch 420 loss: 2.8440346717834473\n",
      "epoch 2 batch 421 loss: 2.64068603515625\n",
      "epoch 2 batch 422 loss: 2.5626468658447266\n",
      "epoch 2 batch 423 loss: 2.8413290977478027\n",
      "epoch 2 batch 424 loss: 2.692262649536133\n",
      "epoch 2 batch 425 loss: 2.7203989028930664\n",
      "epoch 2 batch 426 loss: 2.4762778282165527\n",
      "epoch 2 batch 427 loss: 2.8180480003356934\n",
      "epoch 2 batch 428 loss: 2.5743887424468994\n",
      "epoch 2 batch 429 loss: 2.575908660888672\n",
      "epoch 2 batch 430 loss: 2.875474452972412\n",
      "epoch 2 batch 431 loss: 2.411630392074585\n",
      "epoch 2 batch 432 loss: 2.274888277053833\n",
      "epoch 2 batch 433 loss: 2.9543375968933105\n",
      "epoch 2 batch 434 loss: 2.5245325565338135\n",
      "epoch 2 batch 435 loss: 2.802663564682007\n",
      "epoch 2 batch 436 loss: 2.7859599590301514\n",
      "epoch 2 batch 437 loss: 2.97222900390625\n",
      "epoch 2 batch 438 loss: 2.779597759246826\n",
      "epoch 2 batch 439 loss: 2.571404457092285\n",
      "epoch 2 batch 440 loss: 2.74591064453125\n",
      "epoch 2 batch 441 loss: 2.820011854171753\n",
      "epoch 2 batch 442 loss: 2.863851547241211\n",
      "epoch 2 batch 443 loss: 2.1662373542785645\n",
      "epoch 2 batch 444 loss: 2.565688371658325\n",
      "epoch 2 batch 445 loss: 2.7302846908569336\n",
      "epoch 2 batch 446 loss: 2.4691362380981445\n",
      "epoch 2 batch 447 loss: 2.528498649597168\n",
      "epoch 2 batch 448 loss: 2.5848355293273926\n",
      "epoch 2 batch 449 loss: 2.9096720218658447\n",
      "epoch 2 batch 450 loss: 2.377702236175537\n",
      "epoch 2 batch 451 loss: 2.3597795963287354\n",
      "epoch 2 batch 452 loss: 2.4931929111480713\n",
      "epoch 2 batch 453 loss: 2.803403377532959\n",
      "epoch 2 batch 454 loss: 2.6200039386749268\n",
      "epoch 2 batch 455 loss: 2.8122000694274902\n",
      "epoch 2 batch 456 loss: 2.582540988922119\n",
      "epoch 2 batch 457 loss: 2.405268430709839\n",
      "epoch 2 batch 458 loss: 2.3519318103790283\n",
      "epoch 2 batch 459 loss: 2.589392900466919\n",
      "epoch 2 batch 460 loss: 2.5362391471862793\n",
      "epoch 2 batch 461 loss: 2.799959897994995\n",
      "epoch 2 batch 462 loss: 2.53342604637146\n",
      "epoch 2 batch 463 loss: 2.475771188735962\n",
      "epoch 2 batch 464 loss: 2.3692989349365234\n",
      "epoch 2 batch 465 loss: 2.754222869873047\n",
      "epoch 2 batch 466 loss: 2.60896635055542\n",
      "epoch 2 batch 467 loss: 3.3269362449645996\n",
      "epoch 2 batch 468 loss: 2.422438621520996\n",
      "epoch 2 batch 469 loss: 2.8720149993896484\n",
      "epoch 2 batch 470 loss: 2.678386688232422\n",
      "epoch 2 batch 471 loss: 2.736692190170288\n",
      "epoch 2 batch 472 loss: 2.593665599822998\n",
      "epoch 2 batch 473 loss: 2.516993522644043\n",
      "epoch 2 batch 474 loss: 2.7060446739196777\n",
      "epoch 2 batch 475 loss: 2.2816267013549805\n",
      "epoch 2 batch 476 loss: 2.4548802375793457\n",
      "epoch 2 batch 477 loss: 2.7142629623413086\n",
      "epoch 2 batch 478 loss: 2.457986831665039\n",
      "epoch 2 batch 479 loss: 2.577547073364258\n",
      "epoch 2 batch 480 loss: 2.550874710083008\n",
      "epoch 2 batch 481 loss: 2.509161949157715\n",
      "epoch 2 batch 482 loss: 2.4514214992523193\n",
      "epoch 2 batch 483 loss: 3.321964979171753\n",
      "epoch 2 batch 484 loss: 2.86348819732666\n",
      "epoch 2 batch 485 loss: 2.6977686882019043\n",
      "epoch 2 batch 486 loss: 2.6587367057800293\n",
      "epoch 2 batch 487 loss: 2.7816944122314453\n",
      "epoch 2 batch 488 loss: 2.292983055114746\n",
      "epoch 2 batch 489 loss: 2.603659152984619\n",
      "epoch 2 batch 490 loss: 2.5968573093414307\n",
      "epoch 2 batch 491 loss: 3.0269081592559814\n",
      "epoch 2 batch 492 loss: 2.4341797828674316\n",
      "epoch 2 batch 493 loss: 2.5643157958984375\n",
      "epoch 2 batch 494 loss: 2.568376064300537\n",
      "epoch 2 batch 495 loss: 2.5054829120635986\n",
      "epoch 2 batch 496 loss: 2.6469154357910156\n",
      "epoch 2 batch 497 loss: 2.613442897796631\n",
      "epoch 2 batch 498 loss: 2.739942789077759\n",
      "epoch 2 batch 499 loss: 2.4002583026885986\n",
      "epoch 2 batch 500 loss: 2.638425588607788\n",
      "epoch 2 batch 501 loss: 2.736027717590332\n",
      "epoch 2 batch 502 loss: 2.4776482582092285\n",
      "epoch 2 batch 503 loss: 2.175910472869873\n",
      "epoch 2 batch 504 loss: 2.281747817993164\n",
      "epoch 2 batch 505 loss: 2.303269147872925\n",
      "epoch 2 batch 506 loss: 2.557429313659668\n",
      "epoch 2 batch 507 loss: 2.6367099285125732\n",
      "epoch 2 batch 508 loss: 2.520646095275879\n",
      "epoch 2 batch 509 loss: 2.724844455718994\n",
      "epoch 2 batch 510 loss: 2.3498919010162354\n",
      "epoch 2 batch 511 loss: 2.5457139015197754\n",
      "epoch 2 batch 512 loss: 2.3417506217956543\n",
      "epoch 2 batch 513 loss: 2.6856136322021484\n",
      "epoch 2 batch 514 loss: 2.5735342502593994\n",
      "epoch 2 batch 515 loss: 2.4589955806732178\n",
      "epoch 2 batch 516 loss: 2.5583362579345703\n",
      "epoch 2 batch 517 loss: 2.5102341175079346\n",
      "epoch 2 batch 518 loss: 2.505977153778076\n",
      "epoch 2 batch 519 loss: 2.790111541748047\n",
      "epoch 2 batch 520 loss: 2.3759891986846924\n",
      "epoch 2 batch 521 loss: 2.9239354133605957\n",
      "epoch 2 batch 522 loss: 2.570890188217163\n",
      "epoch 2 batch 523 loss: 2.2612292766571045\n",
      "epoch 2 batch 524 loss: 2.4548563957214355\n",
      "epoch 2 batch 525 loss: 2.374746322631836\n",
      "epoch 2 batch 526 loss: 2.565751552581787\n",
      "epoch 2 batch 527 loss: 2.4748284816741943\n",
      "epoch 2 batch 528 loss: 2.4341447353363037\n",
      "epoch 2 batch 529 loss: 2.6573128700256348\n",
      "epoch 2 batch 530 loss: 2.499192237854004\n",
      "epoch 2 batch 531 loss: 2.4425454139709473\n",
      "epoch 2 batch 532 loss: 2.4863762855529785\n",
      "epoch 2 batch 533 loss: 2.7474184036254883\n",
      "epoch 2 batch 534 loss: 2.615288734436035\n",
      "epoch 2 batch 535 loss: 2.637525796890259\n",
      "epoch 2 batch 536 loss: 2.8927154541015625\n",
      "epoch 2 batch 537 loss: 2.4778361320495605\n",
      "epoch 2 batch 538 loss: 2.4921445846557617\n",
      "epoch 2 batch 539 loss: 2.5343017578125\n",
      "epoch 2 batch 540 loss: 2.626683235168457\n",
      "epoch 2 batch 541 loss: 2.3320086002349854\n",
      "epoch 2 batch 542 loss: 2.6200919151306152\n",
      "epoch 2 batch 543 loss: 2.621915578842163\n",
      "epoch 2 batch 544 loss: 2.5694262981414795\n",
      "epoch 2 batch 545 loss: 2.3888940811157227\n",
      "epoch 2 batch 546 loss: 2.409069538116455\n",
      "epoch 2 batch 547 loss: 2.568300724029541\n",
      "epoch 2 batch 548 loss: 2.5962815284729004\n",
      "epoch 2 batch 549 loss: 2.45560884475708\n",
      "epoch 2 batch 550 loss: 2.4861831665039062\n",
      "epoch 2 batch 551 loss: 2.632430076599121\n",
      "epoch 2 batch 552 loss: 2.3963189125061035\n",
      "epoch 2 batch 553 loss: 2.977008819580078\n",
      "epoch 2 batch 554 loss: 2.5339951515197754\n",
      "epoch 2 batch 555 loss: 2.4799680709838867\n",
      "epoch 2 batch 556 loss: 2.615507125854492\n",
      "epoch 2 batch 557 loss: 2.538407325744629\n",
      "epoch 2 batch 558 loss: 2.8982372283935547\n",
      "epoch 2 batch 559 loss: 2.4985363483428955\n",
      "epoch 2 batch 560 loss: 2.6877121925354004\n",
      "epoch 2 batch 561 loss: 2.533991575241089\n",
      "epoch 2 batch 562 loss: 2.7487053871154785\n",
      "epoch 2 batch 563 loss: 2.726034641265869\n",
      "epoch 2 batch 564 loss: 2.9107542037963867\n",
      "epoch 2 batch 565 loss: 2.6520795822143555\n",
      "epoch 2 batch 566 loss: 2.4563305377960205\n",
      "epoch 2 batch 567 loss: 2.570436954498291\n",
      "epoch 2 batch 568 loss: 2.5067288875579834\n",
      "epoch 2 batch 569 loss: 2.7982683181762695\n",
      "epoch 2 batch 570 loss: 2.7059152126312256\n",
      "epoch 2 batch 571 loss: 2.885896682739258\n",
      "epoch 2 batch 572 loss: 2.5734825134277344\n",
      "epoch 2 batch 573 loss: 2.3284847736358643\n",
      "epoch 2 batch 574 loss: 2.318364143371582\n",
      "epoch 2 batch 575 loss: 2.548975944519043\n",
      "epoch 2 batch 576 loss: 2.6594762802124023\n",
      "epoch 2 batch 577 loss: 2.784069538116455\n",
      "epoch 2 batch 578 loss: 2.36997127532959\n",
      "epoch 2 batch 579 loss: 2.9605047702789307\n",
      "epoch 2 batch 580 loss: 2.563070058822632\n",
      "epoch 2 batch 581 loss: 2.5114269256591797\n",
      "epoch 2 batch 582 loss: 2.5415549278259277\n",
      "epoch 2 batch 583 loss: 2.5804200172424316\n",
      "epoch 2 batch 584 loss: 2.744804859161377\n",
      "epoch 2 batch 585 loss: 2.6990184783935547\n",
      "epoch 2 batch 586 loss: 2.5991997718811035\n",
      "epoch 2 batch 587 loss: 2.4432811737060547\n",
      "epoch 2 batch 588 loss: 2.3929316997528076\n",
      "epoch 2 batch 589 loss: 2.9123711585998535\n",
      "epoch 2 batch 590 loss: 2.4894468784332275\n",
      "epoch 2 batch 591 loss: 2.810551643371582\n",
      "epoch 2 batch 592 loss: 2.805126667022705\n",
      "epoch 2 batch 593 loss: 2.5592284202575684\n",
      "epoch 2 batch 594 loss: 2.8296265602111816\n",
      "epoch 2 batch 595 loss: 2.5331079959869385\n",
      "epoch 2 batch 596 loss: 2.8338823318481445\n",
      "epoch 2 batch 597 loss: 2.688164234161377\n",
      "epoch 2 batch 598 loss: 3.0188636779785156\n",
      "epoch 2 batch 599 loss: 2.452792167663574\n",
      "epoch 2 batch 600 loss: 2.768197536468506\n",
      "epoch 2 batch 601 loss: 2.3409042358398438\n",
      "epoch 2 batch 602 loss: 2.8476243019104004\n",
      "epoch 2 batch 603 loss: 2.336320400238037\n",
      "epoch 2 batch 604 loss: 2.659153938293457\n",
      "epoch 2 batch 605 loss: 2.52048921585083\n",
      "epoch 2 batch 606 loss: 2.411188840866089\n",
      "epoch 2 batch 607 loss: 2.768773317337036\n",
      "epoch 2 batch 608 loss: 2.5564017295837402\n",
      "epoch 2 batch 609 loss: 2.9692976474761963\n",
      "epoch 2 batch 610 loss: 2.6931958198547363\n",
      "epoch 2 batch 611 loss: 2.7384414672851562\n",
      "epoch 2 batch 612 loss: 2.5043065547943115\n",
      "epoch 2 batch 613 loss: 2.5436549186706543\n",
      "epoch 2 batch 614 loss: 2.6345624923706055\n",
      "epoch 2 batch 615 loss: 2.3677878379821777\n",
      "epoch 2 batch 616 loss: 2.577035903930664\n",
      "epoch 2 batch 617 loss: 3.034183979034424\n",
      "epoch 2 batch 618 loss: 2.6380796432495117\n",
      "epoch 2 batch 619 loss: 2.2177560329437256\n",
      "epoch 2 batch 620 loss: 2.3202474117279053\n",
      "epoch 2 batch 621 loss: 2.5242199897766113\n",
      "epoch 2 batch 622 loss: 2.7138028144836426\n",
      "epoch 2 batch 623 loss: 2.609614372253418\n",
      "epoch 2 batch 624 loss: 2.4797067642211914\n",
      "epoch 2 batch 625 loss: 2.6814918518066406\n",
      "epoch 2 batch 626 loss: 2.54482364654541\n",
      "epoch 2 batch 627 loss: 2.5359556674957275\n",
      "epoch 2 batch 628 loss: 2.4365038871765137\n",
      "epoch 2 batch 629 loss: 2.887014389038086\n",
      "epoch 2 batch 630 loss: 3.0948376655578613\n",
      "epoch 2 batch 631 loss: 2.940302848815918\n",
      "epoch 2 batch 632 loss: 2.2026026248931885\n",
      "epoch 2 batch 633 loss: 2.622178554534912\n",
      "epoch 2 batch 634 loss: 2.470306873321533\n",
      "epoch 2 batch 635 loss: 2.4575304985046387\n",
      "epoch 2 batch 636 loss: 2.4141526222229004\n",
      "epoch 2 batch 637 loss: 2.605884552001953\n",
      "epoch 2 batch 638 loss: 2.593899965286255\n",
      "epoch 2 batch 639 loss: 2.775733470916748\n",
      "epoch 2 batch 640 loss: 2.736855983734131\n",
      "epoch 2 batch 641 loss: 2.7001075744628906\n",
      "epoch 2 batch 642 loss: 2.4817659854888916\n",
      "epoch 2 batch 643 loss: 2.6196398735046387\n",
      "epoch 2 batch 644 loss: 2.5623371601104736\n",
      "epoch 2 batch 645 loss: 2.4380264282226562\n",
      "epoch 2 batch 646 loss: 2.618440628051758\n",
      "epoch 2 batch 647 loss: 2.4041311740875244\n",
      "epoch 2 batch 648 loss: 2.4936654567718506\n",
      "epoch 2 batch 649 loss: 2.6167521476745605\n",
      "epoch 2 batch 650 loss: 2.6996350288391113\n",
      "epoch 2 batch 651 loss: 2.677992820739746\n",
      "epoch 2 batch 652 loss: 2.4638962745666504\n",
      "epoch 2 batch 653 loss: 2.3716392517089844\n",
      "epoch 2 batch 654 loss: 2.34565806388855\n",
      "epoch 2 batch 655 loss: 2.4697823524475098\n",
      "epoch 2 batch 656 loss: 2.5413293838500977\n",
      "epoch 2 batch 657 loss: 2.822296619415283\n",
      "epoch 2 batch 658 loss: 2.5017659664154053\n",
      "epoch 2 batch 659 loss: 2.5038881301879883\n",
      "epoch 2 batch 660 loss: 2.393122911453247\n",
      "epoch 2 batch 661 loss: 2.167928695678711\n",
      "epoch 2 batch 662 loss: 2.6864776611328125\n",
      "epoch 2 batch 663 loss: 2.450831890106201\n",
      "epoch 2 batch 664 loss: 2.7861151695251465\n",
      "epoch 2 batch 665 loss: 2.2348008155822754\n",
      "epoch 2 batch 666 loss: 2.586895227432251\n",
      "epoch 2 batch 667 loss: 2.414055109024048\n",
      "epoch 2 batch 668 loss: 2.2300262451171875\n",
      "epoch 2 batch 669 loss: 2.5298519134521484\n",
      "epoch 2 batch 670 loss: 2.269435405731201\n",
      "epoch 2 batch 671 loss: 2.5006251335144043\n",
      "epoch 2 batch 672 loss: 2.6002988815307617\n",
      "epoch 2 batch 673 loss: 2.5868756771087646\n",
      "epoch 2 batch 674 loss: 2.5189647674560547\n",
      "epoch 2 batch 675 loss: 2.482421875\n",
      "epoch 2 batch 676 loss: 2.6137869358062744\n",
      "epoch 2 batch 677 loss: 2.347938060760498\n",
      "epoch 2 batch 678 loss: 2.936056613922119\n",
      "epoch 2 batch 679 loss: 2.42502498626709\n",
      "epoch 2 batch 680 loss: 3.042074203491211\n",
      "epoch 2 batch 681 loss: 2.432063102722168\n",
      "epoch 2 batch 682 loss: 2.5611138343811035\n",
      "epoch 2 batch 683 loss: 2.6965909004211426\n",
      "epoch 2 batch 684 loss: 2.3779687881469727\n",
      "epoch 2 batch 685 loss: 2.665371894836426\n",
      "epoch 2 batch 686 loss: 2.4737303256988525\n",
      "epoch 2 batch 687 loss: 2.4280178546905518\n",
      "epoch 2 batch 688 loss: 2.5524938106536865\n",
      "epoch 2 batch 689 loss: 2.855468273162842\n",
      "epoch 2 batch 690 loss: 2.5599164962768555\n",
      "epoch 2 batch 691 loss: 2.2720959186553955\n",
      "epoch 2 batch 692 loss: 2.5705981254577637\n",
      "epoch 2 batch 693 loss: 2.2407803535461426\n",
      "epoch 2 batch 694 loss: 2.527798652648926\n",
      "epoch 2 batch 695 loss: 2.657989978790283\n",
      "epoch 2 batch 696 loss: 2.3997974395751953\n",
      "epoch 2 batch 697 loss: 2.657149314880371\n",
      "epoch 2 batch 698 loss: 2.5742039680480957\n",
      "epoch 2 batch 699 loss: 2.5695483684539795\n",
      "epoch 2 batch 700 loss: 2.7552237510681152\n",
      "epoch 2 batch 701 loss: 3.083953857421875\n",
      "epoch 2 batch 702 loss: 2.52510142326355\n",
      "epoch 2 batch 703 loss: 2.4933207035064697\n",
      "epoch 2 batch 704 loss: 2.4209582805633545\n",
      "epoch 2 batch 705 loss: 2.601041078567505\n",
      "epoch 2 batch 706 loss: 2.5567171573638916\n",
      "epoch 2 batch 707 loss: 2.527656078338623\n",
      "epoch 2 batch 708 loss: 2.9020090103149414\n",
      "epoch 2 batch 709 loss: 2.3650074005126953\n",
      "epoch 2 batch 710 loss: 2.5735716819763184\n",
      "epoch 2 batch 711 loss: 2.7443771362304688\n",
      "epoch 2 batch 712 loss: 2.4296746253967285\n",
      "epoch 2 batch 713 loss: 2.581926107406616\n",
      "epoch 2 batch 714 loss: 2.610635995864868\n",
      "epoch 2 batch 715 loss: 2.5474965572357178\n",
      "epoch 2 batch 716 loss: 2.602916955947876\n",
      "epoch 2 batch 717 loss: 2.4377355575561523\n",
      "epoch 2 batch 718 loss: 2.49025821685791\n",
      "epoch 2 batch 719 loss: 2.510087013244629\n",
      "epoch 2 batch 720 loss: 2.750359058380127\n",
      "epoch 2 batch 721 loss: 2.608428955078125\n",
      "epoch 2 batch 722 loss: 2.373091697692871\n",
      "epoch 2 batch 723 loss: 2.542644500732422\n",
      "epoch 2 batch 724 loss: 2.523202419281006\n",
      "epoch 2 batch 725 loss: 2.3305368423461914\n",
      "epoch 2 batch 726 loss: 2.669551372528076\n",
      "epoch 2 batch 727 loss: 2.5654797554016113\n",
      "epoch 2 batch 728 loss: 2.1632628440856934\n",
      "epoch 2 batch 729 loss: 2.809225559234619\n",
      "epoch 2 batch 730 loss: 2.7408528327941895\n",
      "epoch 2 batch 731 loss: 2.249959945678711\n",
      "epoch 2 batch 732 loss: 2.5565223693847656\n",
      "epoch 2 batch 733 loss: 2.7162275314331055\n",
      "epoch 2 batch 734 loss: 2.4552059173583984\n",
      "epoch 2 batch 735 loss: 2.638240337371826\n",
      "epoch 2 batch 736 loss: 2.4414560794830322\n",
      "epoch 2 batch 737 loss: 2.6418142318725586\n",
      "epoch 2 batch 738 loss: 2.745213031768799\n",
      "epoch 2 batch 739 loss: 2.791412353515625\n",
      "epoch 2 batch 740 loss: 2.425421714782715\n",
      "epoch 2 batch 741 loss: 2.6397881507873535\n",
      "epoch 2 batch 742 loss: 2.316352367401123\n",
      "epoch 2 batch 743 loss: 2.6404082775115967\n",
      "epoch 2 batch 744 loss: 2.4766411781311035\n",
      "epoch 2 batch 745 loss: 2.5609374046325684\n",
      "epoch 2 batch 746 loss: 2.5057549476623535\n",
      "epoch 2 batch 747 loss: 2.581727981567383\n",
      "epoch 2 batch 748 loss: 2.7090840339660645\n",
      "epoch 2 batch 749 loss: 2.6207172870635986\n",
      "epoch 2 batch 750 loss: 2.5351083278656006\n",
      "epoch 2 batch 751 loss: 2.745760917663574\n",
      "epoch 2 batch 752 loss: 2.6194944381713867\n",
      "epoch 2 batch 753 loss: 2.491727113723755\n",
      "epoch 2 batch 754 loss: 2.8003697395324707\n",
      "epoch 2 batch 755 loss: 2.3780622482299805\n",
      "epoch 2 batch 756 loss: 2.497337818145752\n",
      "epoch 2 batch 757 loss: 2.503790855407715\n",
      "epoch 2 batch 758 loss: 2.22807240486145\n",
      "epoch 2 batch 759 loss: 2.3774361610412598\n",
      "epoch 2 batch 760 loss: 2.6445441246032715\n",
      "epoch 2 batch 761 loss: 2.3231749534606934\n",
      "epoch 2 batch 762 loss: 2.431586742401123\n",
      "epoch 2 batch 763 loss: 2.3948848247528076\n",
      "epoch 2 batch 764 loss: 2.4416136741638184\n",
      "epoch 2 batch 765 loss: 2.355484962463379\n",
      "epoch 2 batch 766 loss: 3.0058698654174805\n",
      "epoch 2 batch 767 loss: 2.4942121505737305\n",
      "epoch 2 batch 768 loss: 2.2367467880249023\n",
      "epoch 2 batch 769 loss: 2.640721321105957\n",
      "epoch 2 batch 770 loss: 2.6403579711914062\n",
      "epoch 2 batch 771 loss: 2.5282769203186035\n",
      "epoch 2 batch 772 loss: 2.556682586669922\n",
      "epoch 2 batch 773 loss: 2.5394132137298584\n",
      "epoch 2 batch 774 loss: 2.6558306217193604\n",
      "epoch 2 batch 775 loss: 2.459994077682495\n",
      "epoch 2 batch 776 loss: 2.7497193813323975\n",
      "epoch 2 batch 777 loss: 2.5419907569885254\n",
      "epoch 2 batch 778 loss: 2.411921501159668\n",
      "epoch 2 batch 779 loss: 2.826482057571411\n",
      "epoch 2 batch 780 loss: 2.6320669651031494\n",
      "epoch 2 batch 781 loss: 2.515608787536621\n",
      "epoch 2 batch 782 loss: 2.467118263244629\n",
      "epoch 2 batch 783 loss: 2.287388801574707\n",
      "epoch 2 batch 784 loss: 2.548776388168335\n",
      "epoch 2 batch 785 loss: 2.751079559326172\n",
      "epoch 2 batch 786 loss: 2.186295509338379\n",
      "epoch 2 batch 787 loss: 2.4576709270477295\n",
      "epoch 2 batch 788 loss: 2.4254653453826904\n",
      "epoch 2 batch 789 loss: 2.6730241775512695\n",
      "epoch 2 batch 790 loss: 2.370784282684326\n",
      "epoch 2 batch 791 loss: 2.5942156314849854\n",
      "epoch 2 batch 792 loss: 2.508838176727295\n",
      "epoch 2 batch 793 loss: 2.6910667419433594\n",
      "epoch 2 batch 794 loss: 2.210688829421997\n",
      "epoch 2 batch 795 loss: 2.444842576980591\n",
      "epoch 2 batch 796 loss: 2.487327814102173\n",
      "epoch 2 batch 797 loss: 2.713381767272949\n",
      "epoch 2 batch 798 loss: 2.0969152450561523\n",
      "epoch 2 batch 799 loss: 2.814302682876587\n",
      "epoch 2 batch 800 loss: 2.4565234184265137\n",
      "epoch 2 batch 801 loss: 2.510823965072632\n",
      "epoch 2 batch 802 loss: 2.561828136444092\n",
      "epoch 2 batch 803 loss: 2.507693290710449\n",
      "epoch 2 batch 804 loss: 2.643061637878418\n",
      "epoch 2 batch 805 loss: 2.8523826599121094\n",
      "epoch 2 batch 806 loss: 2.4593067169189453\n",
      "epoch 2 batch 807 loss: 2.5405280590057373\n",
      "epoch 2 batch 808 loss: 2.6936373710632324\n",
      "epoch 2 batch 809 loss: 2.4799914360046387\n",
      "epoch 2 batch 810 loss: 2.423820972442627\n",
      "epoch 2 batch 811 loss: 2.7083048820495605\n",
      "epoch 2 batch 812 loss: 2.690232992172241\n",
      "epoch 2 batch 813 loss: 2.527339458465576\n",
      "epoch 2 batch 814 loss: 2.6902151107788086\n",
      "epoch 2 batch 815 loss: 2.7583255767822266\n",
      "epoch 2 batch 816 loss: 2.6094987392425537\n",
      "epoch 2 batch 817 loss: 2.6478607654571533\n",
      "epoch 2 batch 818 loss: 2.5119991302490234\n",
      "epoch 2 batch 819 loss: 2.5166783332824707\n",
      "epoch 2 batch 820 loss: 2.3891611099243164\n",
      "epoch 2 batch 821 loss: 2.614175796508789\n",
      "epoch 2 batch 822 loss: 2.808581829071045\n",
      "epoch 2 batch 823 loss: 2.536480188369751\n",
      "epoch 2 batch 824 loss: 2.592075824737549\n",
      "epoch 2 batch 825 loss: 2.452198028564453\n",
      "epoch 2 batch 826 loss: 2.8192825317382812\n",
      "epoch 2 batch 827 loss: 2.7639834880828857\n",
      "epoch 2 batch 828 loss: 2.3739311695098877\n",
      "epoch 2 batch 829 loss: 2.3731131553649902\n",
      "epoch 2 batch 830 loss: 2.3308944702148438\n",
      "epoch 2 batch 831 loss: 2.2460663318634033\n",
      "epoch 2 batch 832 loss: 2.4877500534057617\n",
      "epoch 2 batch 833 loss: 2.4786641597747803\n",
      "epoch 2 batch 834 loss: 2.5560142993927\n",
      "epoch 2 batch 835 loss: 2.270688056945801\n",
      "epoch 2 batch 836 loss: 2.4403204917907715\n",
      "epoch 2 batch 837 loss: 2.380378246307373\n",
      "epoch 2 batch 838 loss: 2.7373173236846924\n",
      "epoch 2 batch 839 loss: 2.5136942863464355\n",
      "epoch 2 batch 840 loss: 2.49090313911438\n",
      "epoch 2 batch 841 loss: 2.725405693054199\n",
      "epoch 2 batch 842 loss: 2.9534666538238525\n",
      "epoch 2 batch 843 loss: 2.3832881450653076\n",
      "epoch 2 batch 844 loss: 2.422757387161255\n",
      "epoch 2 batch 845 loss: 2.623718738555908\n",
      "epoch 2 batch 846 loss: 2.556757926940918\n",
      "epoch 2 batch 847 loss: 2.745055675506592\n",
      "epoch 2 batch 848 loss: 2.562549114227295\n",
      "epoch 2 batch 849 loss: 2.767745018005371\n",
      "epoch 2 batch 850 loss: 2.3016390800476074\n",
      "epoch 2 batch 851 loss: 2.810892105102539\n",
      "epoch 2 batch 852 loss: 2.578587293624878\n",
      "epoch 2 batch 853 loss: 2.7575886249542236\n",
      "epoch 2 batch 854 loss: 2.740457057952881\n",
      "epoch 2 batch 855 loss: 2.7522687911987305\n",
      "epoch 2 batch 856 loss: 2.721456527709961\n",
      "epoch 2 batch 857 loss: 2.4838948249816895\n",
      "epoch 2 batch 858 loss: 2.6950392723083496\n",
      "epoch 2 batch 859 loss: 2.5127763748168945\n",
      "epoch 2 batch 860 loss: 2.640596628189087\n",
      "epoch 2 batch 861 loss: 2.4824163913726807\n",
      "epoch 2 batch 862 loss: 2.4776759147644043\n",
      "epoch 2 batch 863 loss: 2.3799257278442383\n",
      "epoch 2 batch 864 loss: 2.443635940551758\n",
      "epoch 2 batch 865 loss: 2.5476551055908203\n",
      "epoch 2 batch 866 loss: 2.5452818870544434\n",
      "epoch 2 batch 867 loss: 2.9110283851623535\n",
      "epoch 2 batch 868 loss: 2.613605499267578\n",
      "epoch 2 batch 869 loss: 2.5726380348205566\n",
      "epoch 2 batch 870 loss: 2.6374588012695312\n",
      "epoch 2 batch 871 loss: 2.5444798469543457\n",
      "epoch 2 batch 872 loss: 2.3352036476135254\n",
      "epoch 2 batch 873 loss: 2.779588222503662\n",
      "epoch 2 batch 874 loss: 3.1052184104919434\n",
      "epoch 2 batch 875 loss: 2.635652780532837\n",
      "epoch 2 batch 876 loss: 2.1501641273498535\n",
      "epoch 2 batch 877 loss: 2.3850505352020264\n",
      "epoch 2 batch 878 loss: 2.40213942527771\n",
      "epoch 2 batch 879 loss: 2.48128604888916\n",
      "epoch 2 batch 880 loss: 2.598510503768921\n",
      "epoch 2 batch 881 loss: 2.555813789367676\n",
      "epoch 2 batch 882 loss: 2.4468350410461426\n",
      "epoch 2 batch 883 loss: 2.497678279876709\n",
      "epoch 2 batch 884 loss: 2.45772123336792\n",
      "epoch 2 batch 885 loss: 2.619626998901367\n",
      "epoch 2 batch 886 loss: 2.687138557434082\n",
      "epoch 2 batch 887 loss: 2.540639638900757\n",
      "epoch 2 batch 888 loss: 2.2480454444885254\n",
      "epoch 2 batch 889 loss: 2.545563220977783\n",
      "epoch 2 batch 890 loss: 2.4087867736816406\n",
      "epoch 2 batch 891 loss: 2.369340419769287\n",
      "epoch 2 batch 892 loss: 2.574899196624756\n",
      "epoch 2 batch 893 loss: 2.6904239654541016\n",
      "epoch 2 batch 894 loss: 2.322874069213867\n",
      "epoch 2 batch 895 loss: 2.889768123626709\n",
      "epoch 2 batch 896 loss: 2.412463665008545\n",
      "epoch 2 batch 897 loss: 2.2461602687835693\n",
      "epoch 2 batch 898 loss: 2.554795742034912\n",
      "epoch 2 batch 899 loss: 2.7824645042419434\n",
      "epoch 2 batch 900 loss: 2.9808778762817383\n",
      "epoch 2 batch 901 loss: 2.4139838218688965\n",
      "epoch 2 batch 902 loss: 2.497598171234131\n",
      "epoch 2 batch 903 loss: 2.624863386154175\n",
      "epoch 2 batch 904 loss: 2.583052635192871\n",
      "epoch 2 batch 905 loss: 2.5551095008850098\n",
      "epoch 2 batch 906 loss: 2.562116861343384\n",
      "epoch 2 batch 907 loss: 2.2288413047790527\n",
      "epoch 2 batch 908 loss: 2.5210399627685547\n",
      "epoch 2 batch 909 loss: 2.7657384872436523\n",
      "epoch 2 batch 910 loss: 2.5355911254882812\n",
      "epoch 2 batch 911 loss: 2.526533603668213\n",
      "epoch 2 batch 912 loss: 2.5395381450653076\n",
      "epoch 2 batch 913 loss: 2.50192928314209\n",
      "epoch 2 batch 914 loss: 2.7549009323120117\n",
      "epoch 2 batch 915 loss: 2.5884194374084473\n",
      "epoch 2 batch 916 loss: 2.471646308898926\n",
      "epoch 2 batch 917 loss: 2.5184311866760254\n",
      "epoch 2 batch 918 loss: 2.392167806625366\n",
      "epoch 2 batch 919 loss: 2.222205638885498\n",
      "epoch 2 batch 920 loss: 2.4385766983032227\n",
      "epoch 2 batch 921 loss: 2.502257823944092\n",
      "epoch 2 batch 922 loss: 2.348952293395996\n",
      "epoch 2 batch 923 loss: 2.734591007232666\n",
      "epoch 2 batch 924 loss: 2.7344155311584473\n",
      "epoch 2 batch 925 loss: 2.447489023208618\n",
      "epoch 2 batch 926 loss: 2.408588409423828\n",
      "epoch 2 batch 927 loss: 2.4939894676208496\n",
      "epoch 2 batch 928 loss: 2.6402931213378906\n",
      "epoch 2 batch 929 loss: 2.591750144958496\n",
      "epoch 2 batch 930 loss: 2.5999722480773926\n",
      "epoch 2 batch 931 loss: 2.548495054244995\n",
      "epoch 2 batch 932 loss: 2.4646708965301514\n",
      "epoch 2 batch 933 loss: 2.463381290435791\n",
      "epoch 2 batch 934 loss: 2.3016767501831055\n",
      "epoch 2 batch 935 loss: 2.685194730758667\n",
      "epoch 2 batch 936 loss: 2.81276273727417\n",
      "epoch 2 batch 937 loss: 2.3842902183532715\n",
      "epoch 2 batch 938 loss: 2.7001352310180664\n",
      "epoch 2 batch 939 loss: 2.5195860862731934\n",
      "epoch 2 batch 940 loss: 2.5181493759155273\n",
      "epoch 2 batch 941 loss: 2.44692325592041\n",
      "epoch 2 batch 942 loss: 2.3654112815856934\n",
      "epoch 2 batch 943 loss: 2.5077078342437744\n",
      "epoch 2 batch 944 loss: 2.7571420669555664\n",
      "epoch 2 batch 945 loss: 2.5816423892974854\n",
      "epoch 2 batch 946 loss: 2.9908201694488525\n",
      "epoch 2 batch 947 loss: 2.6603281497955322\n",
      "epoch 2 batch 948 loss: 2.3769874572753906\n",
      "epoch 2 batch 949 loss: 2.8854777812957764\n",
      "epoch 2 batch 950 loss: 2.6154379844665527\n",
      "epoch 2 batch 951 loss: 2.6781039237976074\n",
      "epoch 2 batch 952 loss: 2.5940747261047363\n",
      "epoch 2 batch 953 loss: 2.62864351272583\n",
      "epoch 2 batch 954 loss: 2.5275702476501465\n",
      "epoch 2 batch 955 loss: 2.4744820594787598\n",
      "epoch 2 batch 956 loss: 2.3927230834960938\n",
      "epoch 2 batch 957 loss: 2.94736385345459\n",
      "epoch 2 batch 958 loss: 2.887394905090332\n",
      "epoch 2 batch 959 loss: 2.6322805881500244\n",
      "epoch 2 batch 960 loss: 2.338378667831421\n",
      "epoch 2 batch 961 loss: 2.5377469062805176\n",
      "epoch 2 batch 962 loss: 2.6958913803100586\n",
      "epoch 2 batch 963 loss: 2.3394293785095215\n",
      "epoch 2 batch 964 loss: 2.541734218597412\n",
      "epoch 2 batch 965 loss: 2.5665197372436523\n",
      "epoch 2 batch 966 loss: 2.5369038581848145\n",
      "epoch 2 batch 967 loss: 2.496384620666504\n",
      "epoch 2 batch 968 loss: 2.7546939849853516\n",
      "epoch 2 batch 969 loss: 2.781096935272217\n",
      "epoch 2 batch 970 loss: 2.6415839195251465\n",
      "epoch 2 batch 971 loss: 2.5014445781707764\n",
      "epoch 2 batch 972 loss: 2.7391281127929688\n",
      "epoch 2 batch 973 loss: 2.694964647293091\n",
      "epoch 2 batch 974 loss: 2.4900453090667725\n",
      "epoch 2 batch 975 loss: 2.432612419128418\n",
      "epoch 2 batch 976 loss: 2.5571770668029785\n",
      "epoch 2 batch 977 loss: 2.6349215507507324\n",
      "epoch 2 batch 978 loss: 2.3223280906677246\n",
      "epoch 2 batch 979 loss: 2.3108067512512207\n",
      "epoch 2 batch 980 loss: 2.6975808143615723\n",
      "epoch 2 batch 981 loss: 2.4911699295043945\n",
      "epoch 2 batch 982 loss: 2.867482900619507\n",
      "epoch 2 batch 983 loss: 2.5415685176849365\n",
      "epoch 2 batch 984 loss: 2.665571689605713\n",
      "epoch 2 batch 985 loss: 2.444650173187256\n",
      "epoch 2 batch 986 loss: 2.5264415740966797\n",
      "epoch 2 batch 987 loss: 2.6412863731384277\n",
      "epoch 2 batch 988 loss: 2.4445464611053467\n",
      "epoch 2 batch 989 loss: 2.679698944091797\n",
      "epoch 2 batch 990 loss: 2.479158878326416\n",
      "epoch 2 batch 991 loss: 2.5037384033203125\n",
      "epoch 2 batch 992 loss: 2.644101619720459\n",
      "epoch 2 batch 993 loss: 2.796370029449463\n",
      "epoch 2 batch 994 loss: 2.513150691986084\n",
      "epoch 2 batch 995 loss: 2.561124086380005\n",
      "epoch 2 batch 996 loss: 2.6444644927978516\n",
      "epoch 2 batch 997 loss: 2.527596950531006\n",
      "epoch 2 batch 998 loss: 2.4969568252563477\n",
      "epoch 2 batch 999 loss: 2.555227279663086\n",
      "epoch 2 batch 1000 loss: 2.8963704109191895\n",
      "epoch 2 batch 1001 loss: 2.469329833984375\n",
      "epoch 2 batch 1002 loss: 2.812250852584839\n",
      "epoch 2 batch 1003 loss: 2.2846622467041016\n",
      "epoch 2 batch 1004 loss: 2.5383095741271973\n",
      "epoch 2 batch 1005 loss: 2.4365134239196777\n",
      "epoch 2 batch 1006 loss: 2.2376999855041504\n",
      "epoch 2 batch 1007 loss: 2.4543118476867676\n",
      "epoch 2 batch 1008 loss: 2.8045334815979004\n",
      "epoch 2 batch 1009 loss: 2.5630719661712646\n",
      "epoch 2 batch 1010 loss: 2.375729560852051\n",
      "epoch 2 batch 1011 loss: 2.3600058555603027\n",
      "epoch 2 batch 1012 loss: 2.609060287475586\n",
      "epoch 2 batch 1013 loss: 2.5082831382751465\n",
      "epoch 2 batch 1014 loss: 2.325455665588379\n",
      "epoch 2 batch 1015 loss: 3.256166458129883\n",
      "epoch 2 batch 1016 loss: 2.5417349338531494\n",
      "epoch 2 batch 1017 loss: 2.5817770957946777\n",
      "epoch 2 batch 1018 loss: 2.4729862213134766\n",
      "epoch 2 batch 1019 loss: 2.774077892303467\n",
      "epoch 2 batch 1020 loss: 2.828807830810547\n",
      "epoch 2 batch 1021 loss: 2.726640462875366\n",
      "epoch 2 batch 1022 loss: 2.2895517349243164\n",
      "epoch 2 batch 1023 loss: 2.4279208183288574\n",
      "epoch 2 batch 1024 loss: 2.2160303592681885\n",
      "epoch 2 batch 1025 loss: 2.621971607208252\n",
      "epoch 2 batch 1026 loss: 2.453381061553955\n",
      "epoch 2 batch 1027 loss: 2.7370505332946777\n",
      "epoch 2 batch 1028 loss: 2.4506306648254395\n",
      "epoch 2 batch 1029 loss: 2.591069459915161\n",
      "epoch 2 batch 1030 loss: 2.374197244644165\n",
      "epoch 2 batch 1031 loss: 2.5421228408813477\n",
      "epoch 2 batch 1032 loss: 2.5822954177856445\n",
      "epoch 2 batch 1033 loss: 2.761739730834961\n",
      "epoch 2 batch 1034 loss: 2.766667127609253\n",
      "epoch 2 batch 1035 loss: 2.4456112384796143\n",
      "epoch 2 batch 1036 loss: 2.630565643310547\n",
      "epoch 2 batch 1037 loss: 2.3687915802001953\n",
      "epoch 2 batch 1038 loss: 2.6262192726135254\n",
      "epoch 2 batch 1039 loss: 2.3123087882995605\n",
      "epoch 2 batch 1040 loss: 2.6646339893341064\n",
      "epoch 2 batch 1041 loss: 2.6562047004699707\n",
      "epoch 2 batch 1042 loss: 2.7133216857910156\n",
      "epoch 2 batch 1043 loss: 2.914759397506714\n",
      "epoch 2 batch 1044 loss: 2.4670114517211914\n",
      "epoch 2 batch 1045 loss: 2.357016086578369\n",
      "epoch 2 batch 1046 loss: 2.5344903469085693\n",
      "epoch 2 batch 1047 loss: 2.386641502380371\n",
      "epoch 2 batch 1048 loss: 2.675107479095459\n",
      "epoch 2 batch 1049 loss: 2.4580326080322266\n",
      "epoch 2 batch 1050 loss: 2.5794243812561035\n",
      "epoch 2 batch 1051 loss: 2.492372989654541\n",
      "epoch 2 batch 1052 loss: 2.41886568069458\n",
      "epoch 2 batch 1053 loss: 2.4626662731170654\n",
      "epoch 2 batch 1054 loss: 2.312289237976074\n",
      "epoch 2 batch 1055 loss: 2.635362386703491\n",
      "epoch 2 batch 1056 loss: 2.3639705181121826\n",
      "epoch 2 batch 1057 loss: 2.2272653579711914\n",
      "epoch 2 batch 1058 loss: 2.6123547554016113\n",
      "epoch 2 batch 1059 loss: 2.3679025173187256\n",
      "epoch 2 batch 1060 loss: 2.6103808879852295\n",
      "epoch 2 batch 1061 loss: 2.2269577980041504\n",
      "epoch 2 batch 1062 loss: 2.595431089401245\n",
      "epoch 2 batch 1063 loss: 2.831519365310669\n",
      "epoch 2 batch 1064 loss: 2.65681791305542\n",
      "epoch 2 batch 1065 loss: 2.6725893020629883\n",
      "epoch 2 batch 1066 loss: 2.67885684967041\n",
      "epoch 2 batch 1067 loss: 2.355334758758545\n",
      "epoch 2 batch 1068 loss: 2.350144386291504\n",
      "epoch 2 batch 1069 loss: 2.3387057781219482\n",
      "epoch 2 batch 1070 loss: 2.2110376358032227\n",
      "epoch 2 batch 1071 loss: 2.501844644546509\n",
      "epoch 2 batch 1072 loss: 2.666583299636841\n",
      "epoch 2 batch 1073 loss: 2.5829434394836426\n",
      "epoch 2 batch 1074 loss: 2.5097482204437256\n",
      "epoch 2 batch 1075 loss: 2.6387171745300293\n",
      "epoch 2 batch 1076 loss: 2.343916177749634\n",
      "epoch 2 batch 1077 loss: 2.271651268005371\n",
      "epoch 2 batch 1078 loss: 2.725994110107422\n",
      "epoch 2 batch 1079 loss: 2.563413619995117\n",
      "epoch 2 batch 1080 loss: 2.3862853050231934\n",
      "epoch 2 batch 1081 loss: 2.8046364784240723\n",
      "epoch 2 batch 1082 loss: 2.7369472980499268\n",
      "epoch 2 batch 1083 loss: 2.512821674346924\n",
      "epoch 2 batch 1084 loss: 2.7316336631774902\n",
      "epoch 2 batch 1085 loss: 2.492800235748291\n",
      "epoch 2 batch 1086 loss: 2.742819309234619\n",
      "epoch 2 batch 1087 loss: 2.5751442909240723\n",
      "epoch 2 batch 1088 loss: 2.541865587234497\n",
      "epoch 2 batch 1089 loss: 2.6435937881469727\n",
      "epoch 2 batch 1090 loss: 2.640716314315796\n",
      "epoch 2 batch 1091 loss: 2.9655935764312744\n",
      "epoch 2 batch 1092 loss: 2.371835231781006\n",
      "epoch 2 batch 1093 loss: 2.4354472160339355\n",
      "epoch 2 batch 1094 loss: 2.6532537937164307\n",
      "epoch 2 batch 1095 loss: 2.4650471210479736\n",
      "epoch 2 batch 1096 loss: 2.6827664375305176\n",
      "epoch 2 batch 1097 loss: 2.6165785789489746\n",
      "epoch 2 batch 1098 loss: 2.7454519271850586\n",
      "epoch 2 batch 1099 loss: 2.67496919631958\n",
      "epoch 2 batch 1100 loss: 3.086642265319824\n",
      "epoch 2 batch 1101 loss: 2.44429349899292\n",
      "epoch 2 batch 1102 loss: 2.401249885559082\n",
      "epoch 2 batch 1103 loss: 2.8307723999023438\n",
      "epoch 2 batch 1104 loss: 2.779386520385742\n",
      "epoch 2 batch 1105 loss: 2.4178948402404785\n",
      "epoch 2 batch 1106 loss: 2.7216992378234863\n",
      "epoch 2 batch 1107 loss: 2.7257566452026367\n",
      "epoch 2 batch 1108 loss: 2.4901795387268066\n",
      "epoch 2 batch 1109 loss: 2.3647565841674805\n",
      "epoch 2 batch 1110 loss: 2.3833775520324707\n",
      "epoch 2 batch 1111 loss: 2.774017095565796\n",
      "epoch 2 batch 1112 loss: 2.743443489074707\n",
      "epoch 2 batch 1113 loss: 2.489187240600586\n",
      "epoch 2 batch 1114 loss: 2.5949292182922363\n",
      "epoch 2 batch 1115 loss: 2.2677338123321533\n",
      "epoch 2 batch 1116 loss: 2.396636486053467\n",
      "epoch 2 batch 1117 loss: 2.555861711502075\n",
      "epoch 2 batch 1118 loss: 2.479429244995117\n",
      "epoch 2 batch 1119 loss: 2.567741870880127\n",
      "epoch 2 batch 1120 loss: 2.5285229682922363\n",
      "epoch 2 batch 1121 loss: 2.535268545150757\n",
      "epoch 2 batch 1122 loss: 2.534440755844116\n",
      "epoch 2 batch 1123 loss: 2.6636877059936523\n",
      "epoch 2 batch 1124 loss: 2.5926456451416016\n",
      "epoch 2 batch 1125 loss: 2.7730088233947754\n",
      "epoch 2 batch 1126 loss: 2.292621612548828\n",
      "epoch 2 batch 1127 loss: 2.79142165184021\n",
      "epoch 2 batch 1128 loss: 2.7272448539733887\n",
      "epoch 2 batch 1129 loss: 2.5732784271240234\n",
      "epoch 2 batch 1130 loss: 2.7664413452148438\n",
      "epoch 2 batch 1131 loss: 2.7113499641418457\n",
      "epoch 2 batch 1132 loss: 2.622757911682129\n",
      "epoch 2 batch 1133 loss: 2.4117865562438965\n",
      "epoch 2 batch 1134 loss: 2.3815178871154785\n",
      "epoch 2 batch 1135 loss: 2.564420223236084\n",
      "epoch 2 batch 1136 loss: 2.393949508666992\n",
      "epoch 2 batch 1137 loss: 2.7460384368896484\n",
      "epoch 2 batch 1138 loss: 2.2615911960601807\n",
      "epoch 2 batch 1139 loss: 2.5898656845092773\n",
      "epoch 2 batch 1140 loss: 2.361697196960449\n",
      "epoch 2 batch 1141 loss: 2.5661120414733887\n",
      "epoch 2 batch 1142 loss: 2.5955312252044678\n",
      "epoch 2 batch 1143 loss: 2.5308852195739746\n",
      "epoch 2 batch 1144 loss: 2.307216167449951\n",
      "epoch 2 batch 1145 loss: 2.5450992584228516\n",
      "epoch 2 batch 1146 loss: 2.5642948150634766\n",
      "epoch 2 batch 1147 loss: 2.3870925903320312\n",
      "epoch 2 batch 1148 loss: 2.5128984451293945\n",
      "epoch 2 batch 1149 loss: 2.5440030097961426\n",
      "epoch 2 batch 1150 loss: 2.9455435276031494\n",
      "epoch 2 batch 1151 loss: 2.7264628410339355\n",
      "epoch 2 batch 1152 loss: 2.5794224739074707\n",
      "epoch 2 batch 1153 loss: 2.9691593647003174\n",
      "epoch 2 batch 1154 loss: 2.331716775894165\n",
      "epoch 2 batch 1155 loss: 2.43747615814209\n",
      "epoch 2 batch 1156 loss: 2.743570327758789\n",
      "epoch 2 batch 1157 loss: 2.3619091510772705\n",
      "epoch 2 batch 1158 loss: 2.8501217365264893\n",
      "epoch 2 batch 1159 loss: 2.317986011505127\n",
      "epoch 2 batch 1160 loss: 2.461841344833374\n",
      "epoch 2 batch 1161 loss: 2.506877899169922\n",
      "epoch 2 batch 1162 loss: 2.760990619659424\n",
      "epoch 2 batch 1163 loss: 2.7818257808685303\n",
      "epoch 2 batch 1164 loss: 2.6352667808532715\n",
      "epoch 2 batch 1165 loss: 2.514486789703369\n",
      "epoch 2 batch 1166 loss: 2.6962790489196777\n",
      "epoch 2 batch 1167 loss: 2.6645450592041016\n",
      "epoch 2 batch 1168 loss: 2.4570889472961426\n",
      "epoch 2 batch 1169 loss: 2.746107816696167\n",
      "epoch 2 batch 1170 loss: 2.4822521209716797\n",
      "epoch 2 batch 1171 loss: 2.615300178527832\n",
      "epoch 2 batch 1172 loss: 2.506075620651245\n",
      "epoch 2 batch 1173 loss: 2.507608652114868\n",
      "epoch 2 batch 1174 loss: 2.665249824523926\n",
      "epoch 2 batch 1175 loss: 2.174960136413574\n",
      "epoch 2 batch 1176 loss: 2.4300589561462402\n",
      "epoch 2 batch 1177 loss: 2.7358593940734863\n",
      "epoch 2 batch 1178 loss: 2.5026230812072754\n",
      "epoch 2 batch 1179 loss: 2.5680742263793945\n",
      "epoch 2 batch 1180 loss: 2.4417290687561035\n",
      "epoch 2 batch 1181 loss: 2.8119335174560547\n",
      "epoch 2 batch 1182 loss: 2.6690473556518555\n",
      "epoch 2 batch 1183 loss: 2.351393461227417\n",
      "epoch 2 batch 1184 loss: 2.639676094055176\n",
      "epoch 2 batch 1185 loss: 2.5496115684509277\n",
      "epoch 2 batch 1186 loss: 2.8385772705078125\n",
      "epoch 2 batch 1187 loss: 2.624295234680176\n",
      "epoch 2 batch 1188 loss: 2.5410542488098145\n",
      "epoch 2 batch 1189 loss: 2.6164889335632324\n",
      "epoch 2 batch 1190 loss: 2.34028697013855\n",
      "epoch 2 batch 1191 loss: 2.765918731689453\n",
      "epoch 2 batch 1192 loss: 2.8431637287139893\n",
      "epoch 2 batch 1193 loss: 2.5720129013061523\n",
      "epoch 2 batch 1194 loss: 2.6545443534851074\n",
      "epoch 2 batch 1195 loss: 2.353945732116699\n",
      "epoch 2 batch 1196 loss: 2.6886825561523438\n",
      "epoch 2 batch 1197 loss: 2.599579334259033\n",
      "epoch 2 batch 1198 loss: 2.4604640007019043\n",
      "epoch 2 batch 1199 loss: 2.3764774799346924\n",
      "epoch 2 batch 1200 loss: 2.572500228881836\n",
      "epoch 2 batch 1201 loss: 2.5806615352630615\n",
      "epoch 2 batch 1202 loss: 2.2530159950256348\n",
      "epoch 2 batch 1203 loss: 2.5403409004211426\n",
      "epoch 2 batch 1204 loss: 2.419508218765259\n",
      "epoch 2 batch 1205 loss: 2.5232772827148438\n",
      "epoch 2 batch 1206 loss: 2.4841721057891846\n",
      "epoch 2 batch 1207 loss: 2.692502737045288\n",
      "epoch 2 batch 1208 loss: 2.6861677169799805\n",
      "epoch 2 batch 1209 loss: 2.4812240600585938\n",
      "epoch 2 batch 1210 loss: 2.3457212448120117\n",
      "epoch 2 batch 1211 loss: 2.901998519897461\n",
      "epoch 2 batch 1212 loss: 2.597954511642456\n",
      "epoch 2 batch 1213 loss: 2.7569336891174316\n",
      "epoch 2 batch 1214 loss: 2.5388031005859375\n",
      "epoch 2 batch 1215 loss: 2.0839076042175293\n",
      "epoch 2 batch 1216 loss: 2.4625816345214844\n",
      "epoch 2 batch 1217 loss: 2.598235607147217\n",
      "epoch 2 batch 1218 loss: 2.346290111541748\n",
      "epoch 2 batch 1219 loss: 2.559927225112915\n",
      "epoch 2 batch 1220 loss: 2.42038631439209\n",
      "epoch 2 batch 1221 loss: 2.508087158203125\n",
      "epoch 2 batch 1222 loss: 2.775104284286499\n",
      "epoch 2 batch 1223 loss: 2.4428629875183105\n",
      "epoch 2 batch 1224 loss: 2.3282198905944824\n",
      "epoch 2 batch 1225 loss: 2.446547031402588\n",
      "epoch 2 batch 1226 loss: 2.409820079803467\n",
      "epoch 2 batch 1227 loss: 2.885197162628174\n",
      "epoch 2 batch 1228 loss: 2.711246967315674\n",
      "epoch 2 batch 1229 loss: 2.765923500061035\n",
      "epoch 2 batch 1230 loss: 2.874535083770752\n",
      "epoch 2 batch 1231 loss: 2.4256508350372314\n",
      "epoch 2 batch 1232 loss: 2.254979372024536\n",
      "epoch 2 batch 1233 loss: 2.3255515098571777\n",
      "epoch 2 batch 1234 loss: 2.5623388290405273\n",
      "epoch 2 batch 1235 loss: 2.105969190597534\n",
      "epoch 2 batch 1236 loss: 3.1178789138793945\n",
      "epoch 2 batch 1237 loss: 2.4370172023773193\n",
      "epoch 2 batch 1238 loss: 2.2770214080810547\n",
      "epoch 2 batch 1239 loss: 2.4459712505340576\n",
      "epoch 2 batch 1240 loss: 2.6590323448181152\n",
      "epoch 2 batch 1241 loss: 2.6968116760253906\n",
      "epoch 2 batch 1242 loss: 2.3890068531036377\n",
      "epoch 2 batch 1243 loss: 2.691324234008789\n",
      "epoch 2 batch 1244 loss: 2.5548932552337646\n",
      "epoch 2 batch 1245 loss: 2.4328041076660156\n",
      "epoch 2 batch 1246 loss: 3.0049123764038086\n",
      "epoch 2 batch 1247 loss: 2.695498466491699\n",
      "epoch 2 batch 1248 loss: 2.5440053939819336\n",
      "epoch 2 batch 1249 loss: 2.3612735271453857\n",
      "epoch 2 batch 1250 loss: 2.4502065181732178\n",
      "epoch 2 batch 1251 loss: 2.3128933906555176\n",
      "epoch 2 batch 1252 loss: 2.800408124923706\n",
      "epoch 2 batch 1253 loss: 2.326262950897217\n",
      "epoch 2 batch 1254 loss: 2.6446218490600586\n",
      "epoch 2 batch 1255 loss: 2.45106840133667\n",
      "epoch 2 batch 1256 loss: 2.688657283782959\n",
      "epoch 2 batch 1257 loss: 2.48106050491333\n",
      "epoch 2 batch 1258 loss: 2.141569137573242\n",
      "epoch 2 batch 1259 loss: 2.5613646507263184\n",
      "epoch 2 batch 1260 loss: 2.3397908210754395\n",
      "epoch 2 batch 1261 loss: 2.436588764190674\n",
      "epoch 2 batch 1262 loss: 2.3857421875\n",
      "epoch 2 batch 1263 loss: 2.7110157012939453\n",
      "epoch 2 batch 1264 loss: 2.6195662021636963\n",
      "epoch 2 batch 1265 loss: 2.557722330093384\n",
      "epoch 2 batch 1266 loss: 2.2929677963256836\n",
      "epoch 2 batch 1267 loss: 2.493821144104004\n",
      "epoch 2 batch 1268 loss: 2.6110682487487793\n",
      "epoch 2 batch 1269 loss: 2.5614171028137207\n",
      "epoch 2 batch 1270 loss: 2.3593568801879883\n",
      "epoch 2 batch 1271 loss: 2.4157299995422363\n",
      "epoch 2 batch 1272 loss: 2.3924179077148438\n",
      "epoch 2 batch 1273 loss: 2.4973080158233643\n",
      "epoch 2 batch 1274 loss: 2.5993685722351074\n",
      "epoch 2 batch 1275 loss: 2.4589014053344727\n",
      "epoch 2 batch 1276 loss: 2.2515196800231934\n",
      "epoch 2 batch 1277 loss: 2.184542655944824\n",
      "epoch 2 batch 1278 loss: 3.1521992683410645\n",
      "epoch 2 batch 1279 loss: 2.5383810997009277\n",
      "epoch 2 batch 1280 loss: 2.7586917877197266\n",
      "epoch 2 batch 1281 loss: 2.7482123374938965\n",
      "epoch 2 batch 1282 loss: 2.6393814086914062\n",
      "epoch 2 batch 1283 loss: 2.6027615070343018\n",
      "epoch 2 batch 1284 loss: 2.3769960403442383\n",
      "epoch 2 batch 1285 loss: 2.4594173431396484\n",
      "epoch 2 batch 1286 loss: 2.4321610927581787\n",
      "epoch 2 batch 1287 loss: 2.654143810272217\n",
      "epoch 2 batch 1288 loss: 2.556396961212158\n",
      "epoch 2 batch 1289 loss: 2.6708736419677734\n",
      "epoch 2 batch 1290 loss: 2.595465660095215\n",
      "epoch 2 batch 1291 loss: 2.3044118881225586\n",
      "epoch 2 batch 1292 loss: 2.5266664028167725\n",
      "epoch 2 batch 1293 loss: 2.3782849311828613\n",
      "epoch 2 batch 1294 loss: 2.394563674926758\n",
      "epoch 2 batch 1295 loss: 2.7243247032165527\n",
      "epoch 2 batch 1296 loss: 2.638352394104004\n",
      "epoch 2 batch 1297 loss: 2.348402976989746\n",
      "epoch 2 batch 1298 loss: 2.442901849746704\n",
      "epoch 2 batch 1299 loss: 2.509793758392334\n",
      "epoch 2 batch 1300 loss: 2.918607711791992\n",
      "epoch 2 batch 1301 loss: 2.2997026443481445\n",
      "epoch 2 batch 1302 loss: 2.3714542388916016\n",
      "epoch 2 batch 1303 loss: 2.7188775539398193\n",
      "epoch 2 batch 1304 loss: 2.593416690826416\n",
      "epoch 2 batch 1305 loss: 2.459804058074951\n",
      "epoch 2 batch 1306 loss: 2.358218193054199\n",
      "epoch 2 batch 1307 loss: 2.3870673179626465\n",
      "epoch 2 batch 1308 loss: 2.709010124206543\n",
      "epoch 2 batch 1309 loss: 2.7208118438720703\n",
      "epoch 2 batch 1310 loss: 2.3918380737304688\n",
      "epoch 2 batch 1311 loss: 2.789259910583496\n",
      "epoch 2 batch 1312 loss: 2.616764545440674\n",
      "epoch 2 batch 1313 loss: 2.5629096031188965\n",
      "epoch 2 batch 1314 loss: 2.463608741760254\n",
      "epoch 2 batch 1315 loss: 2.4660484790802\n",
      "epoch 2 batch 1316 loss: 2.4626049995422363\n",
      "epoch 2 batch 1317 loss: 2.662322998046875\n",
      "epoch 2 batch 1318 loss: 2.6553735733032227\n",
      "epoch 2 batch 1319 loss: 2.404351234436035\n",
      "epoch 2 batch 1320 loss: 2.5182337760925293\n",
      "epoch 2 batch 1321 loss: 2.630293369293213\n",
      "epoch 2 batch 1322 loss: 2.753666400909424\n",
      "epoch 2 batch 1323 loss: 2.3132824897766113\n",
      "epoch 2 batch 1324 loss: 2.4138031005859375\n",
      "epoch 2 batch 1325 loss: 2.387723445892334\n",
      "epoch 2 batch 1326 loss: 2.7461209297180176\n",
      "epoch 2 batch 1327 loss: 3.024998188018799\n",
      "epoch 2 batch 1328 loss: 2.6332919597625732\n",
      "epoch 2 batch 1329 loss: 2.5098490715026855\n",
      "epoch 2 batch 1330 loss: 2.810959815979004\n",
      "epoch 2 batch 1331 loss: 2.3367085456848145\n",
      "epoch 2 batch 1332 loss: 2.422297954559326\n",
      "epoch 2 batch 1333 loss: 2.2684760093688965\n",
      "epoch 2 batch 1334 loss: 2.6790268421173096\n",
      "epoch 2 batch 1335 loss: 2.5837669372558594\n",
      "epoch 2 batch 1336 loss: 2.429260730743408\n",
      "epoch 2 batch 1337 loss: 2.6644792556762695\n",
      "epoch 2 batch 1338 loss: 2.4134931564331055\n",
      "epoch 2 batch 1339 loss: 2.535156726837158\n",
      "epoch 2 batch 1340 loss: 2.6542720794677734\n",
      "epoch 2 batch 1341 loss: 2.2827138900756836\n",
      "epoch 2 batch 1342 loss: 2.5214529037475586\n",
      "epoch 2 batch 1343 loss: 2.500596523284912\n",
      "epoch 2 batch 1344 loss: 2.48543643951416\n",
      "epoch 2 batch 1345 loss: 2.6876087188720703\n",
      "epoch 2 batch 1346 loss: 2.626344680786133\n",
      "epoch 2 batch 1347 loss: 2.310800075531006\n",
      "epoch 2 batch 1348 loss: 2.4446651935577393\n",
      "epoch 2 batch 1349 loss: 2.585186243057251\n",
      "epoch 2 batch 1350 loss: 2.459167957305908\n",
      "epoch 2 batch 1351 loss: 2.4365944862365723\n",
      "epoch 2 batch 1352 loss: 2.8974361419677734\n",
      "epoch 2 batch 1353 loss: 2.5959699153900146\n",
      "epoch 2 batch 1354 loss: 2.43988037109375\n",
      "epoch 2 batch 1355 loss: 2.9209375381469727\n",
      "epoch 2 batch 1356 loss: 2.479836940765381\n",
      "epoch 2 batch 1357 loss: 2.6801843643188477\n",
      "epoch 2 batch 1358 loss: 2.7258241176605225\n",
      "epoch 2 batch 1359 loss: 2.566030979156494\n",
      "epoch 2 batch 1360 loss: 2.2999658584594727\n",
      "epoch 2 batch 1361 loss: 2.3992252349853516\n",
      "epoch 2 batch 1362 loss: 2.5067267417907715\n",
      "epoch 2 batch 1363 loss: 2.272124767303467\n",
      "epoch 2 batch 1364 loss: 2.4547481536865234\n",
      "epoch 2 batch 1365 loss: 2.6873092651367188\n",
      "epoch 2 batch 1366 loss: 2.743943214416504\n",
      "epoch 2 batch 1367 loss: 2.3240294456481934\n",
      "epoch 2 batch 1368 loss: 2.7727017402648926\n",
      "epoch 2 batch 1369 loss: 2.450331211090088\n",
      "epoch 2 batch 1370 loss: 2.5153491497039795\n",
      "epoch 2 batch 1371 loss: 2.876797914505005\n",
      "epoch 2 batch 1372 loss: 2.6519775390625\n",
      "epoch 2 batch 1373 loss: 2.278475761413574\n",
      "epoch 2 batch 1374 loss: 2.8044230937957764\n",
      "epoch 2 batch 1375 loss: 2.442251682281494\n",
      "epoch 2 batch 1376 loss: 2.453522205352783\n",
      "epoch 2 batch 1377 loss: 2.3919239044189453\n",
      "epoch 2 batch 1378 loss: 2.6987252235412598\n",
      "epoch 2 batch 1379 loss: 2.437255382537842\n",
      "epoch 2 batch 1380 loss: 2.4800243377685547\n",
      "epoch 2 batch 1381 loss: 2.618227481842041\n",
      "epoch 2 batch 1382 loss: 2.3104681968688965\n",
      "epoch 2 batch 1383 loss: 2.5238425731658936\n",
      "epoch 2 batch 1384 loss: 2.4429550170898438\n",
      "epoch 2 batch 1385 loss: 2.3770222663879395\n",
      "epoch 2 batch 1386 loss: 2.51638126373291\n",
      "epoch 2 batch 1387 loss: 2.66241455078125\n",
      "epoch 2 batch 1388 loss: 2.7802064418792725\n",
      "epoch 2 batch 1389 loss: 2.6499507427215576\n",
      "epoch 2 batch 1390 loss: 2.499315023422241\n",
      "epoch 2 batch 1391 loss: 2.143122911453247\n",
      "epoch 2 batch 1392 loss: 2.4450130462646484\n",
      "epoch 2 batch 1393 loss: 2.768831253051758\n",
      "epoch 2 batch 1394 loss: 2.426366090774536\n",
      "epoch 2 batch 1395 loss: 2.4944262504577637\n",
      "epoch 2 batch 1396 loss: 2.5749588012695312\n",
      "epoch 2 batch 1397 loss: 2.3871734142303467\n",
      "epoch 2 batch 1398 loss: 2.4141664505004883\n",
      "epoch 2 batch 1399 loss: 2.270050525665283\n",
      "epoch 2 batch 1400 loss: 2.6766531467437744\n",
      "epoch 2 batch 1401 loss: 2.2254409790039062\n",
      "epoch 2 batch 1402 loss: 2.2749876976013184\n",
      "epoch 2 batch 1403 loss: 2.2322885990142822\n",
      "epoch 2 batch 1404 loss: 2.3413703441619873\n",
      "epoch 2 batch 1405 loss: 2.4286460876464844\n",
      "epoch 2 batch 1406 loss: 2.5371453762054443\n",
      "epoch 2 batch 1407 loss: 2.7086021900177\n",
      "epoch 2 batch 1408 loss: 2.57706880569458\n",
      "epoch 2 batch 1409 loss: 2.5175530910491943\n",
      "epoch 2 batch 1410 loss: 2.524531841278076\n",
      "epoch 2 batch 1411 loss: 2.2620391845703125\n",
      "epoch 2 batch 1412 loss: 2.4176862239837646\n",
      "epoch 2 batch 1413 loss: 2.6865389347076416\n",
      "epoch 2 batch 1414 loss: 2.273930072784424\n",
      "epoch 2 batch 1415 loss: 2.437178373336792\n",
      "epoch 2 batch 1416 loss: 2.5906243324279785\n",
      "epoch 2 batch 1417 loss: 2.931126594543457\n",
      "epoch 2 batch 1418 loss: 2.5520505905151367\n",
      "epoch 2 batch 1419 loss: 2.279010772705078\n",
      "epoch 2 batch 1420 loss: 2.5322508811950684\n",
      "epoch 2 batch 1421 loss: 2.463909864425659\n",
      "epoch 2 batch 1422 loss: 2.4976131916046143\n",
      "epoch 2 batch 1423 loss: 2.4673991203308105\n",
      "epoch 2 batch 1424 loss: 2.353710889816284\n",
      "epoch 2 batch 1425 loss: 2.6143581867218018\n",
      "epoch 2 batch 1426 loss: 2.448756217956543\n",
      "epoch 2 batch 1427 loss: 2.246494770050049\n",
      "epoch 2 batch 1428 loss: 2.5663843154907227\n",
      "epoch 2 batch 1429 loss: 2.4494543075561523\n",
      "epoch 2 batch 1430 loss: 2.57871675491333\n",
      "epoch 2 batch 1431 loss: 2.8964178562164307\n",
      "epoch 2 batch 1432 loss: 2.3836846351623535\n",
      "epoch 2 batch 1433 loss: 2.277651071548462\n",
      "epoch 2 batch 1434 loss: 2.599499464035034\n",
      "epoch 2 batch 1435 loss: 2.638608455657959\n",
      "epoch 2 batch 1436 loss: 2.5718140602111816\n",
      "epoch 2 batch 1437 loss: 2.4927914142608643\n",
      "epoch 2 batch 1438 loss: 2.4102823734283447\n",
      "epoch 2 batch 1439 loss: 2.5825395584106445\n",
      "epoch 2 batch 1440 loss: 2.5270791053771973\n",
      "epoch 2 batch 1441 loss: 2.4573397636413574\n",
      "epoch 2 batch 1442 loss: 2.645003318786621\n",
      "epoch 2 batch 1443 loss: 2.552321672439575\n",
      "epoch 2 batch 1444 loss: 2.42056941986084\n",
      "epoch 2 batch 1445 loss: 2.3735361099243164\n",
      "epoch 2 batch 1446 loss: 2.4900126457214355\n",
      "epoch 2 batch 1447 loss: 2.265629291534424\n",
      "epoch 2 batch 1448 loss: 2.649445056915283\n",
      "epoch 2 batch 1449 loss: 2.8685503005981445\n",
      "epoch 2 batch 1450 loss: 2.5937657356262207\n",
      "epoch 2 batch 1451 loss: 2.3645756244659424\n",
      "epoch 2 batch 1452 loss: 2.639692783355713\n",
      "epoch 2 batch 1453 loss: 2.4460556507110596\n",
      "epoch 2 batch 1454 loss: 2.6087889671325684\n",
      "epoch 2 batch 1455 loss: 2.449660539627075\n",
      "epoch 2 batch 1456 loss: 2.797395944595337\n",
      "epoch 2 batch 1457 loss: 2.3721981048583984\n",
      "epoch 2 batch 1458 loss: 2.565091133117676\n",
      "epoch 2 batch 1459 loss: 2.752742290496826\n",
      "epoch 2 batch 1460 loss: 2.785951614379883\n",
      "epoch 2 batch 1461 loss: 2.3309004306793213\n",
      "epoch 2 batch 1462 loss: 2.6198830604553223\n",
      "epoch 2 batch 1463 loss: 2.4531664848327637\n",
      "epoch 2 batch 1464 loss: 2.3905651569366455\n",
      "epoch 2 batch 1465 loss: 2.3858802318573\n",
      "epoch 2 batch 1466 loss: 2.4174742698669434\n",
      "epoch 2 batch 1467 loss: 2.5971710681915283\n",
      "epoch 2 batch 1468 loss: 2.487765312194824\n",
      "epoch 2 batch 1469 loss: 2.649226188659668\n",
      "epoch 2 batch 1470 loss: 2.472672939300537\n",
      "epoch 2 batch 1471 loss: 2.3702244758605957\n",
      "epoch 2 batch 1472 loss: 2.3455822467803955\n",
      "epoch 2 batch 1473 loss: 2.6479415893554688\n",
      "epoch 2 batch 1474 loss: 2.276742458343506\n",
      "epoch 2 batch 1475 loss: 2.3369219303131104\n",
      "epoch 2 batch 1476 loss: 2.537590980529785\n",
      "epoch 2 batch 1477 loss: 2.420901298522949\n",
      "epoch 2 batch 1478 loss: 2.4998652935028076\n",
      "epoch 2 batch 1479 loss: 2.629398822784424\n",
      "epoch 2 batch 1480 loss: 2.3407270908355713\n",
      "epoch 2 batch 1481 loss: 2.429534435272217\n",
      "epoch 2 batch 1482 loss: 2.6904454231262207\n",
      "epoch 2 batch 1483 loss: 2.650390148162842\n",
      "epoch 2 batch 1484 loss: 2.7233097553253174\n",
      "epoch 2 batch 1485 loss: 2.446333408355713\n",
      "epoch 2 batch 1486 loss: 2.4550914764404297\n",
      "epoch 2 batch 1487 loss: 2.486524820327759\n",
      "epoch 2 batch 1488 loss: 2.342392683029175\n",
      "epoch 2 batch 1489 loss: 2.656689167022705\n",
      "epoch 2 batch 1490 loss: 2.514235734939575\n",
      "epoch 2 batch 1491 loss: 2.416405200958252\n",
      "epoch 2 batch 1492 loss: 2.4258291721343994\n",
      "epoch 2 batch 1493 loss: 2.725175142288208\n",
      "epoch 2 batch 1494 loss: 2.5547947883605957\n",
      "epoch 2 batch 1495 loss: 2.297666549682617\n",
      "epoch 2 batch 1496 loss: 2.727889060974121\n",
      "epoch 2 batch 1497 loss: 2.6007938385009766\n",
      "epoch 2 batch 1498 loss: 2.5632190704345703\n",
      "epoch 2 batch 1499 loss: 2.24655818939209\n",
      "epoch 2 batch 1500 loss: 2.505929470062256\n",
      "epoch 2 batch 1501 loss: 2.4627952575683594\n",
      "epoch 2 batch 1502 loss: 2.8596653938293457\n",
      "epoch 2 batch 1503 loss: 2.6491854190826416\n",
      "epoch 2 batch 1504 loss: 2.592515230178833\n",
      "epoch 2 batch 1505 loss: 2.3561336994171143\n",
      "epoch 2 batch 1506 loss: 2.6522748470306396\n",
      "epoch 2 batch 1507 loss: 2.6298816204071045\n",
      "epoch 2 batch 1508 loss: 2.4727349281311035\n",
      "epoch 2 batch 1509 loss: 2.8884530067443848\n",
      "epoch 2 batch 1510 loss: 2.353015422821045\n",
      "epoch 2 batch 1511 loss: 2.582986354827881\n",
      "epoch 2 batch 1512 loss: 2.766921043395996\n",
      "epoch 2 batch 1513 loss: 2.4527535438537598\n",
      "epoch 2 batch 1514 loss: 2.636747360229492\n",
      "epoch 2 batch 1515 loss: 2.452988386154175\n",
      "epoch 2 batch 1516 loss: 3.1087822914123535\n",
      "epoch 2 batch 1517 loss: 2.4961867332458496\n",
      "epoch 2 batch 1518 loss: 2.6339128017425537\n",
      "epoch 2 batch 1519 loss: 2.509176731109619\n",
      "epoch 2 batch 1520 loss: 2.6450607776641846\n",
      "epoch 2 batch 1521 loss: 2.8993735313415527\n",
      "epoch 2 batch 1522 loss: 2.5996341705322266\n",
      "epoch 2 batch 1523 loss: 2.8588132858276367\n",
      "epoch 2 batch 1524 loss: 2.871269941329956\n",
      "epoch 2 batch 1525 loss: 2.4023914337158203\n",
      "epoch 2 batch 1526 loss: 2.4333913326263428\n",
      "epoch 2 batch 1527 loss: 2.5725865364074707\n",
      "epoch 2 batch 1528 loss: 2.6939988136291504\n",
      "epoch 2 batch 1529 loss: 2.5608556270599365\n",
      "epoch 2 batch 1530 loss: 2.383653402328491\n",
      "epoch 2 batch 1531 loss: 2.3896431922912598\n",
      "epoch 2 batch 1532 loss: 2.4614224433898926\n",
      "epoch 2 batch 1533 loss: 2.548257350921631\n",
      "epoch 2 batch 1534 loss: 2.69602370262146\n",
      "epoch 2 batch 1535 loss: 2.6911184787750244\n",
      "epoch 2 batch 1536 loss: 2.2848904132843018\n",
      "epoch 2 batch 1537 loss: 2.9802634716033936\n",
      "epoch 2 batch 1538 loss: 2.4603943824768066\n",
      "epoch 2 batch 1539 loss: 2.5634734630584717\n",
      "epoch 2 batch 1540 loss: 2.815887451171875\n",
      "epoch 2 batch 1541 loss: 2.3022677898406982\n",
      "epoch 2 batch 1542 loss: 2.293067216873169\n",
      "epoch 2 batch 1543 loss: 2.7159111499786377\n",
      "epoch 2 batch 1544 loss: 2.511852741241455\n",
      "epoch 2 batch 1545 loss: 2.798060894012451\n",
      "epoch 2 batch 1546 loss: 2.4229187965393066\n",
      "epoch 2 batch 1547 loss: 2.663123846054077\n",
      "epoch 2 batch 1548 loss: 2.352407932281494\n",
      "epoch 2 batch 1549 loss: 2.637669563293457\n",
      "epoch 2 batch 1550 loss: 2.649094581604004\n",
      "epoch 2 batch 1551 loss: 2.1458795070648193\n",
      "epoch 2 batch 1552 loss: 2.5817649364471436\n",
      "epoch 2 batch 1553 loss: 2.5062313079833984\n",
      "epoch 2 batch 1554 loss: 2.365020751953125\n",
      "epoch 2 batch 1555 loss: 2.5700130462646484\n",
      "epoch 2 batch 1556 loss: 2.597658157348633\n",
      "epoch 2 batch 1557 loss: 2.4784374237060547\n",
      "epoch 2 batch 1558 loss: 2.424844264984131\n",
      "epoch 2 batch 1559 loss: 2.5826146602630615\n",
      "epoch 2 batch 1560 loss: 2.4800987243652344\n",
      "epoch 2 batch 1561 loss: 2.7641613483428955\n",
      "epoch 2 batch 1562 loss: 2.251753330230713\n",
      "epoch 2 batch 1563 loss: 2.6222681999206543\n",
      "epoch 2 batch 1564 loss: 2.385571002960205\n",
      "epoch 2 batch 1565 loss: 2.4694862365722656\n",
      "epoch 2 batch 1566 loss: 2.13326358795166\n",
      "epoch 2 batch 1567 loss: 2.632628917694092\n",
      "epoch 2 batch 1568 loss: 2.6558971405029297\n",
      "epoch 2 batch 1569 loss: 2.538970708847046\n",
      "epoch 2 batch 1570 loss: 2.4473161697387695\n",
      "epoch 2 batch 1571 loss: 2.492832660675049\n",
      "epoch 2 batch 1572 loss: 2.505934000015259\n",
      "epoch 2 batch 1573 loss: 2.2548296451568604\n",
      "epoch 2 batch 1574 loss: 2.4961800575256348\n",
      "epoch 2 batch 1575 loss: 2.1917920112609863\n",
      "epoch 2 batch 1576 loss: 2.3638885021209717\n",
      "epoch 2 batch 1577 loss: 2.4992501735687256\n",
      "epoch 2 batch 1578 loss: 2.7533938884735107\n",
      "epoch 2 batch 1579 loss: 2.5195136070251465\n",
      "epoch 2 batch 1580 loss: 2.6667566299438477\n",
      "epoch 2 batch 1581 loss: 2.2403411865234375\n",
      "epoch 2 batch 1582 loss: 2.3785762786865234\n",
      "epoch 2 batch 1583 loss: 2.2342724800109863\n",
      "epoch 2 batch 1584 loss: 2.566112756729126\n",
      "epoch 2 batch 1585 loss: 2.595489978790283\n",
      "epoch 2 batch 1586 loss: 2.3083715438842773\n",
      "epoch 2 batch 1587 loss: 2.6028308868408203\n",
      "epoch 2 batch 1588 loss: 2.4189133644104004\n",
      "epoch 2 batch 1589 loss: 2.56809139251709\n",
      "epoch 2 batch 1590 loss: 2.5534520149230957\n",
      "epoch 2 batch 1591 loss: 2.4056549072265625\n",
      "epoch 2 batch 1592 loss: 2.605437755584717\n",
      "epoch 2 batch 1593 loss: 2.5932223796844482\n",
      "epoch 2 batch 1594 loss: 2.661773204803467\n",
      "epoch 2 batch 1595 loss: 2.848011016845703\n",
      "epoch 2 batch 1596 loss: 2.3318076133728027\n",
      "epoch 2 batch 1597 loss: 2.4277641773223877\n",
      "epoch 2 batch 1598 loss: 2.468979835510254\n",
      "epoch 2 batch 1599 loss: 2.4729065895080566\n",
      "epoch 2 batch 1600 loss: 2.6052310466766357\n",
      "epoch 2 batch 1601 loss: 2.8284411430358887\n",
      "epoch 2 batch 1602 loss: 2.392059087753296\n",
      "epoch 2 batch 1603 loss: 2.8232173919677734\n",
      "epoch 2 batch 1604 loss: 2.739529609680176\n",
      "epoch 2 batch 1605 loss: 2.4246325492858887\n",
      "epoch 2 batch 1606 loss: 2.8611581325531006\n",
      "epoch 2 batch 1607 loss: 2.916571617126465\n",
      "epoch 2 batch 1608 loss: 2.477947473526001\n",
      "epoch 2 batch 1609 loss: 2.510249376296997\n",
      "epoch 2 batch 1610 loss: 2.3429207801818848\n",
      "epoch 2 batch 1611 loss: 2.649712562561035\n",
      "epoch 2 batch 1612 loss: 2.8628926277160645\n",
      "epoch 2 batch 1613 loss: 2.348600387573242\n",
      "epoch 2 batch 1614 loss: 2.7565555572509766\n",
      "epoch 2 batch 1615 loss: 2.6762123107910156\n",
      "epoch 2 batch 1616 loss: 2.5730347633361816\n",
      "epoch 2 batch 1617 loss: 2.7365026473999023\n",
      "epoch 2 batch 1618 loss: 2.537189245223999\n",
      "epoch 2 batch 1619 loss: 2.700425386428833\n",
      "epoch 2 batch 1620 loss: 2.534183979034424\n",
      "epoch 2 batch 1621 loss: 2.3706676959991455\n",
      "epoch 2 batch 1622 loss: 3.1457719802856445\n",
      "epoch 2 batch 1623 loss: 2.6451034545898438\n",
      "epoch 2 batch 1624 loss: 2.484651565551758\n",
      "epoch 2 batch 1625 loss: 2.257277488708496\n",
      "epoch 2 batch 1626 loss: 2.677767276763916\n",
      "epoch 2 batch 1627 loss: 2.2754480838775635\n",
      "epoch 2 batch 1628 loss: 2.466414451599121\n",
      "epoch 2 batch 1629 loss: 2.4994373321533203\n",
      "epoch 2 batch 1630 loss: 2.107245922088623\n",
      "epoch 2 batch 1631 loss: 2.7145133018493652\n",
      "epoch 2 batch 1632 loss: 2.154615640640259\n",
      "epoch 2 batch 1633 loss: 2.660893440246582\n",
      "epoch 2 batch 1634 loss: 2.5285696983337402\n",
      "epoch 2 batch 1635 loss: 2.707554817199707\n",
      "epoch 2 batch 1636 loss: 2.7523679733276367\n",
      "epoch 2 batch 1637 loss: 2.636098861694336\n",
      "epoch 2 batch 1638 loss: 2.4162468910217285\n",
      "epoch 2 batch 1639 loss: 2.5735881328582764\n",
      "epoch 2 batch 1640 loss: 2.469334125518799\n",
      "epoch 2 batch 1641 loss: 2.458454132080078\n",
      "epoch 2 batch 1642 loss: 2.343338966369629\n",
      "epoch 2 batch 1643 loss: 2.3284473419189453\n",
      "epoch 2 batch 1644 loss: 2.2855770587921143\n",
      "epoch 2 batch 1645 loss: 2.2334675788879395\n",
      "epoch 2 batch 1646 loss: 2.4918038845062256\n",
      "epoch 2 batch 1647 loss: 2.2696619033813477\n",
      "epoch 2 batch 1648 loss: 2.746682643890381\n",
      "epoch 2 batch 1649 loss: 2.3403568267822266\n",
      "epoch 2 batch 1650 loss: 2.4474949836730957\n",
      "epoch 2 batch 1651 loss: 2.659411907196045\n",
      "epoch 2 batch 1652 loss: 2.7355151176452637\n",
      "epoch 2 batch 1653 loss: 2.6341171264648438\n",
      "epoch 2 batch 1654 loss: 2.876878499984741\n",
      "epoch 2 batch 1655 loss: 2.324402093887329\n",
      "epoch 2 batch 1656 loss: 2.6976475715637207\n",
      "epoch 2 batch 1657 loss: 2.508216619491577\n",
      "epoch 2 batch 1658 loss: 2.245360851287842\n",
      "epoch 2 batch 1659 loss: 2.5978198051452637\n",
      "epoch 2 batch 1660 loss: 2.63073992729187\n",
      "epoch 2 batch 1661 loss: 2.4147582054138184\n",
      "epoch 2 batch 1662 loss: 2.4638383388519287\n",
      "epoch 2 batch 1663 loss: 2.482003688812256\n",
      "epoch 2 batch 1664 loss: 2.4824838638305664\n",
      "epoch 2 batch 1665 loss: 2.363309383392334\n",
      "epoch 2 batch 1666 loss: 2.557344675064087\n",
      "epoch 2 batch 1667 loss: 2.4154605865478516\n",
      "epoch 2 batch 1668 loss: 2.4774327278137207\n",
      "epoch 2 batch 1669 loss: 2.5307631492614746\n",
      "epoch 2 batch 1670 loss: 2.5199642181396484\n",
      "epoch 2 batch 1671 loss: 2.2957258224487305\n",
      "epoch 2 batch 1672 loss: 2.912325382232666\n",
      "epoch 2 batch 1673 loss: 2.8247108459472656\n",
      "epoch 2 batch 1674 loss: 2.5506396293640137\n",
      "epoch 2 batch 1675 loss: 2.4237685203552246\n",
      "epoch 2 batch 1676 loss: 2.3405468463897705\n",
      "epoch 2 batch 1677 loss: 2.7140188217163086\n",
      "epoch 2 batch 1678 loss: 2.476921796798706\n",
      "epoch 2 batch 1679 loss: 2.5158262252807617\n",
      "epoch 2 batch 1680 loss: 2.3987462520599365\n",
      "epoch 2 batch 1681 loss: 2.7113547325134277\n",
      "epoch 2 batch 1682 loss: 2.3030431270599365\n",
      "epoch 2 batch 1683 loss: 2.7243123054504395\n",
      "epoch 2 batch 1684 loss: 2.44431734085083\n",
      "epoch 2 batch 1685 loss: 2.552321434020996\n",
      "epoch 2 batch 1686 loss: 2.9626431465148926\n",
      "epoch 2 batch 1687 loss: 2.3725547790527344\n",
      "epoch 2 batch 1688 loss: 2.3949856758117676\n",
      "epoch 2 batch 1689 loss: 2.700880289077759\n",
      "epoch 2 batch 1690 loss: 2.223238468170166\n",
      "epoch 2 batch 1691 loss: 2.3852624893188477\n",
      "epoch 2 batch 1692 loss: 2.4942996501922607\n",
      "epoch 2 batch 1693 loss: 2.6908211708068848\n",
      "epoch 2 batch 1694 loss: 2.5988965034484863\n",
      "epoch 2 batch 1695 loss: 2.6321909427642822\n",
      "epoch 2 batch 1696 loss: 2.3685660362243652\n",
      "epoch 2 batch 1697 loss: 2.4197490215301514\n",
      "epoch 2 batch 1698 loss: 2.468010902404785\n",
      "epoch 2 batch 1699 loss: 2.8783493041992188\n",
      "epoch 2 batch 1700 loss: 2.5790772438049316\n",
      "epoch 2 batch 1701 loss: 2.6054606437683105\n",
      "epoch 2 batch 1702 loss: 3.0112693309783936\n",
      "epoch 2 batch 1703 loss: 2.3529016971588135\n",
      "epoch 2 batch 1704 loss: 2.6658663749694824\n",
      "epoch 2 batch 1705 loss: 2.376753330230713\n",
      "epoch 2 batch 1706 loss: 2.499469757080078\n",
      "epoch 2 batch 1707 loss: 2.305903911590576\n",
      "epoch 2 batch 1708 loss: 2.4693620204925537\n",
      "epoch 2 batch 1709 loss: 2.5749082565307617\n",
      "epoch 2 batch 1710 loss: 2.541734457015991\n",
      "epoch 2 batch 1711 loss: 2.5480539798736572\n",
      "epoch 2 batch 1712 loss: 2.9850549697875977\n",
      "epoch 2 batch 1713 loss: 2.390892744064331\n",
      "epoch 2 batch 1714 loss: 2.5501937866210938\n",
      "epoch 2 batch 1715 loss: 2.531466484069824\n",
      "epoch 2 batch 1716 loss: 2.400994300842285\n",
      "epoch 2 batch 1717 loss: 2.575511932373047\n",
      "epoch 2 batch 1718 loss: 2.4651951789855957\n",
      "epoch 2 batch 1719 loss: 2.706733226776123\n",
      "epoch 2 batch 1720 loss: 2.6698720455169678\n",
      "epoch 2 batch 1721 loss: 2.638216972351074\n",
      "epoch 2 batch 1722 loss: 2.5420608520507812\n",
      "epoch 2 batch 1723 loss: 2.6080846786499023\n",
      "epoch 2 batch 1724 loss: 2.6734118461608887\n",
      "epoch 2 batch 1725 loss: 2.5208208560943604\n",
      "epoch 2 batch 1726 loss: 2.591216802597046\n",
      "epoch 2 batch 1727 loss: 2.5452160835266113\n",
      "epoch 2 batch 1728 loss: 2.469026565551758\n",
      "epoch 2 batch 1729 loss: 2.277984142303467\n",
      "epoch 2 batch 1730 loss: 2.5187838077545166\n",
      "epoch 2 batch 1731 loss: 2.551976203918457\n",
      "epoch 2 batch 1732 loss: 2.339820623397827\n",
      "epoch 2 batch 1733 loss: 2.257492780685425\n",
      "epoch 2 batch 1734 loss: 2.737245559692383\n",
      "epoch 2 batch 1735 loss: 2.5791015625\n",
      "epoch 2 batch 1736 loss: 2.33662748336792\n",
      "epoch 2 batch 1737 loss: 2.213228225708008\n",
      "epoch 2 batch 1738 loss: 2.514043092727661\n",
      "epoch 2 batch 1739 loss: 2.3112802505493164\n",
      "epoch 2 batch 1740 loss: 2.1175384521484375\n",
      "epoch 2 batch 1741 loss: 2.5383944511413574\n",
      "epoch 2 batch 1742 loss: 2.3508472442626953\n",
      "epoch 2 batch 1743 loss: 2.565974712371826\n",
      "epoch 2 batch 1744 loss: 2.2369370460510254\n",
      "epoch 2 batch 1745 loss: 2.3894283771514893\n",
      "epoch 2 batch 1746 loss: 2.339614152908325\n",
      "epoch 2 batch 1747 loss: 2.357880115509033\n",
      "epoch 2 batch 1748 loss: 2.6593642234802246\n",
      "epoch 2 batch 1749 loss: 2.7564380168914795\n",
      "epoch 2 batch 1750 loss: 2.890467882156372\n",
      "epoch 2 batch 1751 loss: 2.456890106201172\n",
      "epoch 2 batch 1752 loss: 2.57462739944458\n",
      "epoch 2 batch 1753 loss: 2.6262035369873047\n",
      "epoch 2 batch 1754 loss: 2.3317816257476807\n",
      "epoch 2 batch 1755 loss: 2.5050745010375977\n",
      "epoch 2 batch 1756 loss: 2.582728624343872\n",
      "epoch 2 batch 1757 loss: 2.4955883026123047\n",
      "epoch 2 batch 1758 loss: 2.6832046508789062\n",
      "epoch 2 batch 1759 loss: 2.523487091064453\n",
      "epoch 2 batch 1760 loss: 2.432154893875122\n",
      "epoch 2 batch 1761 loss: 2.524320363998413\n",
      "epoch 2 batch 1762 loss: 2.754507303237915\n",
      "epoch 2 batch 1763 loss: 2.5175328254699707\n",
      "epoch 2 batch 1764 loss: 2.339815855026245\n",
      "epoch 2 batch 1765 loss: 2.6291043758392334\n",
      "epoch 2 batch 1766 loss: 2.4133400917053223\n",
      "epoch 2 batch 1767 loss: 2.366478204727173\n",
      "epoch 2 batch 1768 loss: 2.688448429107666\n",
      "epoch 2 batch 1769 loss: 2.4793734550476074\n",
      "epoch 2 batch 1770 loss: 2.681647300720215\n",
      "epoch 2 batch 1771 loss: 2.3633310794830322\n",
      "epoch 2 batch 1772 loss: 2.585996150970459\n",
      "epoch 2 batch 1773 loss: 2.3750877380371094\n",
      "epoch 2 batch 1774 loss: 2.520155906677246\n",
      "epoch 2 batch 1775 loss: 2.751063823699951\n",
      "epoch 2 batch 1776 loss: 2.4856719970703125\n",
      "epoch 2 batch 1777 loss: 2.3625736236572266\n",
      "epoch 2 batch 1778 loss: 2.4802165031433105\n",
      "epoch 2 batch 1779 loss: 2.781804323196411\n",
      "epoch 2 batch 1780 loss: 2.600306510925293\n",
      "epoch 2 batch 1781 loss: 2.1867666244506836\n",
      "epoch 2 batch 1782 loss: 2.547548770904541\n",
      "epoch 2 batch 1783 loss: 2.5756735801696777\n",
      "epoch 2 batch 1784 loss: 2.7523882389068604\n",
      "epoch 2 batch 1785 loss: 2.693364143371582\n",
      "epoch 2 batch 1786 loss: 2.6323766708374023\n",
      "epoch 2 batch 1787 loss: 2.454296350479126\n",
      "epoch 2 batch 1788 loss: 2.737989902496338\n",
      "epoch 2 batch 1789 loss: 2.4008378982543945\n",
      "epoch 2 batch 1790 loss: 2.472269058227539\n",
      "epoch 2 batch 1791 loss: 2.5802156925201416\n",
      "epoch 2 batch 1792 loss: 2.5700464248657227\n",
      "epoch 2 batch 1793 loss: 2.392134189605713\n",
      "epoch 2 batch 1794 loss: 2.2575931549072266\n",
      "epoch 2 batch 1795 loss: 3.0093538761138916\n",
      "epoch 2 batch 1796 loss: 2.884528636932373\n",
      "epoch 2 batch 1797 loss: 2.4704203605651855\n",
      "epoch 2 batch 1798 loss: 2.293912887573242\n",
      "epoch 2 batch 1799 loss: 2.453176975250244\n",
      "epoch 2 batch 1800 loss: 2.4268085956573486\n",
      "epoch 2 batch 1801 loss: 2.525278091430664\n",
      "epoch 2 batch 1802 loss: 2.3288726806640625\n",
      "epoch 2 batch 1803 loss: 2.3081564903259277\n",
      "epoch 2 batch 1804 loss: 2.3371825218200684\n",
      "epoch 2 batch 1805 loss: 2.3223624229431152\n",
      "epoch 2 batch 1806 loss: 2.2817811965942383\n",
      "epoch 2 batch 1807 loss: 2.447307825088501\n",
      "epoch 2 batch 1808 loss: 2.58931303024292\n",
      "epoch 2 batch 1809 loss: 2.280125141143799\n",
      "epoch 2 batch 1810 loss: 2.4961061477661133\n",
      "epoch 2 batch 1811 loss: 2.6190474033355713\n",
      "epoch 2 batch 1812 loss: 2.5039849281311035\n",
      "epoch 2 batch 1813 loss: 2.666555881500244\n",
      "epoch 2 batch 1814 loss: 2.7257494926452637\n",
      "epoch 2 batch 1815 loss: 2.4188039302825928\n",
      "epoch 2 batch 1816 loss: 3.014364719390869\n",
      "epoch 2 batch 1817 loss: 2.379739761352539\n",
      "epoch 2 batch 1818 loss: 2.7323217391967773\n",
      "epoch 2 batch 1819 loss: 2.6073856353759766\n",
      "epoch 2 batch 1820 loss: 2.571345329284668\n",
      "epoch 2 batch 1821 loss: 2.7737059593200684\n",
      "epoch 2 batch 1822 loss: 2.6367945671081543\n",
      "epoch 2 batch 1823 loss: 2.46232271194458\n",
      "epoch 2 batch 1824 loss: 2.4544663429260254\n",
      "epoch 2 batch 1825 loss: 2.229278564453125\n",
      "epoch 2 batch 1826 loss: 2.682864189147949\n",
      "epoch 2 batch 1827 loss: 2.273381233215332\n",
      "epoch 2 batch 1828 loss: 2.614157199859619\n",
      "epoch 2 batch 1829 loss: 2.5300848484039307\n",
      "epoch 2 batch 1830 loss: 2.5910098552703857\n",
      "epoch 2 batch 1831 loss: 2.3975563049316406\n",
      "epoch 2 batch 1832 loss: 2.534818649291992\n",
      "epoch 2 batch 1833 loss: 2.332289218902588\n",
      "epoch 2 batch 1834 loss: 2.4522902965545654\n",
      "epoch 2 batch 1835 loss: 2.4830501079559326\n",
      "epoch 2 batch 1836 loss: 2.348630428314209\n",
      "epoch 2 batch 1837 loss: 2.350430965423584\n",
      "epoch 2 batch 1838 loss: 2.4944581985473633\n",
      "epoch 2 batch 1839 loss: 2.631293773651123\n",
      "epoch 2 batch 1840 loss: 2.8029985427856445\n",
      "epoch 2 batch 1841 loss: 2.4588027000427246\n",
      "epoch 2 batch 1842 loss: 2.409514904022217\n",
      "epoch 2 batch 1843 loss: 2.3983731269836426\n",
      "epoch 2 batch 1844 loss: 2.500185489654541\n",
      "epoch 2 batch 1845 loss: 2.5536248683929443\n",
      "epoch 2 batch 1846 loss: 2.3515923023223877\n",
      "epoch 2 batch 1847 loss: 2.4324498176574707\n",
      "epoch 2 batch 1848 loss: 2.437143564224243\n",
      "epoch 2 batch 1849 loss: 2.5769360065460205\n",
      "epoch 2 batch 1850 loss: 2.151038646697998\n",
      "epoch 2 batch 1851 loss: 2.3149046897888184\n",
      "epoch 2 batch 1852 loss: 2.462940216064453\n",
      "epoch 2 batch 1853 loss: 2.496739387512207\n",
      "epoch 2 batch 1854 loss: 2.3455824851989746\n",
      "epoch 2 batch 1855 loss: 2.531536340713501\n",
      "epoch 2 batch 1856 loss: 2.7167129516601562\n",
      "epoch 2 batch 1857 loss: 2.725649118423462\n",
      "epoch 2 batch 1858 loss: 2.2718820571899414\n",
      "epoch 2 batch 1859 loss: 2.3431148529052734\n",
      "epoch 2 batch 1860 loss: 2.866626024246216\n",
      "epoch 2 batch 1861 loss: 2.23586368560791\n",
      "epoch 2 batch 1862 loss: 2.636600971221924\n",
      "epoch 2 batch 1863 loss: 2.4910521507263184\n",
      "epoch 2 batch 1864 loss: 2.769197702407837\n",
      "epoch 2 batch 1865 loss: 2.8134589195251465\n",
      "epoch 2 batch 1866 loss: 2.572054386138916\n",
      "epoch 2 batch 1867 loss: 2.7855896949768066\n",
      "epoch 2 batch 1868 loss: 2.4425134658813477\n",
      "epoch 2 batch 1869 loss: 2.4283640384674072\n",
      "epoch 2 batch 1870 loss: 2.553823232650757\n",
      "epoch 2 batch 1871 loss: 2.4935598373413086\n",
      "epoch 2 batch 1872 loss: 2.4808034896850586\n",
      "epoch 2 batch 1873 loss: 2.293933868408203\n",
      "epoch 2 batch 1874 loss: 2.2944588661193848\n",
      "epoch 2 batch 1875 loss: 2.193325996398926\n",
      "epoch 2 batch 1876 loss: 2.4648361206054688\n",
      "epoch 2 batch 1877 loss: 2.661149501800537\n",
      "epoch 2 batch 1878 loss: 2.8025546073913574\n",
      "epoch 2 batch 1879 loss: 2.64492130279541\n",
      "epoch 2 batch 1880 loss: 2.400426149368286\n",
      "epoch 2 batch 1881 loss: 2.5068776607513428\n",
      "epoch 2 batch 1882 loss: 2.670945644378662\n",
      "epoch 2 batch 1883 loss: 2.324157238006592\n",
      "epoch 2 batch 1884 loss: 2.4893875122070312\n",
      "epoch 2 batch 1885 loss: 2.656421184539795\n",
      "epoch 2 batch 1886 loss: 2.39396333694458\n",
      "epoch 2 batch 1887 loss: 2.7395386695861816\n",
      "epoch 2 batch 1888 loss: 2.5453248023986816\n",
      "epoch 2 batch 1889 loss: 2.6231789588928223\n",
      "epoch 2 batch 1890 loss: 2.561041831970215\n",
      "epoch 2 batch 1891 loss: 2.4402923583984375\n",
      "epoch 2 batch 1892 loss: 2.5451478958129883\n",
      "epoch 2 batch 1893 loss: 2.3782882690429688\n",
      "epoch 2 batch 1894 loss: 2.5414974689483643\n",
      "epoch 2 batch 1895 loss: 2.5458478927612305\n",
      "epoch 2 batch 1896 loss: 2.4733333587646484\n",
      "epoch 2 batch 1897 loss: 2.8806891441345215\n",
      "epoch 2 batch 1898 loss: 2.5345261096954346\n",
      "epoch 2 batch 1899 loss: 2.5997800827026367\n",
      "epoch 2 batch 1900 loss: 2.5450186729431152\n",
      "epoch 2 batch 1901 loss: 2.6048402786254883\n",
      "epoch 2 batch 1902 loss: 2.6444926261901855\n",
      "epoch 2 batch 1903 loss: 2.5861587524414062\n",
      "epoch 2 batch 1904 loss: 2.1234633922576904\n",
      "epoch 2 batch 1905 loss: 2.4227793216705322\n",
      "epoch 2 batch 1906 loss: 2.7111825942993164\n",
      "epoch 2 batch 1907 loss: 2.5895066261291504\n",
      "epoch 2 batch 1908 loss: 2.3307886123657227\n",
      "epoch 2 batch 1909 loss: 2.2410545349121094\n",
      "epoch 2 batch 1910 loss: 2.6045384407043457\n",
      "epoch 2 batch 1911 loss: 2.462733745574951\n",
      "epoch 2 batch 1912 loss: 2.486448287963867\n",
      "epoch 2 batch 1913 loss: 2.5016469955444336\n",
      "epoch 2 batch 1914 loss: 2.2398838996887207\n",
      "epoch 2 batch 1915 loss: 2.634441375732422\n",
      "epoch 2 batch 1916 loss: 2.7859017848968506\n",
      "epoch 2 batch 1917 loss: 2.491669178009033\n",
      "epoch 2 batch 1918 loss: 2.195957899093628\n",
      "epoch 2 batch 1919 loss: 2.4540369510650635\n",
      "epoch 2 batch 1920 loss: 2.5764405727386475\n",
      "epoch 2 batch 1921 loss: 2.7627878189086914\n",
      "epoch 2 batch 1922 loss: 2.6297833919525146\n",
      "epoch 2 batch 1923 loss: 2.6154587268829346\n",
      "epoch 2 batch 1924 loss: 2.461153745651245\n",
      "epoch 2 batch 1925 loss: 2.6244547367095947\n",
      "epoch 2 batch 1926 loss: 2.6612138748168945\n",
      "epoch 2 batch 1927 loss: 2.5908803939819336\n",
      "epoch 2 batch 1928 loss: 2.681852340698242\n",
      "epoch 2 batch 1929 loss: 2.209576368331909\n",
      "epoch 2 batch 1930 loss: 2.700136184692383\n",
      "epoch 2 batch 1931 loss: 2.542834520339966\n",
      "epoch 2 batch 1932 loss: 2.4306769371032715\n",
      "epoch 2 batch 1933 loss: 2.3668136596679688\n",
      "epoch 2 batch 1934 loss: 2.4180824756622314\n",
      "epoch 2 batch 1935 loss: 2.7390570640563965\n",
      "epoch 2 batch 1936 loss: 2.635504961013794\n",
      "epoch 2 batch 1937 loss: 2.571512222290039\n",
      "epoch 2 batch 1938 loss: 2.7276034355163574\n",
      "epoch 2 batch 1939 loss: 2.440168857574463\n",
      "epoch 2 batch 1940 loss: 3.0062971115112305\n",
      "epoch 2 batch 1941 loss: 2.4225845336914062\n",
      "epoch 2 batch 1942 loss: 2.816284656524658\n",
      "epoch 2 batch 1943 loss: 2.419947624206543\n",
      "epoch 2 batch 1944 loss: 2.1488702297210693\n",
      "epoch 2 batch 1945 loss: 2.707489490509033\n",
      "epoch 2 batch 1946 loss: 2.603961944580078\n",
      "epoch 2 batch 1947 loss: 2.520862102508545\n",
      "epoch 2 batch 1948 loss: 2.4086639881134033\n",
      "epoch 2 batch 1949 loss: 2.4441754817962646\n",
      "epoch 2 batch 1950 loss: 2.473179340362549\n",
      "epoch 2 batch 1951 loss: 2.8251795768737793\n",
      "epoch 2 batch 1952 loss: 2.4359970092773438\n",
      "epoch 2 batch 1953 loss: 2.4105446338653564\n",
      "epoch 2 batch 1954 loss: 2.697347640991211\n",
      "epoch 2 batch 1955 loss: 2.502032995223999\n",
      "epoch 2 batch 1956 loss: 2.3469793796539307\n",
      "epoch 2 batch 1957 loss: 2.4218034744262695\n",
      "epoch 2 batch 1958 loss: 2.4304752349853516\n",
      "epoch 2 batch 1959 loss: 2.4982118606567383\n",
      "epoch 2 batch 1960 loss: 2.655839443206787\n",
      "epoch 2 batch 1961 loss: 2.62324857711792\n",
      "epoch 2 batch 1962 loss: 2.886260986328125\n",
      "epoch 2 batch 1963 loss: 2.7099227905273438\n",
      "epoch 2 batch 1964 loss: 2.2364001274108887\n",
      "epoch 2 batch 1965 loss: 2.444004535675049\n",
      "epoch 2 batch 1966 loss: 2.444389820098877\n",
      "epoch 2 batch 1967 loss: 2.6152737140655518\n",
      "epoch 2 batch 1968 loss: 2.990841865539551\n",
      "epoch 2 batch 1969 loss: 2.6119256019592285\n",
      "epoch 2 batch 1970 loss: 2.3083648681640625\n",
      "epoch 2 batch 1971 loss: 2.4957692623138428\n",
      "epoch 2 batch 1972 loss: 2.348667621612549\n",
      "epoch 2 batch 1973 loss: 2.616048812866211\n",
      "epoch 2 batch 1974 loss: 2.2928380966186523\n",
      "epoch 2 batch 1975 loss: 2.891284465789795\n",
      "epoch 2 batch 1976 loss: 2.560542106628418\n",
      "epoch 2 batch 1977 loss: 2.808633804321289\n",
      "epoch 2 batch 1978 loss: 2.183656930923462\n",
      "epoch 2 batch 1979 loss: 2.406526565551758\n",
      "epoch 2 batch 1980 loss: 2.4411208629608154\n",
      "epoch 2 batch 1981 loss: 2.5619912147521973\n",
      "epoch 2 batch 1982 loss: 2.6796622276306152\n",
      "epoch 2 batch 1983 loss: 2.5847270488739014\n",
      "epoch 2 batch 1984 loss: 2.632117748260498\n",
      "epoch 2 batch 1985 loss: 2.3433890342712402\n",
      "epoch 2 batch 1986 loss: 2.577099561691284\n",
      "epoch 2 batch 1987 loss: 2.5706682205200195\n",
      "epoch 2 batch 1988 loss: 2.6574907302856445\n",
      "epoch 2 batch 1989 loss: 2.2119390964508057\n",
      "epoch 2 batch 1990 loss: 2.605940580368042\n",
      "epoch 2 batch 1991 loss: 2.982086658477783\n",
      "epoch 2 batch 1992 loss: 2.411804676055908\n",
      "epoch 2 batch 1993 loss: 2.1551640033721924\n",
      "epoch 2 batch 1994 loss: 2.355841875076294\n",
      "epoch 2 batch 1995 loss: 2.8691632747650146\n",
      "epoch 2 batch 1996 loss: 2.4635581970214844\n",
      "epoch 2 batch 1997 loss: 2.4272799491882324\n",
      "epoch 2 batch 1998 loss: 2.3694605827331543\n",
      "epoch 2 batch 1999 loss: 2.3330206871032715\n",
      "epoch 2 batch 2000 loss: 2.6442155838012695\n",
      "epoch 2 batch 2001 loss: 2.323856830596924\n",
      "epoch 2 batch 2002 loss: 2.331149101257324\n",
      "epoch 2 batch 2003 loss: 2.294167995452881\n",
      "epoch 2 batch 2004 loss: 2.831371784210205\n",
      "epoch 2 batch 2005 loss: 2.503047466278076\n",
      "epoch 2 batch 2006 loss: 2.58689546585083\n",
      "epoch 2 batch 2007 loss: 2.517267942428589\n",
      "epoch 2 batch 2008 loss: 2.3168785572052\n",
      "epoch 2 batch 2009 loss: 2.646289110183716\n",
      "epoch 2 batch 2010 loss: 2.78882098197937\n",
      "epoch 2 batch 2011 loss: 2.5592126846313477\n",
      "epoch 2 batch 2012 loss: 2.366396188735962\n",
      "epoch 2 batch 2013 loss: 2.55432391166687\n",
      "epoch 2 batch 2014 loss: 2.566319704055786\n",
      "epoch 2 batch 2015 loss: 2.4642891883850098\n",
      "epoch 2 batch 2016 loss: 2.272864580154419\n",
      "epoch 2 batch 2017 loss: 2.4630343914031982\n",
      "epoch 2 batch 2018 loss: 2.3798255920410156\n",
      "epoch 2 batch 2019 loss: 2.3002700805664062\n",
      "epoch 2 batch 2020 loss: 2.4819722175598145\n",
      "epoch 2 batch 2021 loss: 2.5693488121032715\n",
      "epoch 2 batch 2022 loss: 2.8208162784576416\n",
      "epoch 2 batch 2023 loss: 2.3964052200317383\n",
      "epoch 2 batch 2024 loss: 2.7211709022521973\n",
      "epoch 2 batch 2025 loss: 2.5757720470428467\n",
      "epoch 2 batch 2026 loss: 2.3864641189575195\n",
      "epoch 2 batch 2027 loss: 2.686021566390991\n",
      "epoch 2 batch 2028 loss: 2.2089900970458984\n",
      "epoch 2 batch 2029 loss: 2.507683277130127\n",
      "epoch 2 batch 2030 loss: 2.3557865619659424\n",
      "epoch 2 batch 2031 loss: 2.5470223426818848\n",
      "epoch 2 batch 2032 loss: 2.4198648929595947\n",
      "epoch 2 batch 2033 loss: 2.399517297744751\n",
      "epoch 2 batch 2034 loss: 2.4519495964050293\n",
      "epoch 2 batch 2035 loss: 2.4695024490356445\n",
      "epoch 2 batch 2036 loss: 2.5323867797851562\n",
      "epoch 2 batch 2037 loss: 2.712857961654663\n",
      "epoch 2 batch 2038 loss: 2.6941580772399902\n",
      "epoch 2 batch 2039 loss: 2.2513492107391357\n",
      "epoch 2 batch 2040 loss: 2.5343520641326904\n",
      "epoch 2 batch 2041 loss: 2.39874529838562\n",
      "epoch 2 batch 2042 loss: 2.1550257205963135\n",
      "epoch 2 batch 2043 loss: 2.498664379119873\n",
      "epoch 2 batch 2044 loss: 2.410645008087158\n",
      "epoch 2 batch 2045 loss: 2.608283758163452\n",
      "epoch 2 batch 2046 loss: 2.755126476287842\n",
      "epoch 2 batch 2047 loss: 2.202353000640869\n",
      "epoch 2 batch 2048 loss: 2.2506797313690186\n",
      "epoch 2 batch 2049 loss: 2.301443099975586\n",
      "epoch 2 batch 2050 loss: 2.386748790740967\n",
      "epoch 2 batch 2051 loss: 2.416149139404297\n",
      "epoch 2 batch 2052 loss: 2.733428955078125\n",
      "epoch 2 batch 2053 loss: 2.6387739181518555\n",
      "epoch 2 batch 2054 loss: 2.3847103118896484\n",
      "epoch 2 batch 2055 loss: 2.5767605304718018\n",
      "epoch 2 batch 2056 loss: 2.474780321121216\n",
      "epoch 2 batch 2057 loss: 2.4032111167907715\n",
      "epoch 2 batch 2058 loss: 2.763725996017456\n",
      "epoch 2 batch 2059 loss: 2.5134317874908447\n",
      "epoch 2 batch 2060 loss: 2.2295889854431152\n",
      "epoch 2 batch 2061 loss: 2.5181989669799805\n",
      "epoch 2 batch 2062 loss: 2.6680736541748047\n",
      "epoch 2 batch 2063 loss: 2.41928768157959\n",
      "epoch 2 batch 2064 loss: 2.765866279602051\n",
      "epoch 2 batch 2065 loss: 2.7890028953552246\n",
      "epoch 2 batch 2066 loss: 2.4370553493499756\n",
      "epoch 2 batch 2067 loss: 2.583090305328369\n",
      "epoch 2 batch 2068 loss: 2.445885181427002\n",
      "epoch 2 batch 2069 loss: 2.3709869384765625\n",
      "epoch 2 batch 2070 loss: 2.3022210597991943\n",
      "epoch 2 batch 2071 loss: 2.4369006156921387\n",
      "epoch 2 batch 2072 loss: 2.412055730819702\n",
      "epoch 2 batch 2073 loss: 2.50235652923584\n",
      "epoch 2 batch 2074 loss: 2.4415132999420166\n",
      "epoch 2 batch 2075 loss: 2.255673408508301\n",
      "epoch 2 batch 2076 loss: 2.4641354084014893\n",
      "epoch 2 batch 2077 loss: 2.687197208404541\n",
      "epoch 2 batch 2078 loss: 2.5794026851654053\n",
      "epoch 2 batch 2079 loss: 2.50643253326416\n",
      "epoch 2 batch 2080 loss: 2.204967975616455\n",
      "epoch 2 batch 2081 loss: 2.3731627464294434\n",
      "epoch 2 batch 2082 loss: 2.280662775039673\n",
      "epoch 2 batch 2083 loss: 2.8033320903778076\n",
      "epoch 2 batch 2084 loss: 2.71012806892395\n",
      "epoch 2 batch 2085 loss: 2.774674415588379\n",
      "epoch 2 batch 2086 loss: 2.260824203491211\n",
      "epoch 2 batch 2087 loss: 2.513579845428467\n",
      "epoch 2 batch 2088 loss: 2.5056142807006836\n",
      "epoch 2 batch 2089 loss: 2.533944606781006\n",
      "epoch 2 batch 2090 loss: 2.55954647064209\n",
      "epoch 2 batch 2091 loss: 2.4672257900238037\n",
      "epoch 2 batch 2092 loss: 2.6995537281036377\n",
      "epoch 2 batch 2093 loss: 2.3454935550689697\n",
      "epoch 2 batch 2094 loss: 2.5800065994262695\n",
      "epoch 2 batch 2095 loss: 2.656190872192383\n",
      "epoch 2 batch 2096 loss: 2.437401294708252\n",
      "epoch 2 batch 2097 loss: 2.4506475925445557\n",
      "epoch 2 batch 2098 loss: 2.3877334594726562\n",
      "epoch 2 batch 2099 loss: 2.636387825012207\n",
      "epoch 2 batch 2100 loss: 2.5097949504852295\n",
      "epoch 2 batch 2101 loss: 2.4862375259399414\n",
      "epoch 2 batch 2102 loss: 2.8644678592681885\n",
      "epoch 2 batch 2103 loss: 2.418754816055298\n",
      "epoch 2 batch 2104 loss: 2.6198244094848633\n",
      "epoch 2 batch 2105 loss: 2.3521180152893066\n",
      "epoch 2 batch 2106 loss: 2.9350426197052\n",
      "epoch 2 batch 2107 loss: 2.8382205963134766\n",
      "epoch 2 batch 2108 loss: 2.352558135986328\n",
      "epoch 2 batch 2109 loss: 2.6800737380981445\n",
      "epoch 2 batch 2110 loss: 2.3942248821258545\n",
      "epoch 2 batch 2111 loss: 2.427828788757324\n",
      "epoch 2 batch 2112 loss: 2.3241310119628906\n",
      "epoch 2 batch 2113 loss: 2.505417823791504\n",
      "epoch 2 batch 2114 loss: 2.396355390548706\n",
      "epoch 2 batch 2115 loss: 2.5827646255493164\n",
      "epoch 2 batch 2116 loss: 2.47418475151062\n",
      "epoch 2 batch 2117 loss: 2.477424383163452\n",
      "epoch 2 batch 2118 loss: 2.4378609657287598\n",
      "epoch 2 batch 2119 loss: 2.629880905151367\n",
      "epoch 2 batch 2120 loss: 2.2578165531158447\n",
      "epoch 2 batch 2121 loss: 2.582273483276367\n",
      "epoch 2 batch 2122 loss: 2.588715076446533\n",
      "epoch 2 batch 2123 loss: 2.411552667617798\n",
      "epoch 2 batch 2124 loss: 2.6764779090881348\n",
      "epoch 2 batch 2125 loss: 2.252387046813965\n",
      "epoch 2 batch 2126 loss: 2.5458436012268066\n",
      "epoch 2 batch 2127 loss: 2.9126017093658447\n",
      "epoch 2 batch 2128 loss: 2.5450351238250732\n",
      "epoch 2 batch 2129 loss: 2.3925914764404297\n",
      "epoch 2 batch 2130 loss: 2.291836738586426\n",
      "epoch 2 batch 2131 loss: 2.7640819549560547\n",
      "epoch 2 batch 2132 loss: 2.565789222717285\n",
      "epoch 2 batch 2133 loss: 2.5023770332336426\n",
      "epoch 2 batch 2134 loss: 2.525045871734619\n",
      "epoch 2 batch 2135 loss: 2.2688000202178955\n",
      "epoch 2 batch 2136 loss: 2.5623788833618164\n",
      "epoch 2 batch 2137 loss: 2.4672839641571045\n",
      "epoch 2 batch 2138 loss: 2.4148874282836914\n",
      "epoch 2 batch 2139 loss: 2.1682686805725098\n",
      "epoch 2 batch 2140 loss: 2.3813061714172363\n",
      "epoch 2 batch 2141 loss: 2.4014835357666016\n",
      "epoch 2 batch 2142 loss: 2.604678153991699\n",
      "epoch 2 batch 2143 loss: 2.530935764312744\n",
      "epoch 2 batch 2144 loss: 2.472036838531494\n",
      "epoch 2 batch 2145 loss: 2.5352137088775635\n",
      "epoch 2 batch 2146 loss: 2.645057201385498\n",
      "epoch 2 batch 2147 loss: 2.185563564300537\n",
      "epoch 2 batch 2148 loss: 2.38638973236084\n",
      "epoch 2 batch 2149 loss: 2.4471068382263184\n",
      "epoch 2 batch 2150 loss: 2.5012223720550537\n",
      "epoch 2 batch 2151 loss: 2.4625186920166016\n",
      "epoch 2 batch 2152 loss: 2.3171005249023438\n",
      "epoch 2 batch 2153 loss: 2.3470191955566406\n",
      "epoch 2 batch 2154 loss: 2.4858434200286865\n",
      "epoch 2 batch 2155 loss: 2.6335368156433105\n",
      "epoch 2 batch 2156 loss: 2.7016282081604004\n",
      "epoch 2 batch 2157 loss: 2.4668350219726562\n",
      "epoch 2 batch 2158 loss: 2.751232385635376\n",
      "epoch 2 batch 2159 loss: 2.3720343112945557\n",
      "epoch 2 batch 2160 loss: 2.5204668045043945\n",
      "epoch 2 batch 2161 loss: 2.7349464893341064\n",
      "epoch 2 batch 2162 loss: 2.4957809448242188\n",
      "epoch 2 batch 2163 loss: 2.5776467323303223\n",
      "epoch 2 batch 2164 loss: 2.7536673545837402\n",
      "epoch 2 batch 2165 loss: 2.368706703186035\n",
      "epoch 2 batch 2166 loss: 2.642204999923706\n",
      "epoch 2 batch 2167 loss: 2.3911349773406982\n",
      "epoch 2 batch 2168 loss: 2.2697043418884277\n",
      "epoch 2 batch 2169 loss: 2.568589210510254\n",
      "epoch 2 batch 2170 loss: 2.6041266918182373\n",
      "epoch 2 batch 2171 loss: 2.861685276031494\n",
      "epoch 2 batch 2172 loss: 2.6373751163482666\n",
      "epoch 2 batch 2173 loss: 2.261220932006836\n",
      "epoch 2 batch 2174 loss: 2.4898033142089844\n",
      "epoch 2 batch 2175 loss: 2.6335229873657227\n",
      "epoch 2 batch 2176 loss: 2.5333023071289062\n",
      "epoch 2 batch 2177 loss: 2.7617905139923096\n",
      "epoch 2 batch 2178 loss: 2.4295778274536133\n",
      "epoch 2 batch 2179 loss: 2.9838085174560547\n",
      "epoch 2 batch 2180 loss: 2.312870502471924\n",
      "epoch 2 batch 2181 loss: 2.299919843673706\n",
      "epoch 2 batch 2182 loss: 2.627826690673828\n",
      "epoch 2 batch 2183 loss: 2.4895243644714355\n",
      "epoch 2 batch 2184 loss: 2.257967233657837\n",
      "epoch 2 batch 2185 loss: 2.186779022216797\n",
      "epoch 2 batch 2186 loss: 2.305772304534912\n",
      "epoch 2 batch 2187 loss: 2.642857074737549\n",
      "epoch 2 batch 2188 loss: 2.5662765502929688\n",
      "epoch 2 batch 2189 loss: 3.029855728149414\n",
      "epoch 2 batch 2190 loss: 2.7310872077941895\n",
      "epoch 2 batch 2191 loss: 2.6820950508117676\n",
      "epoch 2 batch 2192 loss: 2.1840643882751465\n",
      "epoch 2 batch 2193 loss: 2.7037582397460938\n",
      "epoch 2 batch 2194 loss: 2.1874301433563232\n",
      "epoch 2 batch 2195 loss: 2.2627573013305664\n",
      "epoch 2 batch 2196 loss: 2.6189517974853516\n",
      "epoch 2 batch 2197 loss: 2.447136878967285\n",
      "epoch 2 batch 2198 loss: 2.2679007053375244\n",
      "epoch 2 batch 2199 loss: 2.433365821838379\n",
      "epoch 2 batch 2200 loss: 2.5222935676574707\n",
      "epoch 2 batch 2201 loss: 2.575960874557495\n",
      "epoch 2 batch 2202 loss: 2.3068511486053467\n",
      "epoch 2 batch 2203 loss: 2.7160286903381348\n",
      "epoch 2 batch 2204 loss: 2.485384464263916\n",
      "epoch 2 batch 2205 loss: 2.5917582511901855\n",
      "epoch 2 batch 2206 loss: 2.6736574172973633\n",
      "epoch 2 batch 2207 loss: 2.3631136417388916\n",
      "epoch 2 batch 2208 loss: 2.470431089401245\n",
      "epoch 2 batch 2209 loss: 2.593491554260254\n",
      "epoch 2 batch 2210 loss: 2.492161750793457\n",
      "epoch 2 batch 2211 loss: 2.346731662750244\n",
      "epoch 2 batch 2212 loss: 2.590043783187866\n",
      "epoch 2 batch 2213 loss: 2.8910973072052\n",
      "epoch 2 batch 2214 loss: 2.810194969177246\n",
      "epoch 2 batch 2215 loss: 2.873516082763672\n",
      "epoch 2 batch 2216 loss: 2.4714651107788086\n",
      "epoch 2 batch 2217 loss: 2.659520149230957\n",
      "epoch 2 batch 2218 loss: 2.5168089866638184\n",
      "epoch 2 batch 2219 loss: 2.9558513164520264\n",
      "epoch 2 batch 2220 loss: 2.491731643676758\n",
      "epoch 2 batch 2221 loss: 2.6401023864746094\n",
      "epoch 2 batch 2222 loss: 2.4798760414123535\n",
      "epoch 2 batch 2223 loss: 2.2930901050567627\n",
      "epoch 2 batch 2224 loss: 2.588198184967041\n",
      "epoch 2 batch 2225 loss: 2.4570093154907227\n",
      "epoch 2 batch 2226 loss: 2.4216127395629883\n",
      "epoch 2 batch 2227 loss: 2.2323875427246094\n",
      "epoch 2 batch 2228 loss: 2.5330312252044678\n",
      "epoch 2 batch 2229 loss: 2.5331034660339355\n",
      "epoch 2 batch 2230 loss: 2.6479811668395996\n",
      "epoch 2 batch 2231 loss: 2.472304344177246\n",
      "epoch 2 batch 2232 loss: 2.4029531478881836\n",
      "epoch 2 batch 2233 loss: 2.4072940349578857\n",
      "epoch 2 batch 2234 loss: 2.554957866668701\n",
      "epoch 2 batch 2235 loss: 2.6787197589874268\n",
      "epoch 2 batch 2236 loss: 2.13570499420166\n",
      "epoch 2 batch 2237 loss: 2.5927135944366455\n",
      "epoch 2 batch 2238 loss: 2.3182597160339355\n",
      "epoch 2 batch 2239 loss: 2.382796049118042\n",
      "epoch 2 batch 2240 loss: 2.5616283416748047\n",
      "epoch 2 batch 2241 loss: 2.43239688873291\n",
      "epoch 2 batch 2242 loss: 2.4557158946990967\n",
      "epoch 2 batch 2243 loss: 2.3370814323425293\n",
      "epoch 2 batch 2244 loss: 2.834852695465088\n",
      "epoch 2 batch 2245 loss: 2.499377727508545\n",
      "epoch 2 batch 2246 loss: 2.40531587600708\n",
      "epoch 2 batch 2247 loss: 2.5409021377563477\n",
      "epoch 2 batch 2248 loss: 3.0745954513549805\n",
      "epoch 2 batch 2249 loss: 2.733065128326416\n",
      "epoch 2 batch 2250 loss: 2.5192394256591797\n",
      "epoch 2 batch 2251 loss: 2.446516513824463\n",
      "epoch 2 batch 2252 loss: 2.505854368209839\n",
      "epoch 2 batch 2253 loss: 2.517009735107422\n",
      "epoch 2 batch 2254 loss: 2.214799404144287\n",
      "epoch 2 batch 2255 loss: 2.221548080444336\n",
      "epoch 2 batch 2256 loss: 2.5984950065612793\n",
      "epoch 2 batch 2257 loss: 2.463569164276123\n",
      "epoch 2 batch 2258 loss: 2.3204822540283203\n",
      "epoch 2 batch 2259 loss: 2.429231882095337\n",
      "epoch 2 batch 2260 loss: 2.7419681549072266\n",
      "epoch 2 batch 2261 loss: 2.784846782684326\n",
      "epoch 2 batch 2262 loss: 2.2809410095214844\n",
      "epoch 2 batch 2263 loss: 2.2219128608703613\n",
      "epoch 2 batch 2264 loss: 2.3820903301239014\n",
      "epoch 2 batch 2265 loss: 2.2215933799743652\n",
      "epoch 2 batch 2266 loss: 2.3387765884399414\n",
      "epoch 2 batch 2267 loss: 2.1926727294921875\n",
      "epoch 2 batch 2268 loss: 2.4576575756073\n",
      "epoch 2 batch 2269 loss: 2.332974910736084\n",
      "epoch 2 batch 2270 loss: 2.4134490489959717\n",
      "epoch 2 batch 2271 loss: 2.241657257080078\n",
      "epoch 2 batch 2272 loss: 2.4338669776916504\n",
      "epoch 2 batch 2273 loss: 2.790677785873413\n",
      "epoch 2 batch 2274 loss: 2.575104236602783\n",
      "epoch 2 batch 2275 loss: 2.36441707611084\n",
      "epoch 2 batch 2276 loss: 2.9083681106567383\n",
      "epoch 2 batch 2277 loss: 2.72733211517334\n",
      "epoch 2 batch 2278 loss: 2.3957457542419434\n",
      "epoch 2 batch 2279 loss: 2.356062889099121\n",
      "epoch 2 batch 2280 loss: 2.4403982162475586\n",
      "epoch 2 batch 2281 loss: 2.2255263328552246\n",
      "epoch 2 batch 2282 loss: 2.29964017868042\n",
      "epoch 2 batch 2283 loss: 2.5320472717285156\n",
      "epoch 2 batch 2284 loss: 2.3246548175811768\n",
      "epoch 2 batch 2285 loss: 2.2251296043395996\n",
      "epoch 2 batch 2286 loss: 2.4094066619873047\n",
      "epoch 2 batch 2287 loss: 2.2966837882995605\n",
      "epoch 2 batch 2288 loss: 2.633265733718872\n",
      "epoch 2 batch 2289 loss: 2.4635424613952637\n",
      "epoch 2 batch 2290 loss: 2.376519203186035\n",
      "epoch 2 batch 2291 loss: 2.638559341430664\n",
      "epoch 2 batch 2292 loss: 2.613603353500366\n",
      "epoch 2 batch 2293 loss: 2.4137325286865234\n",
      "epoch 2 batch 2294 loss: 2.3262438774108887\n",
      "epoch 2 batch 2295 loss: 2.3123111724853516\n",
      "epoch 2 batch 2296 loss: 2.197216033935547\n",
      "epoch 2 batch 2297 loss: 2.39457368850708\n",
      "epoch 2 batch 2298 loss: 2.5048632621765137\n",
      "epoch 2 batch 2299 loss: 2.3876280784606934\n",
      "epoch 2 batch 2300 loss: 2.545783281326294\n",
      "epoch 2 batch 2301 loss: 2.7011451721191406\n",
      "epoch 2 batch 2302 loss: 2.471830129623413\n",
      "epoch 2 batch 2303 loss: 2.430251121520996\n",
      "epoch 2 batch 2304 loss: 2.6651411056518555\n",
      "epoch 2 batch 2305 loss: 2.417086601257324\n",
      "epoch 2 batch 2306 loss: 2.3884084224700928\n",
      "epoch 2 batch 2307 loss: 2.468197822570801\n",
      "epoch 2 batch 2308 loss: 2.6845552921295166\n",
      "epoch 2 batch 2309 loss: 2.4776556491851807\n",
      "epoch 2 batch 2310 loss: 2.4631614685058594\n",
      "epoch 2 batch 2311 loss: 2.1751346588134766\n",
      "epoch 2 batch 2312 loss: 2.3858494758605957\n",
      "epoch 2 batch 2313 loss: 2.58284854888916\n",
      "epoch 2 batch 2314 loss: 2.5780720710754395\n",
      "epoch 2 batch 2315 loss: 2.3629679679870605\n",
      "epoch 2 batch 2316 loss: 2.5363621711730957\n",
      "epoch 2 batch 2317 loss: 2.496685028076172\n",
      "epoch 2 batch 2318 loss: 2.460275888442993\n",
      "epoch 2 batch 2319 loss: 2.7870075702667236\n",
      "epoch 2 batch 2320 loss: 2.4865622520446777\n",
      "epoch 2 batch 2321 loss: 2.4850807189941406\n",
      "epoch 2 batch 2322 loss: 2.5152807235717773\n",
      "epoch 2 batch 2323 loss: 2.3484413623809814\n",
      "epoch 2 batch 2324 loss: 2.5645205974578857\n",
      "epoch 2 batch 2325 loss: 2.1101903915405273\n",
      "epoch 2 batch 2326 loss: 2.4143548011779785\n",
      "epoch 2 batch 2327 loss: 2.4879133701324463\n",
      "epoch 2 batch 2328 loss: 2.219326972961426\n",
      "epoch 2 batch 2329 loss: 2.3299713134765625\n",
      "epoch 2 batch 2330 loss: 2.459935188293457\n",
      "epoch 2 batch 2331 loss: 2.267645835876465\n",
      "epoch 2 batch 2332 loss: 2.2306008338928223\n",
      "epoch 2 batch 2333 loss: 2.35078763961792\n",
      "epoch 2 batch 2334 loss: 2.514862060546875\n",
      "epoch 2 batch 2335 loss: 2.3240389823913574\n",
      "epoch 2 batch 2336 loss: 2.451632499694824\n",
      "epoch 2 batch 2337 loss: 2.7481188774108887\n",
      "epoch 2 batch 2338 loss: 2.4586338996887207\n",
      "epoch 2 batch 2339 loss: 2.3050684928894043\n",
      "epoch 2 batch 2340 loss: 2.2364585399627686\n",
      "epoch 2 batch 2341 loss: 2.376913547515869\n",
      "epoch 2 batch 2342 loss: 2.486720085144043\n",
      "epoch 2 batch 2343 loss: 2.46846079826355\n",
      "epoch 2 batch 2344 loss: 2.4198265075683594\n",
      "epoch 2 batch 2345 loss: 2.811389923095703\n",
      "epoch 2 batch 2346 loss: 2.452768325805664\n",
      "epoch 2 batch 2347 loss: 2.358344793319702\n",
      "epoch 2 batch 2348 loss: 2.334860324859619\n",
      "epoch 2 batch 2349 loss: 2.1975693702697754\n",
      "epoch 2 batch 2350 loss: 2.2676901817321777\n",
      "epoch 2 batch 2351 loss: 2.8023862838745117\n",
      "epoch 2 batch 2352 loss: 2.258253812789917\n",
      "epoch 2 batch 2353 loss: 2.2785398960113525\n",
      "epoch 2 batch 2354 loss: 2.307673454284668\n",
      "epoch 2 batch 2355 loss: 2.2842400074005127\n",
      "epoch 2 batch 2356 loss: 2.5119056701660156\n",
      "epoch 2 batch 2357 loss: 2.397581100463867\n",
      "epoch 2 batch 2358 loss: 2.579592227935791\n",
      "epoch 2 batch 2359 loss: 2.4962453842163086\n",
      "epoch 2 batch 2360 loss: 2.591040849685669\n",
      "epoch 2 batch 2361 loss: 2.4248836040496826\n",
      "epoch 2 batch 2362 loss: 2.0681872367858887\n",
      "epoch 2 batch 2363 loss: 2.5647926330566406\n",
      "epoch 2 batch 2364 loss: 2.331876516342163\n",
      "epoch 2 batch 2365 loss: 2.9899349212646484\n",
      "epoch 2 batch 2366 loss: 2.5535683631896973\n",
      "epoch 2 batch 2367 loss: 2.4419045448303223\n",
      "epoch 2 batch 2368 loss: 2.4210917949676514\n",
      "epoch 2 batch 2369 loss: 2.396786689758301\n",
      "epoch 2 batch 2370 loss: 2.3005495071411133\n",
      "epoch 2 batch 2371 loss: 2.710427761077881\n",
      "epoch 2 batch 2372 loss: 2.4714505672454834\n",
      "epoch 2 batch 2373 loss: 2.3193652629852295\n",
      "epoch 2 batch 2374 loss: 2.707151412963867\n",
      "epoch 2 batch 2375 loss: 2.3544716835021973\n",
      "epoch 2 batch 2376 loss: 2.5129661560058594\n",
      "epoch 2 batch 2377 loss: 2.3001461029052734\n",
      "epoch 2 batch 2378 loss: 2.7560031414031982\n",
      "epoch 2 batch 2379 loss: 2.2351603507995605\n",
      "epoch 2 batch 2380 loss: 2.691983222961426\n",
      "epoch 2 batch 2381 loss: 2.391551971435547\n",
      "epoch 2 batch 2382 loss: 2.5974478721618652\n",
      "epoch 2 batch 2383 loss: 2.6554508209228516\n",
      "epoch 2 batch 2384 loss: 2.236039876937866\n",
      "epoch 2 batch 2385 loss: 2.148116111755371\n",
      "epoch 2 batch 2386 loss: 2.3595781326293945\n",
      "epoch 2 batch 2387 loss: 2.360952854156494\n",
      "epoch 2 batch 2388 loss: 3.103386402130127\n",
      "epoch 2 batch 2389 loss: 2.350625514984131\n",
      "epoch 2 batch 2390 loss: 2.4925193786621094\n",
      "epoch 2 batch 2391 loss: 2.6391992568969727\n",
      "epoch 2 batch 2392 loss: 2.796088695526123\n",
      "epoch 2 batch 2393 loss: 2.403730869293213\n",
      "epoch 2 batch 2394 loss: 2.2224981784820557\n",
      "epoch 2 batch 2395 loss: 2.529327392578125\n",
      "epoch 2 batch 2396 loss: 2.7474701404571533\n",
      "epoch 2 batch 2397 loss: 2.4056272506713867\n",
      "epoch 2 batch 2398 loss: 2.5742239952087402\n",
      "epoch 2 batch 2399 loss: 2.420067310333252\n",
      "epoch 2 batch 2400 loss: 2.524740219116211\n",
      "epoch 2 batch 2401 loss: 2.2767322063446045\n",
      "epoch 2 batch 2402 loss: 2.504101514816284\n",
      "epoch 2 batch 2403 loss: 2.4750804901123047\n",
      "epoch 2 batch 2404 loss: 2.5079755783081055\n",
      "epoch 2 batch 2405 loss: 2.299839496612549\n",
      "epoch 2 batch 2406 loss: 2.4070770740509033\n",
      "epoch 2 batch 2407 loss: 2.741144895553589\n",
      "epoch 2 batch 2408 loss: 2.3770432472229004\n",
      "epoch 2 batch 2409 loss: 2.370718002319336\n",
      "epoch 2 batch 2410 loss: 2.5087080001831055\n",
      "epoch 2 batch 2411 loss: 2.319451093673706\n",
      "epoch 2 batch 2412 loss: 2.47446346282959\n",
      "epoch 2 batch 2413 loss: 2.378516674041748\n",
      "epoch 2 batch 2414 loss: 2.3878417015075684\n",
      "epoch 2 batch 2415 loss: 2.767246723175049\n",
      "epoch 2 batch 2416 loss: 2.427868604660034\n",
      "epoch 2 batch 2417 loss: 2.4011473655700684\n",
      "epoch 2 batch 2418 loss: 2.598625421524048\n",
      "epoch 2 batch 2419 loss: 2.3569724559783936\n",
      "epoch 2 batch 2420 loss: 2.5837221145629883\n",
      "epoch 2 batch 2421 loss: 2.5983076095581055\n",
      "epoch 2 batch 2422 loss: 2.421713352203369\n",
      "epoch 2 batch 2423 loss: 2.5943868160247803\n",
      "epoch 2 batch 2424 loss: 2.393319845199585\n",
      "epoch 2 batch 2425 loss: 2.31412410736084\n",
      "epoch 2 batch 2426 loss: 2.541677951812744\n",
      "epoch 2 batch 2427 loss: 2.8361093997955322\n",
      "epoch 2 batch 2428 loss: 2.316622257232666\n",
      "epoch 2 batch 2429 loss: 2.6123528480529785\n",
      "epoch 2 batch 2430 loss: 2.300600290298462\n",
      "epoch 2 batch 2431 loss: 2.2871053218841553\n",
      "epoch 2 batch 2432 loss: 2.251986026763916\n",
      "epoch 2 batch 2433 loss: 2.373467445373535\n",
      "epoch 2 batch 2434 loss: 2.544110059738159\n",
      "epoch 2 batch 2435 loss: 2.226233959197998\n",
      "epoch 2 batch 2436 loss: 2.4891514778137207\n",
      "epoch 2 batch 2437 loss: 2.3783910274505615\n",
      "epoch 2 batch 2438 loss: 2.3982837200164795\n",
      "epoch 2 batch 2439 loss: 2.6090574264526367\n",
      "epoch 2 batch 2440 loss: 2.3266727924346924\n",
      "epoch 2 batch 2441 loss: 2.391202926635742\n",
      "epoch 2 batch 2442 loss: 2.580580234527588\n",
      "epoch 2 batch 2443 loss: 2.416717767715454\n",
      "epoch 2 batch 2444 loss: 2.527738094329834\n",
      "epoch 2 batch 2445 loss: 2.324389934539795\n",
      "epoch 2 batch 2446 loss: 2.4280741214752197\n",
      "epoch 2 batch 2447 loss: 2.416722536087036\n",
      "epoch 2 batch 2448 loss: 2.495415210723877\n",
      "epoch 2 batch 2449 loss: 2.321944236755371\n",
      "epoch 2 batch 2450 loss: 2.46626615524292\n",
      "epoch 2 batch 2451 loss: 2.7052412033081055\n",
      "epoch 2 batch 2452 loss: 2.3800597190856934\n",
      "epoch 2 batch 2453 loss: 2.648131847381592\n",
      "epoch 2 batch 2454 loss: 2.3751659393310547\n",
      "epoch 2 batch 2455 loss: 2.2659378051757812\n",
      "epoch 2 batch 2456 loss: 2.5838074684143066\n",
      "epoch 2 batch 2457 loss: 2.4892220497131348\n",
      "epoch 2 batch 2458 loss: 2.891878128051758\n",
      "epoch 2 batch 2459 loss: 2.4676501750946045\n",
      "epoch 2 batch 2460 loss: 2.4541563987731934\n",
      "epoch 2 batch 2461 loss: 2.2999420166015625\n",
      "epoch 2 batch 2462 loss: 2.373533248901367\n",
      "epoch 2 batch 2463 loss: 2.368215799331665\n",
      "epoch 2 batch 2464 loss: 2.7018144130706787\n",
      "epoch 2 batch 2465 loss: 2.523635149002075\n",
      "epoch 2 batch 2466 loss: 2.275883674621582\n",
      "epoch 2 batch 2467 loss: 2.4139723777770996\n",
      "epoch 2 batch 2468 loss: 2.601381540298462\n",
      "epoch 2 batch 2469 loss: 2.4729514122009277\n",
      "epoch 2 batch 2470 loss: 2.68959379196167\n",
      "epoch 2 batch 2471 loss: 2.403240203857422\n",
      "epoch 2 batch 2472 loss: 2.6945135593414307\n",
      "epoch 2 batch 2473 loss: 2.193174362182617\n",
      "epoch 2 batch 2474 loss: 2.5731940269470215\n",
      "epoch 2 batch 2475 loss: 2.5613064765930176\n",
      "epoch 2 batch 2476 loss: 2.559194326400757\n",
      "epoch 2 batch 2477 loss: 2.6214184761047363\n",
      "epoch 2 batch 2478 loss: 2.5754549503326416\n",
      "epoch 2 batch 2479 loss: 2.46730375289917\n",
      "epoch 2 batch 2480 loss: 2.439953565597534\n",
      "epoch 2 batch 2481 loss: 2.5168094635009766\n",
      "epoch 2 batch 2482 loss: 2.641547918319702\n",
      "epoch 2 batch 2483 loss: 2.4674901962280273\n",
      "epoch 2 batch 2484 loss: 2.5788686275482178\n",
      "epoch 2 batch 2485 loss: 2.2958853244781494\n",
      "epoch 2 batch 2486 loss: 2.2770490646362305\n",
      "epoch 2 batch 2487 loss: 2.2500083446502686\n",
      "epoch 2 batch 2488 loss: 2.43672251701355\n",
      "epoch 2 batch 2489 loss: 2.495027780532837\n",
      "epoch 2 batch 2490 loss: 2.9106154441833496\n",
      "epoch 2 batch 2491 loss: 2.2913074493408203\n",
      "epoch 2 batch 2492 loss: 2.854125738143921\n",
      "epoch 2 batch 2493 loss: 2.622840642929077\n",
      "epoch 2 batch 2494 loss: 2.590158224105835\n",
      "epoch 2 batch 2495 loss: 2.2722153663635254\n",
      "epoch 2 batch 2496 loss: 2.3791985511779785\n",
      "epoch 2 batch 2497 loss: 2.518127202987671\n",
      "epoch 2 batch 2498 loss: 2.3293771743774414\n",
      "epoch 2 batch 2499 loss: 2.3840227127075195\n",
      "epoch 2 batch 2500 loss: 2.6262941360473633\n",
      "epoch 2 batch 2501 loss: 2.4687962532043457\n",
      "epoch 2 batch 2502 loss: 2.6505088806152344\n",
      "epoch 2 batch 2503 loss: 2.378836154937744\n",
      "epoch 2 batch 2504 loss: 2.8103437423706055\n",
      "epoch 2 batch 2505 loss: 2.3982439041137695\n",
      "epoch 2 batch 2506 loss: 2.4795546531677246\n",
      "epoch 2 batch 2507 loss: 2.3957443237304688\n",
      "epoch 2 batch 2508 loss: 2.554966688156128\n",
      "epoch 2 batch 2509 loss: 2.5573832988739014\n",
      "epoch 2 batch 2510 loss: 2.3936686515808105\n",
      "epoch 2 batch 2511 loss: 2.512958526611328\n",
      "epoch 2 batch 2512 loss: 2.427661895751953\n",
      "epoch 2 batch 2513 loss: 2.4432878494262695\n",
      "epoch 2 batch 2514 loss: 2.3742213249206543\n",
      "epoch 2 batch 2515 loss: 2.4047536849975586\n",
      "epoch 2 batch 2516 loss: 2.4449589252471924\n",
      "epoch 2 batch 2517 loss: 2.592146396636963\n",
      "epoch 2 batch 2518 loss: 2.391206741333008\n",
      "epoch 2 batch 2519 loss: 2.529402732849121\n",
      "epoch 2 batch 2520 loss: 2.3776726722717285\n",
      "epoch 2 batch 2521 loss: 2.4777719974517822\n",
      "epoch 2 batch 2522 loss: 2.425955295562744\n",
      "epoch 2 batch 2523 loss: 2.3049240112304688\n",
      "epoch 2 batch 2524 loss: 2.3401315212249756\n",
      "epoch 2 batch 2525 loss: 2.289527177810669\n",
      "epoch 2 batch 2526 loss: 2.4370291233062744\n",
      "epoch 2 batch 2527 loss: 2.4382314682006836\n",
      "epoch 2 batch 2528 loss: 2.4616458415985107\n",
      "epoch 2 batch 2529 loss: 2.5292809009552\n",
      "epoch 2 batch 2530 loss: 2.4851791858673096\n",
      "epoch 2 batch 2531 loss: 2.210859775543213\n",
      "epoch 2 batch 2532 loss: 3.2147419452667236\n",
      "epoch 2 batch 2533 loss: 2.22467041015625\n",
      "epoch 2 batch 2534 loss: 2.591135025024414\n",
      "epoch 2 batch 2535 loss: 2.5352416038513184\n",
      "epoch 2 batch 2536 loss: 2.244513750076294\n",
      "epoch 2 batch 2537 loss: 2.3883681297302246\n",
      "epoch 2 batch 2538 loss: 2.3925106525421143\n",
      "epoch 2 batch 2539 loss: 2.6450672149658203\n",
      "epoch 2 batch 2540 loss: 2.515066623687744\n",
      "epoch 2 batch 2541 loss: 2.4851813316345215\n",
      "epoch 2 batch 2542 loss: 2.3511276245117188\n",
      "epoch 2 batch 2543 loss: 2.2055249214172363\n",
      "epoch 2 batch 2544 loss: 2.637644052505493\n",
      "epoch 2 batch 2545 loss: 2.3827662467956543\n",
      "epoch 2 batch 2546 loss: 2.672182559967041\n",
      "epoch 2 batch 2547 loss: 2.385354518890381\n",
      "epoch 2 batch 2548 loss: 2.4470789432525635\n",
      "epoch 2 batch 2549 loss: 2.6513357162475586\n",
      "epoch 2 batch 2550 loss: 2.556142568588257\n",
      "epoch 2 batch 2551 loss: 2.3784122467041016\n",
      "epoch 2 batch 2552 loss: 2.3641037940979004\n",
      "epoch 2 batch 2553 loss: 2.366126298904419\n",
      "epoch 2 batch 2554 loss: 2.43144154548645\n",
      "epoch 2 batch 2555 loss: 2.4125585556030273\n",
      "epoch 2 batch 2556 loss: 2.2446718215942383\n",
      "epoch 2 batch 2557 loss: 2.7461323738098145\n",
      "epoch 2 batch 2558 loss: 2.371417999267578\n",
      "epoch 2 batch 2559 loss: 2.379615306854248\n",
      "epoch 2 batch 2560 loss: 2.4763474464416504\n",
      "epoch 2 batch 2561 loss: 2.182126045227051\n",
      "epoch 2 batch 2562 loss: 2.3830716609954834\n",
      "epoch 2 batch 2563 loss: 2.3520615100860596\n",
      "epoch 2 batch 2564 loss: 2.321481704711914\n",
      "epoch 2 batch 2565 loss: 2.3369946479797363\n",
      "epoch 2 batch 2566 loss: 2.4232256412506104\n",
      "epoch 2 batch 2567 loss: 2.593498468399048\n",
      "epoch 2 batch 2568 loss: 2.5707571506500244\n",
      "epoch 2 batch 2569 loss: 2.5188775062561035\n",
      "epoch 2 batch 2570 loss: 2.6883649826049805\n",
      "epoch 2 batch 2571 loss: 2.7051849365234375\n",
      "epoch 2 batch 2572 loss: 2.3210902214050293\n",
      "epoch 2 batch 2573 loss: 2.75048565864563\n",
      "epoch 2 batch 2574 loss: 2.6505703926086426\n",
      "epoch 2 batch 2575 loss: 2.438206672668457\n",
      "epoch 2 batch 2576 loss: 2.6207172870635986\n",
      "epoch 2 batch 2577 loss: 2.596433162689209\n",
      "epoch 2 batch 2578 loss: 2.2682862281799316\n",
      "epoch 2 batch 2579 loss: 2.7910470962524414\n",
      "epoch 2 batch 2580 loss: 2.4847497940063477\n",
      "epoch 2 batch 2581 loss: 2.373567819595337\n",
      "epoch 2 batch 2582 loss: 2.6939263343811035\n",
      "epoch 2 batch 2583 loss: 2.5707173347473145\n",
      "epoch 2 batch 2584 loss: 2.566685438156128\n",
      "epoch 2 batch 2585 loss: 2.5607757568359375\n",
      "epoch 2 batch 2586 loss: 2.6807303428649902\n",
      "epoch 2 batch 2587 loss: 2.551494598388672\n",
      "epoch 2 batch 2588 loss: 2.4274253845214844\n",
      "epoch 2 batch 2589 loss: 2.5336568355560303\n",
      "epoch 2 batch 2590 loss: 2.803554058074951\n",
      "epoch 2 batch 2591 loss: 2.3719635009765625\n",
      "epoch 2 batch 2592 loss: 2.4113998413085938\n",
      "epoch 2 batch 2593 loss: 2.558037281036377\n",
      "epoch 2 batch 2594 loss: 2.3612074851989746\n",
      "epoch 2 batch 2595 loss: 2.6703269481658936\n",
      "epoch 2 batch 2596 loss: 2.2380290031433105\n",
      "epoch 2 batch 2597 loss: 2.507127285003662\n",
      "epoch 2 batch 2598 loss: 2.3909313678741455\n",
      "epoch 2 batch 2599 loss: 2.582028865814209\n",
      "epoch 2 batch 2600 loss: 2.1138510704040527\n",
      "epoch 2 batch 2601 loss: 2.6980957984924316\n",
      "epoch 2 batch 2602 loss: 2.3429512977600098\n",
      "epoch 2 batch 2603 loss: 2.3152928352355957\n",
      "epoch 2 batch 2604 loss: 2.4464683532714844\n",
      "epoch 2 batch 2605 loss: 2.7273926734924316\n",
      "epoch 2 batch 2606 loss: 2.586780548095703\n",
      "epoch 2 batch 2607 loss: 2.2406365871429443\n",
      "epoch 2 batch 2608 loss: 2.7095794677734375\n",
      "epoch 2 batch 2609 loss: 2.4031264781951904\n",
      "epoch 2 batch 2610 loss: 2.4771859645843506\n",
      "epoch 2 batch 2611 loss: 2.7321836948394775\n",
      "epoch 2 batch 2612 loss: 2.3397250175476074\n",
      "epoch 2 batch 2613 loss: 2.358340263366699\n",
      "epoch 2 batch 2614 loss: 2.2914915084838867\n",
      "epoch 2 batch 2615 loss: 2.528689384460449\n",
      "epoch 2 batch 2616 loss: 2.2197060585021973\n",
      "epoch 2 batch 2617 loss: 2.3552026748657227\n",
      "epoch 2 batch 2618 loss: 2.3568568229675293\n",
      "epoch 2 batch 2619 loss: 2.3790781497955322\n",
      "epoch 2 batch 2620 loss: 2.403398275375366\n",
      "epoch 2 batch 2621 loss: 2.5008506774902344\n",
      "epoch 2 batch 2622 loss: 2.368152618408203\n",
      "epoch 2 batch 2623 loss: 2.358811855316162\n",
      "epoch 2 batch 2624 loss: 2.4106645584106445\n",
      "epoch 2 batch 2625 loss: 2.3225760459899902\n",
      "epoch 2 batch 2626 loss: 2.502152919769287\n",
      "epoch 2 batch 2627 loss: 2.581444025039673\n",
      "epoch 2 batch 2628 loss: 2.293958902359009\n",
      "epoch 2 batch 2629 loss: 2.490109920501709\n",
      "epoch 2 batch 2630 loss: 2.4059505462646484\n",
      "epoch 2 batch 2631 loss: 2.4171206951141357\n",
      "epoch 2 batch 2632 loss: 2.2983739376068115\n",
      "epoch 2 batch 2633 loss: 2.5176103115081787\n",
      "epoch 2 batch 2634 loss: 2.5528998374938965\n",
      "epoch 2 batch 2635 loss: 2.4250106811523438\n",
      "epoch 2 batch 2636 loss: 2.5422792434692383\n",
      "epoch 2 batch 2637 loss: 2.8033952713012695\n",
      "epoch 2 batch 2638 loss: 2.4246554374694824\n",
      "epoch 2 batch 2639 loss: 2.445263385772705\n",
      "epoch 2 batch 2640 loss: 2.4945068359375\n",
      "epoch 2 batch 2641 loss: 2.797901153564453\n",
      "epoch 2 batch 2642 loss: 2.286027431488037\n",
      "epoch 2 batch 2643 loss: 2.3570594787597656\n",
      "epoch 2 batch 2644 loss: 2.3657679557800293\n",
      "epoch 2 batch 2645 loss: 2.2802019119262695\n",
      "epoch 2 batch 2646 loss: 2.7451653480529785\n",
      "epoch 2 batch 2647 loss: 2.528714179992676\n",
      "epoch 2 batch 2648 loss: 2.600167989730835\n",
      "epoch 2 batch 2649 loss: 2.271778106689453\n",
      "epoch 2 batch 2650 loss: 2.377513885498047\n",
      "epoch 2 batch 2651 loss: 2.431582450866699\n",
      "epoch 2 batch 2652 loss: 2.5700221061706543\n",
      "epoch 2 batch 2653 loss: 2.384092092514038\n",
      "epoch 2 batch 2654 loss: 2.420746326446533\n",
      "epoch 2 batch 2655 loss: 2.6260712146759033\n",
      "epoch 2 batch 2656 loss: 2.295140266418457\n",
      "epoch 2 batch 2657 loss: 2.401895523071289\n",
      "epoch 2 batch 2658 loss: 2.409881353378296\n",
      "epoch 2 batch 2659 loss: 2.0764689445495605\n",
      "epoch 2 batch 2660 loss: 2.7568418979644775\n",
      "epoch 2 batch 2661 loss: 2.3704638481140137\n",
      "epoch 2 batch 2662 loss: 2.4842090606689453\n",
      "epoch 2 batch 2663 loss: 2.462583303451538\n",
      "epoch 2 batch 2664 loss: 2.6684727668762207\n",
      "epoch 2 batch 2665 loss: 2.4147837162017822\n",
      "epoch 2 batch 2666 loss: 2.4444661140441895\n",
      "epoch 2 batch 2667 loss: 2.0446617603302\n",
      "epoch 2 batch 2668 loss: 2.582280158996582\n",
      "epoch 2 batch 2669 loss: 2.4616341590881348\n",
      "epoch 2 batch 2670 loss: 2.243802070617676\n",
      "epoch 2 batch 2671 loss: 2.4608216285705566\n",
      "epoch 2 batch 2672 loss: 2.2238519191741943\n",
      "epoch 2 batch 2673 loss: 2.592862367630005\n",
      "epoch 2 batch 2674 loss: 2.29702091217041\n",
      "epoch 2 batch 2675 loss: 2.323817253112793\n",
      "epoch 2 batch 2676 loss: 2.3132362365722656\n",
      "epoch 2 batch 2677 loss: 2.437991142272949\n",
      "epoch 2 batch 2678 loss: 2.689753532409668\n",
      "epoch 2 batch 2679 loss: 2.145941734313965\n",
      "epoch 2 batch 2680 loss: 2.548182725906372\n",
      "epoch 2 batch 2681 loss: 2.2292401790618896\n",
      "epoch 2 batch 2682 loss: 2.315530776977539\n",
      "epoch 2 batch 2683 loss: 2.6480159759521484\n",
      "epoch 2 batch 2684 loss: 2.4771313667297363\n",
      "epoch 2 batch 2685 loss: 2.2516517639160156\n",
      "epoch 2 batch 2686 loss: 2.245457649230957\n",
      "epoch 2 batch 2687 loss: 2.697983503341675\n",
      "epoch 2 batch 2688 loss: 2.449951648712158\n",
      "epoch 2 batch 2689 loss: 2.2719595432281494\n",
      "epoch 2 batch 2690 loss: 2.4385547637939453\n",
      "epoch 2 batch 2691 loss: 2.4212560653686523\n",
      "epoch 2 batch 2692 loss: 2.498291015625\n",
      "epoch 2 batch 2693 loss: 2.5976080894470215\n",
      "epoch 2 batch 2694 loss: 2.227966785430908\n",
      "epoch 2 batch 2695 loss: 2.742154121398926\n",
      "epoch 2 batch 2696 loss: 2.8221676349639893\n",
      "epoch 2 batch 2697 loss: 2.4781863689422607\n",
      "epoch 2 batch 2698 loss: 2.4411282539367676\n",
      "epoch 2 batch 2699 loss: 2.4477250576019287\n",
      "epoch 2 batch 2700 loss: 2.574234962463379\n",
      "epoch 2 batch 2701 loss: 2.5969114303588867\n",
      "epoch 2 batch 2702 loss: 2.278794765472412\n",
      "epoch 2 batch 2703 loss: 2.351123332977295\n",
      "epoch 2 batch 2704 loss: 2.4803099632263184\n",
      "epoch 2 batch 2705 loss: 2.579068660736084\n",
      "epoch 2 batch 2706 loss: 2.344477891921997\n",
      "epoch 2 batch 2707 loss: 2.5681943893432617\n",
      "epoch 2 batch 2708 loss: 2.614485740661621\n",
      "epoch 2 batch 2709 loss: 2.5981638431549072\n",
      "epoch 2 batch 2710 loss: 2.5064871311187744\n",
      "epoch 2 batch 2711 loss: 2.4932737350463867\n",
      "epoch 2 batch 2712 loss: 2.4092869758605957\n",
      "epoch 2 batch 2713 loss: 2.255589246749878\n",
      "epoch 2 batch 2714 loss: 2.4747204780578613\n",
      "epoch 2 batch 2715 loss: 2.360767364501953\n",
      "epoch 2 batch 2716 loss: 2.339259386062622\n",
      "epoch 2 batch 2717 loss: 2.393608570098877\n",
      "epoch 2 batch 2718 loss: 2.3511266708374023\n",
      "epoch 2 batch 2719 loss: 2.4844377040863037\n",
      "epoch 2 batch 2720 loss: 2.4360976219177246\n",
      "epoch 2 batch 2721 loss: 2.516036033630371\n",
      "epoch 2 batch 2722 loss: 2.4727532863616943\n",
      "epoch 2 batch 2723 loss: 2.8730030059814453\n",
      "epoch 2 batch 2724 loss: 2.347726345062256\n",
      "epoch 2 batch 2725 loss: 2.1668169498443604\n",
      "epoch 2 batch 2726 loss: 2.490730047225952\n",
      "epoch 2 batch 2727 loss: 2.408316135406494\n",
      "epoch 2 batch 2728 loss: 2.212268829345703\n",
      "epoch 2 batch 2729 loss: 2.406895160675049\n",
      "epoch 2 batch 2730 loss: 2.522637367248535\n",
      "epoch 2 batch 2731 loss: 2.7146410942077637\n",
      "epoch 2 batch 2732 loss: 2.290741443634033\n",
      "epoch 2 batch 2733 loss: 2.5055365562438965\n",
      "epoch 2 batch 2734 loss: 2.5455892086029053\n",
      "epoch 2 batch 2735 loss: 2.5444390773773193\n",
      "epoch 2 batch 2736 loss: 2.5212011337280273\n",
      "epoch 2 batch 2737 loss: 2.5242581367492676\n",
      "epoch 2 batch 2738 loss: 2.3983490467071533\n",
      "epoch 2 batch 2739 loss: 2.253248453140259\n",
      "epoch 2 batch 2740 loss: 2.6770591735839844\n",
      "epoch 2 batch 2741 loss: 2.376311779022217\n",
      "epoch 2 batch 2742 loss: 2.3935885429382324\n",
      "epoch 2 batch 2743 loss: 2.53318190574646\n",
      "epoch 2 batch 2744 loss: 2.4853134155273438\n",
      "epoch 2 batch 2745 loss: 2.3106632232666016\n",
      "epoch 2 batch 2746 loss: 2.361576557159424\n",
      "epoch 2 batch 2747 loss: 2.6037850379943848\n",
      "epoch 2 batch 2748 loss: 2.202733039855957\n",
      "epoch 2 batch 2749 loss: 2.5022449493408203\n",
      "epoch 2 batch 2750 loss: 2.4654955863952637\n",
      "epoch 2 batch 2751 loss: 2.663639545440674\n",
      "epoch 2 batch 2752 loss: 2.5490379333496094\n",
      "epoch 2 batch 2753 loss: 2.4845056533813477\n",
      "epoch 2 batch 2754 loss: 2.6936938762664795\n",
      "epoch 2 batch 2755 loss: 2.2470738887786865\n",
      "epoch 2 batch 2756 loss: 2.4383809566497803\n",
      "epoch 2 batch 2757 loss: 2.318545341491699\n",
      "epoch 2 batch 2758 loss: 2.1244187355041504\n",
      "epoch 2 batch 2759 loss: 2.5136282444000244\n",
      "epoch 2 batch 2760 loss: 2.4794468879699707\n",
      "epoch 2 batch 2761 loss: 2.3357009887695312\n",
      "epoch 2 batch 2762 loss: 2.4649908542633057\n",
      "epoch 2 batch 2763 loss: 2.990605354309082\n",
      "epoch 2 batch 2764 loss: 2.5909836292266846\n",
      "epoch 2 batch 2765 loss: 2.5266473293304443\n",
      "epoch 2 batch 2766 loss: 2.556875228881836\n",
      "epoch 2 batch 2767 loss: 2.323082447052002\n",
      "epoch 2 batch 2768 loss: 2.6897804737091064\n",
      "epoch 2 batch 2769 loss: 2.3756327629089355\n",
      "epoch 2 batch 2770 loss: 2.4235951900482178\n",
      "epoch 2 batch 2771 loss: 2.355046272277832\n",
      "epoch 2 batch 2772 loss: 2.485840320587158\n",
      "epoch 2 batch 2773 loss: 2.426161289215088\n",
      "epoch 2 batch 2774 loss: 2.4612107276916504\n",
      "epoch 2 batch 2775 loss: 2.409061908721924\n",
      "epoch 2 batch 2776 loss: 2.329481601715088\n",
      "epoch 2 batch 2777 loss: 2.5531463623046875\n",
      "epoch 2 batch 2778 loss: 2.547229290008545\n",
      "epoch 2 batch 2779 loss: 2.592522144317627\n",
      "epoch 2 batch 2780 loss: 2.3611106872558594\n",
      "epoch 2 batch 2781 loss: 2.2817742824554443\n",
      "epoch 2 batch 2782 loss: 2.3131771087646484\n",
      "epoch 2 batch 2783 loss: 2.4534342288970947\n",
      "epoch 2 batch 2784 loss: 2.4803786277770996\n",
      "epoch 2 batch 2785 loss: 2.5799732208251953\n",
      "epoch 2 batch 2786 loss: 2.5243921279907227\n",
      "epoch 2 batch 2787 loss: 2.188262462615967\n",
      "epoch 2 batch 2788 loss: 2.534400463104248\n",
      "epoch 2 batch 2789 loss: 2.4870240688323975\n",
      "epoch 2 batch 2790 loss: 2.222519874572754\n",
      "epoch 2 batch 2791 loss: 2.533179759979248\n",
      "epoch 2 batch 2792 loss: 2.6375675201416016\n",
      "epoch 2 batch 2793 loss: 2.5352296829223633\n",
      "epoch 2 batch 2794 loss: 2.729788303375244\n",
      "epoch 2 batch 2795 loss: 3.1158275604248047\n",
      "epoch 2 batch 2796 loss: 2.4465365409851074\n",
      "epoch 2 batch 2797 loss: 2.457956314086914\n",
      "epoch 2 batch 2798 loss: 2.3241326808929443\n",
      "epoch 2 batch 2799 loss: 2.4966533184051514\n",
      "epoch 2 batch 2800 loss: 2.3325159549713135\n",
      "epoch 2 batch 2801 loss: 2.440181255340576\n",
      "epoch 2 batch 2802 loss: 2.4440712928771973\n",
      "epoch 2 batch 2803 loss: 2.387632369995117\n",
      "epoch 2 batch 2804 loss: 2.9859097003936768\n",
      "epoch 2 batch 2805 loss: 2.6233577728271484\n",
      "epoch 2 batch 2806 loss: 2.572578191757202\n",
      "epoch 2 batch 2807 loss: 2.307591199874878\n",
      "epoch 2 batch 2808 loss: 2.5226006507873535\n",
      "epoch 2 batch 2809 loss: 2.373310089111328\n",
      "epoch 2 batch 2810 loss: 2.1822991371154785\n",
      "epoch 2 batch 2811 loss: 2.591512680053711\n",
      "epoch 2 batch 2812 loss: 2.192838430404663\n",
      "epoch 2 batch 2813 loss: 2.616703748703003\n",
      "epoch 2 batch 2814 loss: 2.561441421508789\n",
      "epoch 2 batch 2815 loss: 2.459927797317505\n",
      "epoch 2 batch 2816 loss: 2.524514675140381\n",
      "epoch 2 batch 2817 loss: 2.7230215072631836\n",
      "epoch 2 batch 2818 loss: 2.3262858390808105\n",
      "epoch 2 batch 2819 loss: 2.443995475769043\n",
      "epoch 2 batch 2820 loss: 2.731992721557617\n",
      "epoch 2 batch 2821 loss: 2.493645191192627\n",
      "epoch 2 batch 2822 loss: 2.3012051582336426\n",
      "epoch 2 batch 2823 loss: 2.478940486907959\n",
      "epoch 2 batch 2824 loss: 2.1998953819274902\n",
      "epoch 2 batch 2825 loss: 2.368239402770996\n",
      "epoch 2 batch 2826 loss: 2.352921724319458\n",
      "epoch 2 batch 2827 loss: 2.5344696044921875\n",
      "epoch 2 batch 2828 loss: 2.5532047748565674\n",
      "epoch 2 batch 2829 loss: 2.576096534729004\n",
      "epoch 2 batch 2830 loss: 2.378377914428711\n",
      "epoch 2 batch 2831 loss: 2.3987131118774414\n",
      "epoch 2 batch 2832 loss: 2.4851551055908203\n",
      "epoch 2 batch 2833 loss: 2.57425594329834\n",
      "epoch 2 batch 2834 loss: 2.290506362915039\n",
      "epoch 2 batch 2835 loss: 2.2430174350738525\n",
      "epoch 2 batch 2836 loss: 2.5053398609161377\n",
      "epoch 2 batch 2837 loss: 2.593592882156372\n",
      "epoch 2 batch 2838 loss: 2.3175597190856934\n",
      "epoch 2 batch 2839 loss: 2.2753071784973145\n",
      "epoch 2 batch 2840 loss: 2.448867082595825\n",
      "epoch 2 batch 2841 loss: 2.6434576511383057\n",
      "epoch 2 batch 2842 loss: 2.3873300552368164\n",
      "epoch 2 batch 2843 loss: 2.234229326248169\n",
      "epoch 2 batch 2844 loss: 2.5021073818206787\n",
      "epoch 2 batch 2845 loss: 2.418212413787842\n",
      "epoch 2 batch 2846 loss: 2.1196298599243164\n",
      "epoch 2 batch 2847 loss: 2.176182508468628\n",
      "epoch 2 batch 2848 loss: 2.219632863998413\n",
      "epoch 2 batch 2849 loss: 2.4611916542053223\n",
      "epoch 2 batch 2850 loss: 2.4324374198913574\n",
      "epoch 2 batch 2851 loss: 2.4675605297088623\n",
      "epoch 2 batch 2852 loss: 2.638721466064453\n",
      "epoch 2 batch 2853 loss: 2.5160789489746094\n",
      "epoch 2 batch 2854 loss: 2.616060256958008\n",
      "epoch 2 batch 2855 loss: 2.3830819129943848\n",
      "epoch 2 batch 2856 loss: 2.4679617881774902\n",
      "epoch 2 batch 2857 loss: 2.4673643112182617\n",
      "epoch 2 batch 2858 loss: 2.824456214904785\n",
      "epoch 2 batch 2859 loss: 2.3501157760620117\n",
      "epoch 2 batch 2860 loss: 2.5359413623809814\n",
      "epoch 2 batch 2861 loss: 2.4632654190063477\n",
      "epoch 2 batch 2862 loss: 2.1898703575134277\n",
      "epoch 2 batch 2863 loss: 2.2220728397369385\n",
      "epoch 2 batch 2864 loss: 2.261624336242676\n",
      "epoch 2 batch 2865 loss: 2.6021876335144043\n",
      "epoch 2 batch 2866 loss: 2.5616238117218018\n",
      "epoch 2 batch 2867 loss: 2.8475284576416016\n",
      "epoch 2 batch 2868 loss: 2.7135066986083984\n",
      "epoch 2 batch 2869 loss: 2.3821496963500977\n",
      "epoch 2 batch 2870 loss: 2.7833127975463867\n",
      "epoch 2 batch 2871 loss: 2.117055892944336\n",
      "epoch 2 batch 2872 loss: 2.6724979877471924\n",
      "epoch 2 batch 2873 loss: 2.633331298828125\n",
      "epoch 2 batch 2874 loss: 2.2144625186920166\n",
      "epoch 2 batch 2875 loss: 2.4522900581359863\n",
      "epoch 2 batch 2876 loss: 2.2318012714385986\n",
      "epoch 2 batch 2877 loss: 2.550863742828369\n",
      "epoch 2 batch 2878 loss: 2.3442039489746094\n",
      "epoch 2 batch 2879 loss: 2.452623128890991\n",
      "epoch 2 batch 2880 loss: 2.7333896160125732\n",
      "epoch 2 batch 2881 loss: 2.5808260440826416\n",
      "epoch 2 batch 2882 loss: 2.710791826248169\n",
      "epoch 2 batch 2883 loss: 2.5854549407958984\n",
      "epoch 2 batch 2884 loss: 2.406813144683838\n",
      "epoch 2 batch 2885 loss: 2.3684303760528564\n",
      "epoch 2 batch 2886 loss: 2.4515511989593506\n",
      "epoch 2 batch 2887 loss: 2.8568809032440186\n",
      "epoch 2 batch 2888 loss: 2.437358856201172\n",
      "epoch 2 batch 2889 loss: 2.3260769844055176\n",
      "epoch 2 batch 2890 loss: 2.226557970046997\n",
      "epoch 2 batch 2891 loss: 2.432285785675049\n",
      "epoch 2 batch 2892 loss: 2.8216495513916016\n",
      "epoch 2 batch 2893 loss: 2.377730369567871\n",
      "epoch 2 batch 2894 loss: 2.6383109092712402\n",
      "epoch 2 batch 2895 loss: 2.4900710582733154\n",
      "epoch 2 batch 2896 loss: 2.6150455474853516\n",
      "epoch 2 batch 2897 loss: 2.3718457221984863\n",
      "epoch 2 batch 2898 loss: 2.74702525138855\n",
      "epoch 2 batch 2899 loss: 2.8506743907928467\n",
      "epoch 2 batch 2900 loss: 2.1713528633117676\n",
      "epoch 2 batch 2901 loss: 2.2359533309936523\n",
      "epoch 2 batch 2902 loss: 2.3834986686706543\n",
      "epoch 2 batch 2903 loss: 2.330585479736328\n",
      "epoch 2 batch 2904 loss: 2.591907024383545\n",
      "epoch 2 batch 2905 loss: 2.508094549179077\n",
      "epoch 2 batch 2906 loss: 2.7487401962280273\n",
      "epoch 2 batch 2907 loss: 2.549426794052124\n",
      "epoch 2 batch 2908 loss: 2.6085472106933594\n",
      "epoch 2 batch 2909 loss: 2.3957204818725586\n",
      "epoch 2 batch 2910 loss: 2.431164264678955\n",
      "epoch 2 batch 2911 loss: 2.43513822555542\n",
      "epoch 2 batch 2912 loss: 2.4573922157287598\n",
      "epoch 2 batch 2913 loss: 2.3864564895629883\n",
      "epoch 2 batch 2914 loss: 2.389660358428955\n",
      "epoch 2 batch 2915 loss: 2.4460551738739014\n",
      "epoch 2 batch 2916 loss: 2.2844605445861816\n",
      "epoch 2 batch 2917 loss: 2.306011199951172\n",
      "epoch 2 batch 2918 loss: 2.483609199523926\n",
      "epoch 2 batch 2919 loss: 2.4099173545837402\n",
      "epoch 2 batch 2920 loss: 2.2657060623168945\n",
      "epoch 2 batch 2921 loss: 2.289820671081543\n",
      "epoch 2 batch 2922 loss: 2.5616867542266846\n",
      "epoch 2 batch 2923 loss: 2.4335267543792725\n",
      "epoch 2 batch 2924 loss: 2.583949565887451\n",
      "epoch 2 batch 2925 loss: 2.4351580142974854\n",
      "epoch 2 batch 2926 loss: 2.611400842666626\n",
      "epoch 2 batch 2927 loss: 2.681464433670044\n",
      "epoch 2 batch 2928 loss: 2.4379029273986816\n",
      "epoch 2 batch 2929 loss: 2.4054982662200928\n",
      "epoch 2 batch 2930 loss: 2.38165545463562\n",
      "epoch 2 batch 2931 loss: 2.4377310276031494\n",
      "epoch 2 batch 2932 loss: 2.3895578384399414\n",
      "epoch 2 batch 2933 loss: 2.667546272277832\n",
      "epoch 2 batch 2934 loss: 2.5654382705688477\n",
      "epoch 2 batch 2935 loss: 2.685887336730957\n",
      "epoch 2 batch 2936 loss: 2.452824592590332\n",
      "epoch 2 batch 2937 loss: 2.3562445640563965\n",
      "epoch 2 batch 2938 loss: 2.402832508087158\n",
      "epoch 2 batch 2939 loss: 2.5477418899536133\n",
      "epoch 2 batch 2940 loss: 2.435560464859009\n",
      "epoch 2 batch 2941 loss: 2.9788990020751953\n",
      "epoch 2 batch 2942 loss: 2.504931926727295\n",
      "epoch 2 batch 2943 loss: 2.58154296875\n",
      "epoch 2 batch 2944 loss: 2.439393997192383\n",
      "epoch 2 batch 2945 loss: 2.373208999633789\n",
      "epoch 2 batch 2946 loss: 2.28913950920105\n",
      "epoch 2 batch 2947 loss: 2.4209749698638916\n",
      "epoch 2 batch 2948 loss: 2.630420684814453\n",
      "epoch 2 batch 2949 loss: 2.33522891998291\n",
      "epoch 2 batch 2950 loss: 2.2768983840942383\n",
      "epoch 2 batch 2951 loss: 2.2985289096832275\n",
      "epoch 2 batch 2952 loss: 2.6573190689086914\n",
      "epoch 2 batch 2953 loss: 2.6537654399871826\n",
      "epoch 2 batch 2954 loss: 2.4018125534057617\n",
      "epoch 2 batch 2955 loss: 2.552126884460449\n",
      "epoch 2 batch 2956 loss: 2.4083707332611084\n",
      "epoch 2 batch 2957 loss: 2.599256992340088\n",
      "epoch 2 batch 2958 loss: 2.303957939147949\n",
      "epoch 2 batch 2959 loss: 2.5676116943359375\n",
      "epoch 2 batch 2960 loss: 2.512864112854004\n",
      "epoch 2 batch 2961 loss: 2.3937807083129883\n",
      "epoch 2 batch 2962 loss: 2.136590003967285\n",
      "epoch 2 batch 2963 loss: 2.464815378189087\n",
      "epoch 2 batch 2964 loss: 2.4303102493286133\n",
      "epoch 2 batch 2965 loss: 2.3975210189819336\n",
      "epoch 2 batch 2966 loss: 2.617759943008423\n",
      "epoch 2 batch 2967 loss: 2.364096164703369\n",
      "epoch 2 batch 2968 loss: 2.47764253616333\n",
      "epoch 2 batch 2969 loss: 2.5713319778442383\n",
      "epoch 2 batch 2970 loss: 2.393949508666992\n",
      "epoch 2 batch 2971 loss: 2.54874849319458\n",
      "epoch 2 batch 2972 loss: 2.4157187938690186\n",
      "epoch 2 batch 2973 loss: 2.3980259895324707\n",
      "epoch 2 batch 2974 loss: 2.23858642578125\n",
      "epoch 2 batch 2975 loss: 2.635296106338501\n",
      "epoch 2 batch 2976 loss: 2.567688465118408\n",
      "epoch 2 batch 2977 loss: 2.2071692943573\n",
      "epoch 2 batch 2978 loss: 2.3999900817871094\n",
      "epoch 2 batch 2979 loss: 2.4599592685699463\n",
      "epoch 2 batch 2980 loss: 2.6378111839294434\n",
      "epoch 2 batch 2981 loss: 2.511986494064331\n",
      "epoch 2 batch 2982 loss: 2.353663921356201\n",
      "epoch 2 batch 2983 loss: 2.4920034408569336\n",
      "epoch 2 batch 2984 loss: 2.6964292526245117\n",
      "epoch 2 batch 2985 loss: 2.439795970916748\n",
      "epoch 2 batch 2986 loss: 2.4873712062835693\n",
      "epoch 2 batch 2987 loss: 2.7306132316589355\n",
      "epoch 2 batch 2988 loss: 2.3635196685791016\n",
      "epoch 2 batch 2989 loss: 2.3697800636291504\n",
      "epoch 2 batch 2990 loss: 2.554347038269043\n",
      "epoch 2 batch 2991 loss: 2.4726333618164062\n",
      "epoch 2 batch 2992 loss: 2.472486972808838\n",
      "epoch 2 batch 2993 loss: 2.434807777404785\n",
      "epoch 2 batch 2994 loss: 2.352170467376709\n",
      "epoch 2 batch 2995 loss: 2.3153932094573975\n",
      "epoch 2 batch 2996 loss: 2.4636483192443848\n",
      "epoch 2 batch 2997 loss: 2.5878186225891113\n",
      "epoch 2 batch 2998 loss: 2.2656636238098145\n",
      "epoch 2 batch 2999 loss: 2.096182346343994\n",
      "epoch 2 batch 3000 loss: 2.4784772396087646\n",
      "epoch 2 batch 3001 loss: 2.379978895187378\n",
      "epoch 2 batch 3002 loss: 2.338308811187744\n",
      "epoch 2 batch 3003 loss: 2.586726665496826\n",
      "epoch 2 batch 3004 loss: 2.6151130199432373\n",
      "epoch 2 batch 3005 loss: 2.5687146186828613\n",
      "epoch 2 batch 3006 loss: 2.5470359325408936\n",
      "epoch 2 batch 3007 loss: 2.7746849060058594\n",
      "epoch 2 batch 3008 loss: 2.3137879371643066\n",
      "epoch 2 batch 3009 loss: 2.422351360321045\n",
      "epoch 2 batch 3010 loss: 2.651883125305176\n",
      "epoch 2 batch 3011 loss: 2.2499887943267822\n",
      "epoch 2 batch 3012 loss: 2.6535167694091797\n",
      "epoch 2 batch 3013 loss: 2.5233893394470215\n",
      "epoch 2 batch 3014 loss: 2.6838197708129883\n",
      "epoch 2 batch 3015 loss: 2.4159092903137207\n",
      "epoch 2 batch 3016 loss: 2.31973934173584\n",
      "epoch 2 batch 3017 loss: 2.198037624359131\n",
      "epoch 2 batch 3018 loss: 2.493825674057007\n",
      "epoch 2 batch 3019 loss: 2.3751120567321777\n",
      "epoch 2 batch 3020 loss: 2.3315916061401367\n",
      "epoch 2 batch 3021 loss: 2.582122325897217\n",
      "epoch 2 batch 3022 loss: 2.4787111282348633\n",
      "epoch 2 batch 3023 loss: 2.597230911254883\n",
      "epoch 2 batch 3024 loss: 2.452725887298584\n",
      "epoch 2 batch 3025 loss: 2.2552223205566406\n",
      "epoch 2 batch 3026 loss: 2.73335337638855\n",
      "epoch 2 batch 3027 loss: 2.597905397415161\n",
      "epoch 2 batch 3028 loss: 2.2675938606262207\n",
      "epoch 2 batch 3029 loss: 2.200662136077881\n",
      "epoch 2 batch 3030 loss: 2.3403615951538086\n",
      "epoch 2 batch 3031 loss: 2.7053275108337402\n",
      "epoch 2 batch 3032 loss: 2.6151537895202637\n",
      "epoch 2 batch 3033 loss: 2.467623233795166\n",
      "epoch 2 batch 3034 loss: 2.463667869567871\n",
      "epoch 2 batch 3035 loss: 2.4257125854492188\n",
      "epoch 2 batch 3036 loss: 2.277505397796631\n",
      "epoch 2 batch 3037 loss: 2.641460418701172\n",
      "epoch 2 batch 3038 loss: 2.7885451316833496\n",
      "epoch 2 batch 3039 loss: 2.3063673973083496\n",
      "epoch 2 batch 3040 loss: 2.5353097915649414\n",
      "epoch 2 batch 3041 loss: 2.2273244857788086\n",
      "epoch 2 batch 3042 loss: 2.586272716522217\n",
      "epoch 2 batch 3043 loss: 2.1631088256835938\n",
      "epoch 2 batch 3044 loss: 2.538895606994629\n",
      "epoch 2 batch 3045 loss: 2.417670726776123\n",
      "epoch 2 batch 3046 loss: 2.385425090789795\n",
      "epoch 2 batch 3047 loss: 2.5485122203826904\n",
      "epoch 2 batch 3048 loss: 2.7733194828033447\n",
      "epoch 2 batch 3049 loss: 2.4742884635925293\n",
      "epoch 2 batch 3050 loss: 2.609144449234009\n",
      "epoch 2 batch 3051 loss: 2.2447381019592285\n",
      "epoch 2 batch 3052 loss: 2.4007787704467773\n",
      "epoch 2 batch 3053 loss: 2.547417640686035\n",
      "epoch 2 batch 3054 loss: 2.20147705078125\n",
      "epoch 2 batch 3055 loss: 2.275634765625\n",
      "epoch 2 batch 3056 loss: 2.4458322525024414\n",
      "epoch 2 batch 3057 loss: 2.4516048431396484\n",
      "epoch 2 batch 3058 loss: 2.511841058731079\n",
      "epoch 2 batch 3059 loss: 2.351795196533203\n",
      "epoch 2 batch 3060 loss: 2.424027442932129\n",
      "epoch 2 batch 3061 loss: 2.426370620727539\n",
      "epoch 2 batch 3062 loss: 2.279837131500244\n",
      "epoch 2 batch 3063 loss: 2.685819149017334\n",
      "epoch 2 batch 3064 loss: 2.4891624450683594\n",
      "epoch 2 batch 3065 loss: 2.1477291584014893\n",
      "epoch 2 batch 3066 loss: 2.67529559135437\n",
      "epoch 2 batch 3067 loss: 2.4380831718444824\n",
      "epoch 2 batch 3068 loss: 2.5606138706207275\n",
      "epoch 2 batch 3069 loss: 2.289778232574463\n",
      "epoch 2 batch 3070 loss: 2.5545291900634766\n",
      "epoch 2 batch 3071 loss: 3.1752796173095703\n",
      "epoch 2 batch 3072 loss: 2.1799988746643066\n",
      "epoch 2 batch 3073 loss: 2.424602508544922\n",
      "epoch 2 batch 3074 loss: 2.635849952697754\n",
      "epoch 2 batch 3075 loss: 2.395963191986084\n",
      "epoch 2 batch 3076 loss: 2.5867421627044678\n",
      "epoch 2 batch 3077 loss: 2.6079442501068115\n",
      "epoch 2 batch 3078 loss: 2.434382438659668\n",
      "epoch 2 batch 3079 loss: 2.3245389461517334\n",
      "epoch 2 batch 3080 loss: 2.401660442352295\n",
      "epoch 2 batch 3081 loss: 2.442883014678955\n",
      "epoch 2 batch 3082 loss: 2.2946481704711914\n",
      "epoch 2 batch 3083 loss: 2.3317928314208984\n",
      "epoch 2 batch 3084 loss: 2.579965829849243\n",
      "epoch 2 batch 3085 loss: 2.6114585399627686\n",
      "epoch 2 batch 3086 loss: 2.519714832305908\n",
      "epoch 2 batch 3087 loss: 2.6986632347106934\n",
      "epoch 2 batch 3088 loss: 2.5988125801086426\n",
      "epoch 2 batch 3089 loss: 2.443122625350952\n",
      "epoch 2 batch 3090 loss: 2.2861499786376953\n",
      "epoch 2 batch 3091 loss: 2.30488920211792\n",
      "epoch 2 batch 3092 loss: 2.6050760746002197\n",
      "epoch 2 batch 3093 loss: 2.4742817878723145\n",
      "epoch 2 batch 3094 loss: 2.213162422180176\n",
      "epoch 2 batch 3095 loss: 2.324105739593506\n",
      "epoch 2 batch 3096 loss: 2.3493449687957764\n",
      "epoch 2 batch 3097 loss: 2.3848137855529785\n",
      "epoch 2 batch 3098 loss: 2.109901189804077\n",
      "epoch 2 batch 3099 loss: 2.5490267276763916\n",
      "epoch 2 batch 3100 loss: 2.562462329864502\n",
      "epoch 2 batch 3101 loss: 2.1945629119873047\n",
      "epoch 2 batch 3102 loss: 2.504885673522949\n",
      "epoch 2 batch 3103 loss: 2.5468459129333496\n",
      "epoch 2 batch 3104 loss: 2.4774508476257324\n",
      "epoch 2 batch 3105 loss: 2.4416112899780273\n",
      "epoch 2 batch 3106 loss: 2.7336349487304688\n",
      "epoch 2 batch 3107 loss: 2.5255117416381836\n",
      "epoch 2 batch 3108 loss: 2.5674362182617188\n",
      "epoch 2 batch 3109 loss: 2.4446003437042236\n",
      "epoch 2 batch 3110 loss: 2.188490867614746\n",
      "epoch 2 batch 3111 loss: 2.417146921157837\n",
      "epoch 2 batch 3112 loss: 2.644268274307251\n",
      "epoch 2 batch 3113 loss: 2.3750758171081543\n",
      "epoch 2 batch 3114 loss: 2.7058358192443848\n",
      "epoch 2 batch 3115 loss: 2.449808359146118\n",
      "epoch 2 batch 3116 loss: 2.212085485458374\n",
      "epoch 2 batch 3117 loss: 2.5409271717071533\n",
      "epoch 2 batch 3118 loss: 2.4069981575012207\n",
      "epoch 2 batch 3119 loss: 2.4636826515197754\n",
      "epoch 2 batch 3120 loss: 2.633390188217163\n",
      "epoch 2 batch 3121 loss: 2.4368515014648438\n",
      "epoch 2 batch 3122 loss: 2.5811288356781006\n",
      "epoch 2 batch 3123 loss: 2.3086280822753906\n",
      "epoch 2 batch 3124 loss: 2.572995662689209\n",
      "epoch loss: 2.5286148011779783\n",
      "epoch 3 batch 0 loss: 2.438322067260742\n",
      "epoch 3 batch 1 loss: 2.5658118724823\n",
      "epoch 3 batch 2 loss: 2.41202974319458\n",
      "epoch 3 batch 3 loss: 2.334409236907959\n",
      "epoch 3 batch 4 loss: 2.6580379009246826\n",
      "epoch 3 batch 5 loss: 2.2910516262054443\n",
      "epoch 3 batch 6 loss: 2.7311079502105713\n",
      "epoch 3 batch 7 loss: 2.179682970046997\n",
      "epoch 3 batch 8 loss: 2.392796277999878\n",
      "epoch 3 batch 9 loss: 2.49191951751709\n",
      "epoch 3 batch 10 loss: 2.324335813522339\n",
      "epoch 3 batch 11 loss: 2.4612932205200195\n",
      "epoch 3 batch 12 loss: 2.161548614501953\n",
      "epoch 3 batch 13 loss: 2.5684666633605957\n",
      "epoch 3 batch 14 loss: 2.369753360748291\n",
      "epoch 3 batch 15 loss: 2.703423023223877\n",
      "epoch 3 batch 16 loss: 2.5947647094726562\n",
      "epoch 3 batch 17 loss: 2.6118741035461426\n",
      "epoch 3 batch 18 loss: 2.486083984375\n",
      "epoch 3 batch 19 loss: 2.2581982612609863\n",
      "epoch 3 batch 20 loss: 2.6602554321289062\n",
      "epoch 3 batch 21 loss: 2.4906797409057617\n",
      "epoch 3 batch 22 loss: 2.2131729125976562\n",
      "epoch 3 batch 23 loss: 2.168224573135376\n",
      "epoch 3 batch 24 loss: 2.3865408897399902\n",
      "epoch 3 batch 25 loss: 2.6252925395965576\n",
      "epoch 3 batch 26 loss: 2.663064479827881\n",
      "epoch 3 batch 27 loss: 2.623220443725586\n",
      "epoch 3 batch 28 loss: 2.7924304008483887\n",
      "epoch 3 batch 29 loss: 2.435199499130249\n",
      "epoch 3 batch 30 loss: 2.2129149436950684\n",
      "epoch 3 batch 31 loss: 2.4934935569763184\n",
      "epoch 3 batch 32 loss: 2.472522258758545\n",
      "epoch 3 batch 33 loss: 2.660980224609375\n",
      "epoch 3 batch 34 loss: 2.223663568496704\n",
      "epoch 3 batch 35 loss: 2.3453304767608643\n",
      "epoch 3 batch 36 loss: 2.2444119453430176\n",
      "epoch 3 batch 37 loss: 2.2881650924682617\n",
      "epoch 3 batch 38 loss: 2.7411916255950928\n",
      "epoch 3 batch 39 loss: 2.479318618774414\n",
      "epoch 3 batch 40 loss: 2.333552837371826\n",
      "epoch 3 batch 41 loss: 2.5807719230651855\n",
      "epoch 3 batch 42 loss: 2.39508056640625\n",
      "epoch 3 batch 43 loss: 2.54296875\n",
      "epoch 3 batch 44 loss: 2.4924020767211914\n",
      "epoch 3 batch 45 loss: 2.1991424560546875\n",
      "epoch 3 batch 46 loss: 2.323122978210449\n",
      "epoch 3 batch 47 loss: 2.397007942199707\n",
      "epoch 3 batch 48 loss: 2.4348206520080566\n",
      "epoch 3 batch 49 loss: 2.215566635131836\n",
      "epoch 3 batch 50 loss: 2.567303419113159\n",
      "epoch 3 batch 51 loss: 2.1264808177948\n",
      "epoch 3 batch 52 loss: 2.5986177921295166\n",
      "epoch 3 batch 53 loss: 2.6846113204956055\n",
      "epoch 3 batch 54 loss: 2.366363048553467\n",
      "epoch 3 batch 55 loss: 2.5105957984924316\n",
      "epoch 3 batch 56 loss: 2.3259849548339844\n",
      "epoch 3 batch 57 loss: 2.6996045112609863\n",
      "epoch 3 batch 58 loss: 2.630950450897217\n",
      "epoch 3 batch 59 loss: 2.587764263153076\n",
      "epoch 3 batch 60 loss: 2.2273812294006348\n",
      "epoch 3 batch 61 loss: 2.853572130203247\n",
      "epoch 3 batch 62 loss: 2.6180577278137207\n",
      "epoch 3 batch 63 loss: 2.6686997413635254\n",
      "epoch 3 batch 64 loss: 2.2935452461242676\n",
      "epoch 3 batch 65 loss: 2.505497694015503\n",
      "epoch 3 batch 66 loss: 2.308120012283325\n",
      "epoch 3 batch 67 loss: 2.8508176803588867\n",
      "epoch 3 batch 68 loss: 2.58522891998291\n",
      "epoch 3 batch 69 loss: 2.4486329555511475\n",
      "epoch 3 batch 70 loss: 2.657160520553589\n",
      "epoch 3 batch 71 loss: 2.629395008087158\n",
      "epoch 3 batch 72 loss: 2.2682323455810547\n",
      "epoch 3 batch 73 loss: 2.611478805541992\n",
      "epoch 3 batch 74 loss: 2.5024824142456055\n",
      "epoch 3 batch 75 loss: 2.390530586242676\n",
      "epoch 3 batch 76 loss: 2.3527400493621826\n",
      "epoch 3 batch 77 loss: 2.291478395462036\n",
      "epoch 3 batch 78 loss: 2.549497604370117\n",
      "epoch 3 batch 79 loss: 2.2421419620513916\n",
      "epoch 3 batch 80 loss: 2.4992589950561523\n",
      "epoch 3 batch 81 loss: 2.249469757080078\n",
      "epoch 3 batch 82 loss: 2.517070770263672\n",
      "epoch 3 batch 83 loss: 2.4124770164489746\n",
      "epoch 3 batch 84 loss: 2.1626579761505127\n",
      "epoch 3 batch 85 loss: 2.4388771057128906\n",
      "epoch 3 batch 86 loss: 2.515989303588867\n",
      "epoch 3 batch 87 loss: 2.8775556087493896\n",
      "epoch 3 batch 88 loss: 2.3822295665740967\n",
      "epoch 3 batch 89 loss: 2.400367259979248\n",
      "epoch 3 batch 90 loss: 2.4076461791992188\n",
      "epoch 3 batch 91 loss: 2.608396291732788\n",
      "epoch 3 batch 92 loss: 2.679036855697632\n",
      "epoch 3 batch 93 loss: 2.7719125747680664\n",
      "epoch 3 batch 94 loss: 2.3839778900146484\n",
      "epoch 3 batch 95 loss: 2.3275890350341797\n",
      "epoch 3 batch 96 loss: 2.849954128265381\n",
      "epoch 3 batch 97 loss: 2.1576476097106934\n",
      "epoch 3 batch 98 loss: 2.5293941497802734\n",
      "epoch 3 batch 99 loss: 2.4155502319335938\n",
      "epoch 3 batch 100 loss: 2.6012673377990723\n",
      "epoch 3 batch 101 loss: 2.61716890335083\n",
      "epoch 3 batch 102 loss: 2.3621017932891846\n",
      "epoch 3 batch 103 loss: 2.627307653427124\n",
      "epoch 3 batch 104 loss: 2.3817708492279053\n",
      "epoch 3 batch 105 loss: 2.389962673187256\n",
      "epoch 3 batch 106 loss: 2.3864855766296387\n",
      "epoch 3 batch 107 loss: 2.609992504119873\n",
      "epoch 3 batch 108 loss: 2.3473942279815674\n",
      "epoch 3 batch 109 loss: 2.7431674003601074\n",
      "epoch 3 batch 110 loss: 2.5386643409729004\n",
      "epoch 3 batch 111 loss: 2.4875998497009277\n",
      "epoch 3 batch 112 loss: 2.4721240997314453\n",
      "epoch 3 batch 113 loss: 2.514206647872925\n",
      "epoch 3 batch 114 loss: 2.242053270339966\n",
      "epoch 3 batch 115 loss: 2.5824131965637207\n",
      "epoch 3 batch 116 loss: 2.4364349842071533\n",
      "epoch 3 batch 117 loss: 2.3118700981140137\n",
      "epoch 3 batch 118 loss: 2.4006848335266113\n",
      "epoch 3 batch 119 loss: 2.583010673522949\n",
      "epoch 3 batch 120 loss: 2.4873881340026855\n",
      "epoch 3 batch 121 loss: 2.554238796234131\n",
      "epoch 3 batch 122 loss: 2.299379348754883\n",
      "epoch 3 batch 123 loss: 2.486027717590332\n",
      "epoch 3 batch 124 loss: 2.53495454788208\n",
      "epoch 3 batch 125 loss: 2.4710116386413574\n",
      "epoch 3 batch 126 loss: 2.2792394161224365\n",
      "epoch 3 batch 127 loss: 2.4502038955688477\n",
      "epoch 3 batch 128 loss: 2.5993690490722656\n",
      "epoch 3 batch 129 loss: 2.2952256202697754\n",
      "epoch 3 batch 130 loss: 2.7821455001831055\n",
      "epoch 3 batch 131 loss: 2.3339767456054688\n",
      "epoch 3 batch 132 loss: 2.3416876792907715\n",
      "epoch 3 batch 133 loss: 2.1628081798553467\n",
      "epoch 3 batch 134 loss: 2.4028923511505127\n",
      "epoch 3 batch 135 loss: 2.6089768409729004\n",
      "epoch 3 batch 136 loss: 2.163435935974121\n",
      "epoch 3 batch 137 loss: 2.482635736465454\n",
      "epoch 3 batch 138 loss: 2.469240665435791\n",
      "epoch 3 batch 139 loss: 2.3178210258483887\n",
      "epoch 3 batch 140 loss: 2.341419219970703\n",
      "epoch 3 batch 141 loss: 2.206043243408203\n",
      "epoch 3 batch 142 loss: 2.303851366043091\n",
      "epoch 3 batch 143 loss: 2.570751667022705\n",
      "epoch 3 batch 144 loss: 2.316066265106201\n",
      "epoch 3 batch 145 loss: 2.4750876426696777\n",
      "epoch 3 batch 146 loss: 2.7318387031555176\n",
      "epoch 3 batch 147 loss: 2.1570653915405273\n",
      "epoch 3 batch 148 loss: 2.8865513801574707\n",
      "epoch 3 batch 149 loss: 2.436593770980835\n",
      "epoch 3 batch 150 loss: 2.480078935623169\n",
      "epoch 3 batch 151 loss: 2.3847603797912598\n",
      "epoch 3 batch 152 loss: 2.6934947967529297\n",
      "epoch 3 batch 153 loss: 2.4508512020111084\n",
      "epoch 3 batch 154 loss: 2.5013105869293213\n",
      "epoch 3 batch 155 loss: 2.705976724624634\n",
      "epoch 3 batch 156 loss: 2.502204418182373\n",
      "epoch 3 batch 157 loss: 2.4731898307800293\n",
      "epoch 3 batch 158 loss: 2.480372667312622\n",
      "epoch 3 batch 159 loss: 2.6944947242736816\n",
      "epoch 3 batch 160 loss: 2.2710537910461426\n",
      "epoch 3 batch 161 loss: 2.3954367637634277\n",
      "epoch 3 batch 162 loss: 2.652076244354248\n",
      "epoch 3 batch 163 loss: 2.315554618835449\n",
      "epoch 3 batch 164 loss: 2.5327818393707275\n",
      "epoch 3 batch 165 loss: 2.2134971618652344\n",
      "epoch 3 batch 166 loss: 2.6381337642669678\n",
      "epoch 3 batch 167 loss: 2.5272774696350098\n",
      "epoch 3 batch 168 loss: 2.3547942638397217\n",
      "epoch 3 batch 169 loss: 2.3326544761657715\n",
      "epoch 3 batch 170 loss: 2.7056162357330322\n",
      "epoch 3 batch 171 loss: 2.3374979496002197\n",
      "epoch 3 batch 172 loss: 2.4915285110473633\n",
      "epoch 3 batch 173 loss: 2.2637863159179688\n",
      "epoch 3 batch 174 loss: 2.474059581756592\n",
      "epoch 3 batch 175 loss: 2.7108447551727295\n",
      "epoch 3 batch 176 loss: 2.416292667388916\n",
      "epoch 3 batch 177 loss: 2.5038108825683594\n",
      "epoch 3 batch 178 loss: 2.7182064056396484\n",
      "epoch 3 batch 179 loss: 2.5299625396728516\n",
      "epoch 3 batch 180 loss: 2.256133556365967\n",
      "epoch 3 batch 181 loss: 2.4172210693359375\n",
      "epoch 3 batch 182 loss: 2.7218446731567383\n",
      "epoch 3 batch 183 loss: 2.5929603576660156\n",
      "epoch 3 batch 184 loss: 2.2674264907836914\n",
      "epoch 3 batch 185 loss: 2.3913018703460693\n",
      "epoch 3 batch 186 loss: 2.3896048069000244\n",
      "epoch 3 batch 187 loss: 2.4957034587860107\n",
      "epoch 3 batch 188 loss: 2.304527521133423\n",
      "epoch 3 batch 189 loss: 2.7190747261047363\n",
      "epoch 3 batch 190 loss: 2.382188558578491\n",
      "epoch 3 batch 191 loss: 2.6297607421875\n",
      "epoch 3 batch 192 loss: 2.351689338684082\n",
      "epoch 3 batch 193 loss: 2.559816837310791\n",
      "epoch 3 batch 194 loss: 2.799325942993164\n",
      "epoch 3 batch 195 loss: 2.4703991413116455\n",
      "epoch 3 batch 196 loss: 2.9185454845428467\n",
      "epoch 3 batch 197 loss: 2.3759615421295166\n",
      "epoch 3 batch 198 loss: 2.6643528938293457\n",
      "epoch 3 batch 199 loss: 2.2452805042266846\n",
      "epoch 3 batch 200 loss: 2.161191463470459\n",
      "epoch 3 batch 201 loss: 2.5642006397247314\n",
      "epoch 3 batch 202 loss: 2.379331350326538\n",
      "epoch 3 batch 203 loss: 2.380293846130371\n",
      "epoch 3 batch 204 loss: 2.4318339824676514\n",
      "epoch 3 batch 205 loss: 2.4097747802734375\n",
      "epoch 3 batch 206 loss: 2.502959728240967\n",
      "epoch 3 batch 207 loss: 2.552659273147583\n",
      "epoch 3 batch 208 loss: 2.434467077255249\n",
      "epoch 3 batch 209 loss: 2.7047224044799805\n",
      "epoch 3 batch 210 loss: 2.546766757965088\n",
      "epoch 3 batch 211 loss: 2.7668046951293945\n",
      "epoch 3 batch 212 loss: 2.4792861938476562\n",
      "epoch 3 batch 213 loss: 2.580843448638916\n",
      "epoch 3 batch 214 loss: 2.7548346519470215\n",
      "epoch 3 batch 215 loss: 2.6357836723327637\n",
      "epoch 3 batch 216 loss: 2.454848289489746\n",
      "epoch 3 batch 217 loss: 2.3933262825012207\n",
      "epoch 3 batch 218 loss: 2.518375873565674\n",
      "epoch 3 batch 219 loss: 2.4656946659088135\n",
      "epoch 3 batch 220 loss: 2.2712297439575195\n",
      "epoch 3 batch 221 loss: 2.4660520553588867\n",
      "epoch 3 batch 222 loss: 2.4614078998565674\n",
      "epoch 3 batch 223 loss: 2.460056781768799\n",
      "epoch 3 batch 224 loss: 2.3689205646514893\n",
      "epoch 3 batch 225 loss: 2.2715513706207275\n",
      "epoch 3 batch 226 loss: 2.6816442012786865\n",
      "epoch 3 batch 227 loss: 2.2964582443237305\n",
      "epoch 3 batch 228 loss: 2.568244218826294\n",
      "epoch 3 batch 229 loss: 2.4028875827789307\n",
      "epoch 3 batch 230 loss: 2.4732561111450195\n",
      "epoch 3 batch 231 loss: 2.627763271331787\n",
      "epoch 3 batch 232 loss: 2.418959379196167\n",
      "epoch 3 batch 233 loss: 2.4076030254364014\n",
      "epoch 3 batch 234 loss: 2.521775245666504\n",
      "epoch 3 batch 235 loss: 2.3588309288024902\n",
      "epoch 3 batch 236 loss: 2.590519428253174\n",
      "epoch 3 batch 237 loss: 2.47487735748291\n",
      "epoch 3 batch 238 loss: 2.8598527908325195\n",
      "epoch 3 batch 239 loss: 2.4463307857513428\n",
      "epoch 3 batch 240 loss: 2.5254967212677\n",
      "epoch 3 batch 241 loss: 2.4488015174865723\n",
      "epoch 3 batch 242 loss: 2.490830898284912\n",
      "epoch 3 batch 243 loss: 2.468017578125\n",
      "epoch 3 batch 244 loss: 2.4698588848114014\n",
      "epoch 3 batch 245 loss: 2.2741165161132812\n",
      "epoch 3 batch 246 loss: 2.289088249206543\n",
      "epoch 3 batch 247 loss: 2.4498350620269775\n",
      "epoch 3 batch 248 loss: 2.7159488201141357\n",
      "epoch 3 batch 249 loss: 2.026484966278076\n",
      "epoch 3 batch 250 loss: 2.3123574256896973\n",
      "epoch 3 batch 251 loss: 2.556788444519043\n",
      "epoch 3 batch 252 loss: 2.5086112022399902\n",
      "epoch 3 batch 253 loss: 2.3769209384918213\n",
      "epoch 3 batch 254 loss: 2.634768486022949\n",
      "epoch 3 batch 255 loss: 2.5271077156066895\n",
      "epoch 3 batch 256 loss: 2.6234068870544434\n",
      "epoch 3 batch 257 loss: 2.320554733276367\n",
      "epoch 3 batch 258 loss: 2.3980586528778076\n",
      "epoch 3 batch 259 loss: 2.247765064239502\n",
      "epoch 3 batch 260 loss: 2.448667526245117\n",
      "epoch 3 batch 261 loss: 2.2872745990753174\n",
      "epoch 3 batch 262 loss: 2.605584144592285\n",
      "epoch 3 batch 263 loss: 2.5872325897216797\n",
      "epoch 3 batch 264 loss: 2.587343692779541\n",
      "epoch 3 batch 265 loss: 2.0852649211883545\n",
      "epoch 3 batch 266 loss: 2.5205259323120117\n",
      "epoch 3 batch 267 loss: 2.382680892944336\n",
      "epoch 3 batch 268 loss: 2.6503849029541016\n",
      "epoch 3 batch 269 loss: 2.4833555221557617\n",
      "epoch 3 batch 270 loss: 2.7360644340515137\n",
      "epoch 3 batch 271 loss: 2.517307996749878\n",
      "epoch 3 batch 272 loss: 2.2288990020751953\n",
      "epoch 3 batch 273 loss: 2.608091354370117\n",
      "epoch 3 batch 274 loss: 2.3558363914489746\n",
      "epoch 3 batch 275 loss: 2.5912890434265137\n",
      "epoch 3 batch 276 loss: 2.2525110244750977\n",
      "epoch 3 batch 277 loss: 2.2914814949035645\n",
      "epoch 3 batch 278 loss: 2.405949592590332\n",
      "epoch 3 batch 279 loss: 2.4779112339019775\n",
      "epoch 3 batch 280 loss: 2.3283419609069824\n",
      "epoch 3 batch 281 loss: 2.385033130645752\n",
      "epoch 3 batch 282 loss: 2.430176258087158\n",
      "epoch 3 batch 283 loss: 2.208010196685791\n",
      "epoch 3 batch 284 loss: 2.3949074745178223\n",
      "epoch 3 batch 285 loss: 2.2193593978881836\n",
      "epoch 3 batch 286 loss: 2.319035053253174\n",
      "epoch 3 batch 287 loss: 2.533595085144043\n",
      "epoch 3 batch 288 loss: 2.5007400512695312\n",
      "epoch 3 batch 289 loss: 2.297961473464966\n",
      "epoch 3 batch 290 loss: 2.4813661575317383\n",
      "epoch 3 batch 291 loss: 2.665371894836426\n",
      "epoch 3 batch 292 loss: 2.5721211433410645\n",
      "epoch 3 batch 293 loss: 2.553797721862793\n",
      "epoch 3 batch 294 loss: 2.254448890686035\n",
      "epoch 3 batch 295 loss: 2.469733715057373\n",
      "epoch 3 batch 296 loss: 2.543205499649048\n",
      "epoch 3 batch 297 loss: 2.4624898433685303\n",
      "epoch 3 batch 298 loss: 2.362959384918213\n",
      "epoch 3 batch 299 loss: 2.5255203247070312\n",
      "epoch 3 batch 300 loss: 2.6308116912841797\n",
      "epoch 3 batch 301 loss: 2.6531689167022705\n",
      "epoch 3 batch 302 loss: 2.808332920074463\n",
      "epoch 3 batch 303 loss: 2.2960879802703857\n",
      "epoch 3 batch 304 loss: 2.519233465194702\n",
      "epoch 3 batch 305 loss: 2.280346393585205\n",
      "epoch 3 batch 306 loss: 2.20521879196167\n",
      "epoch 3 batch 307 loss: 2.422431468963623\n",
      "epoch 3 batch 308 loss: 2.438147783279419\n",
      "epoch 3 batch 309 loss: 2.314267873764038\n",
      "epoch 3 batch 310 loss: 2.319042444229126\n",
      "epoch 3 batch 311 loss: 2.3564186096191406\n",
      "epoch 3 batch 312 loss: 2.7088122367858887\n",
      "epoch 3 batch 313 loss: 2.3797354698181152\n",
      "epoch 3 batch 314 loss: 2.6806039810180664\n",
      "epoch 3 batch 315 loss: 2.2741129398345947\n",
      "epoch 3 batch 316 loss: 2.695021152496338\n",
      "epoch 3 batch 317 loss: 2.569089412689209\n",
      "epoch 3 batch 318 loss: 2.4350476264953613\n",
      "epoch 3 batch 319 loss: 2.521121025085449\n",
      "epoch 3 batch 320 loss: 2.2734744548797607\n",
      "epoch 3 batch 321 loss: 2.5986862182617188\n",
      "epoch 3 batch 322 loss: 2.653604507446289\n",
      "epoch 3 batch 323 loss: 2.255098342895508\n",
      "epoch 3 batch 324 loss: 2.344475030899048\n",
      "epoch 3 batch 325 loss: 2.557225227355957\n",
      "epoch 3 batch 326 loss: 2.2141611576080322\n",
      "epoch 3 batch 327 loss: 2.341113567352295\n",
      "epoch 3 batch 328 loss: 2.2787227630615234\n",
      "epoch 3 batch 329 loss: 2.5134119987487793\n",
      "epoch 3 batch 330 loss: 2.422617197036743\n",
      "epoch 3 batch 331 loss: 2.2766072750091553\n",
      "epoch 3 batch 332 loss: 2.3588109016418457\n",
      "epoch 3 batch 333 loss: 2.269127368927002\n",
      "epoch 3 batch 334 loss: 2.5613231658935547\n",
      "epoch 3 batch 335 loss: 2.841517925262451\n",
      "epoch 3 batch 336 loss: 2.544485569000244\n",
      "epoch 3 batch 337 loss: 2.5293970108032227\n",
      "epoch 3 batch 338 loss: 2.3942759037017822\n",
      "epoch 3 batch 339 loss: 2.4131743907928467\n",
      "epoch 3 batch 340 loss: 2.2381253242492676\n",
      "epoch 3 batch 341 loss: 2.5103840827941895\n",
      "epoch 3 batch 342 loss: 2.3252713680267334\n",
      "epoch 3 batch 343 loss: 2.514781951904297\n",
      "epoch 3 batch 344 loss: 2.538280487060547\n",
      "epoch 3 batch 345 loss: 2.300633668899536\n",
      "epoch 3 batch 346 loss: 2.3894853591918945\n",
      "epoch 3 batch 347 loss: 2.2172951698303223\n",
      "epoch 3 batch 348 loss: 2.2593703269958496\n",
      "epoch 3 batch 349 loss: 2.299178123474121\n",
      "epoch 3 batch 350 loss: 2.685213088989258\n",
      "epoch 3 batch 351 loss: 2.8498401641845703\n",
      "epoch 3 batch 352 loss: 2.7348597049713135\n",
      "epoch 3 batch 353 loss: 2.4634764194488525\n",
      "epoch 3 batch 354 loss: 2.1256861686706543\n",
      "epoch 3 batch 355 loss: 2.6358518600463867\n",
      "epoch 3 batch 356 loss: 2.6442065238952637\n",
      "epoch 3 batch 357 loss: 2.1646361351013184\n",
      "epoch 3 batch 358 loss: 2.3332738876342773\n",
      "epoch 3 batch 359 loss: 2.4538159370422363\n",
      "epoch 3 batch 360 loss: 2.3836007118225098\n",
      "epoch 3 batch 361 loss: 2.485031843185425\n",
      "epoch 3 batch 362 loss: 2.575658082962036\n",
      "epoch 3 batch 363 loss: 2.3395304679870605\n",
      "epoch 3 batch 364 loss: 2.261937141418457\n",
      "epoch 3 batch 365 loss: 2.6455612182617188\n",
      "epoch 3 batch 366 loss: 2.457179069519043\n",
      "epoch 3 batch 367 loss: 2.4824957847595215\n",
      "epoch 3 batch 368 loss: 2.533059597015381\n",
      "epoch 3 batch 369 loss: 2.4529614448547363\n",
      "epoch 3 batch 370 loss: 2.5564932823181152\n",
      "epoch 3 batch 371 loss: 2.238438606262207\n",
      "epoch 3 batch 372 loss: 2.364254951477051\n",
      "epoch 3 batch 373 loss: 2.363875150680542\n",
      "epoch 3 batch 374 loss: 2.4171838760375977\n",
      "epoch 3 batch 375 loss: 2.5251526832580566\n",
      "epoch 3 batch 376 loss: 2.260439157485962\n",
      "epoch 3 batch 377 loss: 2.385589599609375\n",
      "epoch 3 batch 378 loss: 2.426631212234497\n",
      "epoch 3 batch 379 loss: 2.3636035919189453\n",
      "epoch 3 batch 380 loss: 2.498605251312256\n",
      "epoch 3 batch 381 loss: 2.4465434551239014\n",
      "epoch 3 batch 382 loss: 2.4668095111846924\n",
      "epoch 3 batch 383 loss: 2.2106258869171143\n",
      "epoch 3 batch 384 loss: 2.1690852642059326\n",
      "epoch 3 batch 385 loss: 2.215540885925293\n",
      "epoch 3 batch 386 loss: 2.27411150932312\n",
      "epoch 3 batch 387 loss: 2.367765426635742\n",
      "epoch 3 batch 388 loss: 2.352816104888916\n",
      "epoch 3 batch 389 loss: 2.657761335372925\n",
      "epoch 3 batch 390 loss: 2.439488410949707\n",
      "epoch 3 batch 391 loss: 2.2355258464813232\n",
      "epoch 3 batch 392 loss: 2.074286460876465\n",
      "epoch 3 batch 393 loss: 2.7481203079223633\n",
      "epoch 3 batch 394 loss: 2.4833977222442627\n",
      "epoch 3 batch 395 loss: 2.712078094482422\n",
      "epoch 3 batch 396 loss: 2.7382936477661133\n",
      "epoch 3 batch 397 loss: 2.368373155593872\n",
      "epoch 3 batch 398 loss: 2.3167731761932373\n",
      "epoch 3 batch 399 loss: 2.546687126159668\n",
      "epoch 3 batch 400 loss: 2.223381996154785\n",
      "epoch 3 batch 401 loss: 2.203193187713623\n",
      "epoch 3 batch 402 loss: 2.355628490447998\n",
      "epoch 3 batch 403 loss: 2.421475648880005\n",
      "epoch 3 batch 404 loss: 2.279125213623047\n",
      "epoch 3 batch 405 loss: 2.4375929832458496\n",
      "epoch 3 batch 406 loss: 2.465942859649658\n",
      "epoch 3 batch 407 loss: 2.741204261779785\n",
      "epoch 3 batch 408 loss: 2.3674230575561523\n",
      "epoch 3 batch 409 loss: 2.3633408546447754\n",
      "epoch 3 batch 410 loss: 2.379117250442505\n",
      "epoch 3 batch 411 loss: 2.5313632488250732\n",
      "epoch 3 batch 412 loss: 2.5393764972686768\n",
      "epoch 3 batch 413 loss: 2.349884271621704\n",
      "epoch 3 batch 414 loss: 2.559713840484619\n",
      "epoch 3 batch 415 loss: 2.0970301628112793\n",
      "epoch 3 batch 416 loss: 2.236176013946533\n",
      "epoch 3 batch 417 loss: 2.244839906692505\n",
      "epoch 3 batch 418 loss: 2.5825085639953613\n",
      "epoch 3 batch 419 loss: 2.5585250854492188\n",
      "epoch 3 batch 420 loss: 2.3279011249542236\n",
      "epoch 3 batch 421 loss: 2.2136473655700684\n",
      "epoch 3 batch 422 loss: 2.2722742557525635\n",
      "epoch 3 batch 423 loss: 2.6134939193725586\n",
      "epoch 3 batch 424 loss: 2.3011274337768555\n",
      "epoch 3 batch 425 loss: 2.251802921295166\n",
      "epoch 3 batch 426 loss: 2.5316433906555176\n",
      "epoch 3 batch 427 loss: 2.42073917388916\n",
      "epoch 3 batch 428 loss: 2.207714319229126\n",
      "epoch 3 batch 429 loss: 2.309535264968872\n",
      "epoch 3 batch 430 loss: 2.456704616546631\n",
      "epoch 3 batch 431 loss: 2.324622631072998\n",
      "epoch 3 batch 432 loss: 2.513683795928955\n",
      "epoch 3 batch 433 loss: 2.399083137512207\n",
      "epoch 3 batch 434 loss: 2.131103277206421\n",
      "epoch 3 batch 435 loss: 2.4234631061553955\n",
      "epoch 3 batch 436 loss: 2.4794058799743652\n",
      "epoch 3 batch 437 loss: 2.7328639030456543\n",
      "epoch 3 batch 438 loss: 2.398120641708374\n",
      "epoch 3 batch 439 loss: 2.566234588623047\n",
      "epoch 3 batch 440 loss: 2.658263921737671\n",
      "epoch 3 batch 441 loss: 2.6653833389282227\n",
      "epoch 3 batch 442 loss: 2.3059117794036865\n",
      "epoch 3 batch 443 loss: 2.337484359741211\n",
      "epoch 3 batch 444 loss: 2.8667094707489014\n",
      "epoch 3 batch 445 loss: 2.4565701484680176\n",
      "epoch 3 batch 446 loss: 2.3016510009765625\n",
      "epoch 3 batch 447 loss: 2.2713053226470947\n",
      "epoch 3 batch 448 loss: 2.3948986530303955\n",
      "epoch 3 batch 449 loss: 2.4786500930786133\n",
      "epoch 3 batch 450 loss: 2.4392588138580322\n",
      "epoch 3 batch 451 loss: 2.258635997772217\n",
      "epoch 3 batch 452 loss: 2.3740928173065186\n",
      "epoch 3 batch 453 loss: 2.2874317169189453\n",
      "epoch 3 batch 454 loss: 2.3066940307617188\n",
      "epoch 3 batch 455 loss: 2.26967191696167\n",
      "epoch 3 batch 456 loss: 2.4539167881011963\n",
      "epoch 3 batch 457 loss: 2.3379998207092285\n",
      "epoch 3 batch 458 loss: 2.17132568359375\n",
      "epoch 3 batch 459 loss: 2.6483147144317627\n",
      "epoch 3 batch 460 loss: 2.5673747062683105\n",
      "epoch 3 batch 461 loss: 2.238823175430298\n",
      "epoch 3 batch 462 loss: 2.347280740737915\n",
      "epoch 3 batch 463 loss: 2.232450008392334\n",
      "epoch 3 batch 464 loss: 2.9402692317962646\n",
      "epoch 3 batch 465 loss: 2.1428351402282715\n",
      "epoch 3 batch 466 loss: 2.326443672180176\n",
      "epoch 3 batch 467 loss: 2.4224443435668945\n",
      "epoch 3 batch 468 loss: 2.416666030883789\n",
      "epoch 3 batch 469 loss: 2.3400626182556152\n",
      "epoch 3 batch 470 loss: 2.254347562789917\n",
      "epoch 3 batch 471 loss: 2.425846576690674\n",
      "epoch 3 batch 472 loss: 2.0970921516418457\n",
      "epoch 3 batch 473 loss: 2.2680726051330566\n",
      "epoch 3 batch 474 loss: 2.5937178134918213\n",
      "epoch 3 batch 475 loss: 2.3317930698394775\n",
      "epoch 3 batch 476 loss: 2.190361499786377\n",
      "epoch 3 batch 477 loss: 2.6389548778533936\n",
      "epoch 3 batch 478 loss: 2.6081717014312744\n",
      "epoch 3 batch 479 loss: 2.46690034866333\n",
      "epoch 3 batch 480 loss: 2.3117613792419434\n",
      "epoch 3 batch 481 loss: 2.3492045402526855\n",
      "epoch 3 batch 482 loss: 2.7820982933044434\n",
      "epoch 3 batch 483 loss: 2.3902668952941895\n",
      "epoch 3 batch 484 loss: 2.4432106018066406\n",
      "epoch 3 batch 485 loss: 2.184288740158081\n",
      "epoch 3 batch 486 loss: 2.800698757171631\n",
      "epoch 3 batch 487 loss: 2.284498929977417\n",
      "epoch 3 batch 488 loss: 2.3013594150543213\n",
      "epoch 3 batch 489 loss: 2.5879063606262207\n",
      "epoch 3 batch 490 loss: 2.4807934761047363\n",
      "epoch 3 batch 491 loss: 2.6219968795776367\n",
      "epoch 3 batch 492 loss: 2.4371728897094727\n",
      "epoch 3 batch 493 loss: 2.586461067199707\n",
      "epoch 3 batch 494 loss: 2.230213165283203\n",
      "epoch 3 batch 495 loss: 2.4330811500549316\n",
      "epoch 3 batch 496 loss: 2.5968658924102783\n",
      "epoch 3 batch 497 loss: 2.6733200550079346\n",
      "epoch 3 batch 498 loss: 2.3089375495910645\n",
      "epoch 3 batch 499 loss: 2.633908987045288\n",
      "epoch 3 batch 500 loss: 2.6489815711975098\n",
      "epoch 3 batch 501 loss: 2.3057665824890137\n",
      "epoch 3 batch 502 loss: 2.564094066619873\n",
      "epoch 3 batch 503 loss: 2.2989494800567627\n",
      "epoch 3 batch 504 loss: 2.4209513664245605\n",
      "epoch 3 batch 505 loss: 2.2523820400238037\n",
      "epoch 3 batch 506 loss: 2.394718885421753\n",
      "epoch 3 batch 507 loss: 2.5983781814575195\n",
      "epoch 3 batch 508 loss: 2.3156843185424805\n",
      "epoch 3 batch 509 loss: 2.370704174041748\n",
      "epoch 3 batch 510 loss: 2.44669246673584\n",
      "epoch 3 batch 511 loss: 2.640502452850342\n",
      "epoch 3 batch 512 loss: 2.712980270385742\n",
      "epoch 3 batch 513 loss: 2.564908504486084\n",
      "epoch 3 batch 514 loss: 2.3434183597564697\n",
      "epoch 3 batch 515 loss: 2.616886854171753\n",
      "epoch 3 batch 516 loss: 2.5488436222076416\n",
      "epoch 3 batch 517 loss: 2.3092432022094727\n",
      "epoch 3 batch 518 loss: 2.129032850265503\n",
      "epoch 3 batch 519 loss: 2.474766969680786\n",
      "epoch 3 batch 520 loss: 2.249922275543213\n",
      "epoch 3 batch 521 loss: 2.5794432163238525\n",
      "epoch 3 batch 522 loss: 2.3669159412384033\n",
      "epoch 3 batch 523 loss: 2.5618739128112793\n",
      "epoch 3 batch 524 loss: 2.2500202655792236\n",
      "epoch 3 batch 525 loss: 2.274672269821167\n",
      "epoch 3 batch 526 loss: 2.622189998626709\n",
      "epoch 3 batch 527 loss: 2.4511160850524902\n",
      "epoch 3 batch 528 loss: 2.487163782119751\n",
      "epoch 3 batch 529 loss: 2.4522619247436523\n",
      "epoch 3 batch 530 loss: 2.7605843544006348\n",
      "epoch 3 batch 531 loss: 2.2808914184570312\n",
      "epoch 3 batch 532 loss: 2.672093391418457\n",
      "epoch 3 batch 533 loss: 2.296203136444092\n",
      "epoch 3 batch 534 loss: 2.288348436355591\n",
      "epoch 3 batch 535 loss: 2.3506550788879395\n",
      "epoch 3 batch 536 loss: 2.531310558319092\n",
      "epoch 3 batch 537 loss: 2.357966661453247\n",
      "epoch 3 batch 538 loss: 2.6029000282287598\n",
      "epoch 3 batch 539 loss: 2.5375142097473145\n",
      "epoch 3 batch 540 loss: 2.5649123191833496\n",
      "epoch 3 batch 541 loss: 2.6079118251800537\n",
      "epoch 3 batch 542 loss: 2.6869730949401855\n",
      "epoch 3 batch 543 loss: 2.4649205207824707\n",
      "epoch 3 batch 544 loss: 2.4735777378082275\n",
      "epoch 3 batch 545 loss: 2.2218520641326904\n",
      "epoch 3 batch 546 loss: 2.463574171066284\n",
      "epoch 3 batch 547 loss: 2.5650978088378906\n",
      "epoch 3 batch 548 loss: 2.425147533416748\n",
      "epoch 3 batch 549 loss: 2.3178365230560303\n",
      "epoch 3 batch 550 loss: 2.1741130352020264\n",
      "epoch 3 batch 551 loss: 2.3655998706817627\n",
      "epoch 3 batch 552 loss: 2.252824068069458\n",
      "epoch 3 batch 553 loss: 2.448288917541504\n",
      "epoch 3 batch 554 loss: 2.2402868270874023\n",
      "epoch 3 batch 555 loss: 2.5221338272094727\n",
      "epoch 3 batch 556 loss: 2.623769998550415\n",
      "epoch 3 batch 557 loss: 2.3353028297424316\n",
      "epoch 3 batch 558 loss: 2.304708480834961\n",
      "epoch 3 batch 559 loss: 2.861651659011841\n",
      "epoch 3 batch 560 loss: 2.368475914001465\n",
      "epoch 3 batch 561 loss: 2.174778938293457\n",
      "epoch 3 batch 562 loss: 2.6384544372558594\n",
      "epoch 3 batch 563 loss: 2.22723650932312\n",
      "epoch 3 batch 564 loss: 2.319516181945801\n",
      "epoch 3 batch 565 loss: 2.3390355110168457\n",
      "epoch 3 batch 566 loss: 2.3626880645751953\n",
      "epoch 3 batch 567 loss: 2.294510841369629\n",
      "epoch 3 batch 568 loss: 2.3907761573791504\n",
      "epoch 3 batch 569 loss: 2.291411876678467\n",
      "epoch 3 batch 570 loss: 2.400939702987671\n",
      "epoch 3 batch 571 loss: 2.469197988510132\n",
      "epoch 3 batch 572 loss: 2.6088056564331055\n",
      "epoch 3 batch 573 loss: 2.5495800971984863\n",
      "epoch 3 batch 574 loss: 2.449449062347412\n",
      "epoch 3 batch 575 loss: 2.3692798614501953\n",
      "epoch 3 batch 576 loss: 2.4412283897399902\n",
      "epoch 3 batch 577 loss: 2.6558656692504883\n",
      "epoch 3 batch 578 loss: 2.531494379043579\n",
      "epoch 3 batch 579 loss: 2.7234625816345215\n",
      "epoch 3 batch 580 loss: 2.5252187252044678\n",
      "epoch 3 batch 581 loss: 2.6041784286499023\n",
      "epoch 3 batch 582 loss: 2.470672607421875\n",
      "epoch 3 batch 583 loss: 2.4555752277374268\n",
      "epoch 3 batch 584 loss: 2.665159225463867\n",
      "epoch 3 batch 585 loss: 2.488400459289551\n",
      "epoch 3 batch 586 loss: 2.40767765045166\n",
      "epoch 3 batch 587 loss: 2.656437873840332\n",
      "epoch 3 batch 588 loss: 2.6967251300811768\n",
      "epoch 3 batch 589 loss: 2.723543643951416\n",
      "epoch 3 batch 590 loss: 2.5023984909057617\n",
      "epoch 3 batch 591 loss: 2.4522533416748047\n",
      "epoch 3 batch 592 loss: 2.1770005226135254\n",
      "epoch 3 batch 593 loss: 2.492248058319092\n",
      "epoch 3 batch 594 loss: 2.3555612564086914\n",
      "epoch 3 batch 595 loss: 2.4494147300720215\n",
      "epoch 3 batch 596 loss: 2.4057812690734863\n",
      "epoch 3 batch 597 loss: 2.396094799041748\n",
      "epoch 3 batch 598 loss: 2.2826571464538574\n",
      "epoch 3 batch 599 loss: 2.57938289642334\n",
      "epoch 3 batch 600 loss: 2.297363758087158\n",
      "epoch 3 batch 601 loss: 2.4265918731689453\n",
      "epoch 3 batch 602 loss: 2.354640007019043\n",
      "epoch 3 batch 603 loss: 2.353346347808838\n",
      "epoch 3 batch 604 loss: 2.051730155944824\n",
      "epoch 3 batch 605 loss: 2.4177937507629395\n",
      "epoch 3 batch 606 loss: 2.7332210540771484\n",
      "epoch 3 batch 607 loss: 2.1542439460754395\n",
      "epoch 3 batch 608 loss: 2.2300586700439453\n",
      "epoch 3 batch 609 loss: 2.503404140472412\n",
      "epoch 3 batch 610 loss: 2.188988208770752\n",
      "epoch 3 batch 611 loss: 2.3953919410705566\n",
      "epoch 3 batch 612 loss: 2.377877712249756\n",
      "epoch 3 batch 613 loss: 2.402916431427002\n",
      "epoch 3 batch 614 loss: 2.3753662109375\n",
      "epoch 3 batch 615 loss: 2.2974753379821777\n",
      "epoch 3 batch 616 loss: 2.5876755714416504\n",
      "epoch 3 batch 617 loss: 2.5425565242767334\n",
      "epoch 3 batch 618 loss: 2.471846580505371\n",
      "epoch 3 batch 619 loss: 2.6968626976013184\n",
      "epoch 3 batch 620 loss: 2.65881085395813\n",
      "epoch 3 batch 621 loss: 2.801928758621216\n",
      "epoch 3 batch 622 loss: 2.352552890777588\n",
      "epoch 3 batch 623 loss: 2.3619942665100098\n",
      "epoch 3 batch 624 loss: 2.245731830596924\n",
      "epoch 3 batch 625 loss: 2.3687429428100586\n",
      "epoch 3 batch 626 loss: 2.502533435821533\n",
      "epoch 3 batch 627 loss: 2.247235059738159\n",
      "epoch 3 batch 628 loss: 2.7713775634765625\n",
      "epoch 3 batch 629 loss: 2.8219189643859863\n",
      "epoch 3 batch 630 loss: 2.7101364135742188\n",
      "epoch 3 batch 631 loss: 2.5649962425231934\n",
      "epoch 3 batch 632 loss: 2.484153985977173\n",
      "epoch 3 batch 633 loss: 2.385077476501465\n",
      "epoch 3 batch 634 loss: 2.3855056762695312\n",
      "epoch 3 batch 635 loss: 2.4562196731567383\n",
      "epoch 3 batch 636 loss: 2.422589063644409\n",
      "epoch 3 batch 637 loss: 2.3912343978881836\n",
      "epoch 3 batch 638 loss: 2.4959545135498047\n",
      "epoch 3 batch 639 loss: 2.2534737586975098\n",
      "epoch 3 batch 640 loss: 2.3366622924804688\n",
      "epoch 3 batch 641 loss: 2.713670492172241\n",
      "epoch 3 batch 642 loss: 2.5237250328063965\n",
      "epoch 3 batch 643 loss: 2.2940425872802734\n",
      "epoch 3 batch 644 loss: 2.2981491088867188\n",
      "epoch 3 batch 645 loss: 2.6129672527313232\n",
      "epoch 3 batch 646 loss: 2.4652438163757324\n",
      "epoch 3 batch 647 loss: 2.4372568130493164\n",
      "epoch 3 batch 648 loss: 2.5159950256347656\n",
      "epoch 3 batch 649 loss: 2.409402847290039\n",
      "epoch 3 batch 650 loss: 2.3216166496276855\n",
      "epoch 3 batch 651 loss: 2.5456044673919678\n",
      "epoch 3 batch 652 loss: 2.672327995300293\n",
      "epoch 3 batch 653 loss: 2.4655842781066895\n",
      "epoch 3 batch 654 loss: 2.374204635620117\n",
      "epoch 3 batch 655 loss: 2.453610420227051\n",
      "epoch 3 batch 656 loss: 2.5818464756011963\n",
      "epoch 3 batch 657 loss: 2.4085161685943604\n",
      "epoch 3 batch 658 loss: 2.3485512733459473\n",
      "epoch 3 batch 659 loss: 2.320101737976074\n",
      "epoch 3 batch 660 loss: 2.3544647693634033\n",
      "epoch 3 batch 661 loss: 2.655759811401367\n",
      "epoch 3 batch 662 loss: 2.483461856842041\n",
      "epoch 3 batch 663 loss: 2.372948408126831\n",
      "epoch 3 batch 664 loss: 2.237787961959839\n",
      "epoch 3 batch 665 loss: 2.416276693344116\n",
      "epoch 3 batch 666 loss: 2.178302049636841\n",
      "epoch 3 batch 667 loss: 2.2851722240448\n",
      "epoch 3 batch 668 loss: 2.3015003204345703\n",
      "epoch 3 batch 669 loss: 2.4343810081481934\n",
      "epoch 3 batch 670 loss: 2.3065385818481445\n",
      "epoch 3 batch 671 loss: 2.3993563652038574\n",
      "epoch 3 batch 672 loss: 2.6993582248687744\n",
      "epoch 3 batch 673 loss: 2.435743808746338\n",
      "epoch 3 batch 674 loss: 2.4793612957000732\n",
      "epoch 3 batch 675 loss: 2.6904454231262207\n",
      "epoch 3 batch 676 loss: 2.5319244861602783\n",
      "epoch 3 batch 677 loss: 2.428469657897949\n",
      "epoch 3 batch 678 loss: 2.569878578186035\n",
      "epoch 3 batch 679 loss: 2.2989907264709473\n",
      "epoch 3 batch 680 loss: 2.65556001663208\n",
      "epoch 3 batch 681 loss: 2.636826992034912\n",
      "epoch 3 batch 682 loss: 2.4769444465637207\n",
      "epoch 3 batch 683 loss: 2.7076780796051025\n",
      "epoch 3 batch 684 loss: 2.280275583267212\n",
      "epoch 3 batch 685 loss: 2.696221351623535\n",
      "epoch 3 batch 686 loss: 2.3162951469421387\n",
      "epoch 3 batch 687 loss: 2.359100341796875\n",
      "epoch 3 batch 688 loss: 2.595789909362793\n",
      "epoch 3 batch 689 loss: 2.570169448852539\n",
      "epoch 3 batch 690 loss: 2.668792724609375\n",
      "epoch 3 batch 691 loss: 2.832764148712158\n",
      "epoch 3 batch 692 loss: 2.63879132270813\n",
      "epoch 3 batch 693 loss: 2.51599383354187\n",
      "epoch 3 batch 694 loss: 2.099616050720215\n",
      "epoch 3 batch 695 loss: 2.5967960357666016\n",
      "epoch 3 batch 696 loss: 2.2301745414733887\n",
      "epoch 3 batch 697 loss: 2.2866926193237305\n",
      "epoch 3 batch 698 loss: 2.445774555206299\n",
      "epoch 3 batch 699 loss: 2.469893217086792\n",
      "epoch 3 batch 700 loss: 2.1333868503570557\n",
      "epoch 3 batch 701 loss: 2.414206027984619\n",
      "epoch 3 batch 702 loss: 2.7485947608947754\n",
      "epoch 3 batch 703 loss: 2.3723678588867188\n",
      "epoch 3 batch 704 loss: 2.47067928314209\n",
      "epoch 3 batch 705 loss: 2.386535167694092\n",
      "epoch 3 batch 706 loss: 2.2664921283721924\n",
      "epoch 3 batch 707 loss: 2.5053951740264893\n",
      "epoch 3 batch 708 loss: 2.5374648571014404\n",
      "epoch 3 batch 709 loss: 2.3641481399536133\n",
      "epoch 3 batch 710 loss: 2.669935464859009\n",
      "epoch 3 batch 711 loss: 2.6606967449188232\n",
      "epoch 3 batch 712 loss: 2.493438720703125\n",
      "epoch 3 batch 713 loss: 2.9137511253356934\n",
      "epoch 3 batch 714 loss: 2.589650869369507\n",
      "epoch 3 batch 715 loss: 2.3618791103363037\n",
      "epoch 3 batch 716 loss: 2.4202704429626465\n",
      "epoch 3 batch 717 loss: 2.8635518550872803\n",
      "epoch 3 batch 718 loss: 2.374143123626709\n",
      "epoch 3 batch 719 loss: 2.725411891937256\n",
      "epoch 3 batch 720 loss: 2.520303726196289\n",
      "epoch 3 batch 721 loss: 2.2729456424713135\n",
      "epoch 3 batch 722 loss: 2.3725664615631104\n",
      "epoch 3 batch 723 loss: 2.724109649658203\n",
      "epoch 3 batch 724 loss: 2.540191650390625\n",
      "epoch 3 batch 725 loss: 2.381394386291504\n",
      "epoch 3 batch 726 loss: 2.688045024871826\n",
      "epoch 3 batch 727 loss: 2.625990390777588\n",
      "epoch 3 batch 728 loss: 2.7416067123413086\n",
      "epoch 3 batch 729 loss: 2.418870687484741\n",
      "epoch 3 batch 730 loss: 2.506880283355713\n",
      "epoch 3 batch 731 loss: 2.6415114402770996\n",
      "epoch 3 batch 732 loss: 2.3946118354797363\n",
      "epoch 3 batch 733 loss: 2.574007034301758\n",
      "epoch 3 batch 734 loss: 2.5039215087890625\n",
      "epoch 3 batch 735 loss: 2.417255401611328\n",
      "epoch 3 batch 736 loss: 2.3292932510375977\n",
      "epoch 3 batch 737 loss: 2.411640167236328\n",
      "epoch 3 batch 738 loss: 2.4954848289489746\n",
      "epoch 3 batch 739 loss: 2.4354047775268555\n",
      "epoch 3 batch 740 loss: 2.268660545349121\n",
      "epoch 3 batch 741 loss: 2.705381393432617\n",
      "epoch 3 batch 742 loss: 2.5043869018554688\n",
      "epoch 3 batch 743 loss: 2.4789013862609863\n",
      "epoch 3 batch 744 loss: 2.256683588027954\n",
      "epoch 3 batch 745 loss: 2.336620807647705\n",
      "epoch 3 batch 746 loss: 2.6402158737182617\n",
      "epoch 3 batch 747 loss: 2.5024962425231934\n",
      "epoch 3 batch 748 loss: 2.564178466796875\n",
      "epoch 3 batch 749 loss: 2.623526096343994\n",
      "epoch 3 batch 750 loss: 2.3518588542938232\n",
      "epoch 3 batch 751 loss: 2.207685947418213\n",
      "epoch 3 batch 752 loss: 2.594379425048828\n",
      "epoch 3 batch 753 loss: 2.5082926750183105\n",
      "epoch 3 batch 754 loss: 2.5546555519104004\n",
      "epoch 3 batch 755 loss: 2.5684642791748047\n",
      "epoch 3 batch 756 loss: 2.3866183757781982\n",
      "epoch 3 batch 757 loss: 2.483415126800537\n",
      "epoch 3 batch 758 loss: 2.6415443420410156\n",
      "epoch 3 batch 759 loss: 2.3548879623413086\n",
      "epoch 3 batch 760 loss: 2.4763967990875244\n",
      "epoch 3 batch 761 loss: 2.5406999588012695\n",
      "epoch 3 batch 762 loss: 2.3938846588134766\n",
      "epoch 3 batch 763 loss: 2.4770655632019043\n",
      "epoch 3 batch 764 loss: 2.521461248397827\n",
      "epoch 3 batch 765 loss: 2.535750389099121\n",
      "epoch 3 batch 766 loss: 2.3656373023986816\n",
      "epoch 3 batch 767 loss: 2.681825637817383\n",
      "epoch 3 batch 768 loss: 2.349153518676758\n",
      "epoch 3 batch 769 loss: 2.618887424468994\n",
      "epoch 3 batch 770 loss: 2.2525739669799805\n",
      "epoch 3 batch 771 loss: 2.425734043121338\n",
      "epoch 3 batch 772 loss: 2.560802698135376\n",
      "epoch 3 batch 773 loss: 2.4782490730285645\n",
      "epoch 3 batch 774 loss: 2.280494213104248\n",
      "epoch 3 batch 775 loss: 2.417820930480957\n",
      "epoch 3 batch 776 loss: 2.448031187057495\n",
      "epoch 3 batch 777 loss: 2.7872135639190674\n",
      "epoch 3 batch 778 loss: 2.564558506011963\n",
      "epoch 3 batch 779 loss: 2.7213215827941895\n",
      "epoch 3 batch 780 loss: 2.15555477142334\n",
      "epoch 3 batch 781 loss: 2.6388113498687744\n",
      "epoch 3 batch 782 loss: 2.565314531326294\n",
      "epoch 3 batch 783 loss: 2.3421502113342285\n",
      "epoch 3 batch 784 loss: 2.7182607650756836\n",
      "epoch 3 batch 785 loss: 2.6054883003234863\n",
      "epoch 3 batch 786 loss: 2.500645875930786\n",
      "epoch 3 batch 787 loss: 2.5693421363830566\n",
      "epoch 3 batch 788 loss: 2.5264065265655518\n",
      "epoch 3 batch 789 loss: 2.6626577377319336\n",
      "epoch 3 batch 790 loss: 2.453695774078369\n",
      "epoch 3 batch 791 loss: 2.450366973876953\n",
      "epoch 3 batch 792 loss: 2.352090835571289\n",
      "epoch 3 batch 793 loss: 2.3906288146972656\n",
      "epoch 3 batch 794 loss: 2.732820987701416\n",
      "epoch 3 batch 795 loss: 2.303389072418213\n",
      "epoch 3 batch 796 loss: 2.26961088180542\n",
      "epoch 3 batch 797 loss: 2.5639431476593018\n",
      "epoch 3 batch 798 loss: 2.3078718185424805\n",
      "epoch 3 batch 799 loss: 2.511556625366211\n",
      "epoch 3 batch 800 loss: 2.6158816814422607\n",
      "epoch 3 batch 801 loss: 2.3140640258789062\n",
      "epoch 3 batch 802 loss: 2.4558911323547363\n",
      "epoch 3 batch 803 loss: 2.590496301651001\n",
      "epoch 3 batch 804 loss: 2.637115478515625\n",
      "epoch 3 batch 805 loss: 2.383362054824829\n",
      "epoch 3 batch 806 loss: 2.4806370735168457\n",
      "epoch 3 batch 807 loss: 2.4601950645446777\n",
      "epoch 3 batch 808 loss: 2.51442289352417\n",
      "epoch 3 batch 809 loss: 2.4088294506073\n",
      "epoch 3 batch 810 loss: 2.1311821937561035\n",
      "epoch 3 batch 811 loss: 2.493196487426758\n",
      "epoch 3 batch 812 loss: 2.411266326904297\n",
      "epoch 3 batch 813 loss: 2.5362353324890137\n",
      "epoch 3 batch 814 loss: 2.185657024383545\n",
      "epoch 3 batch 815 loss: 2.5442068576812744\n",
      "epoch 3 batch 816 loss: 2.3033483028411865\n",
      "epoch 3 batch 817 loss: 2.294856071472168\n",
      "epoch 3 batch 818 loss: 2.427051067352295\n",
      "epoch 3 batch 819 loss: 2.5237624645233154\n",
      "epoch 3 batch 820 loss: 2.317070960998535\n",
      "epoch 3 batch 821 loss: 2.328056812286377\n",
      "epoch 3 batch 822 loss: 2.2343900203704834\n",
      "epoch 3 batch 823 loss: 2.3657658100128174\n",
      "epoch 3 batch 824 loss: 3.0304207801818848\n",
      "epoch 3 batch 825 loss: 2.4850549697875977\n",
      "epoch 3 batch 826 loss: 2.480938673019409\n",
      "epoch 3 batch 827 loss: 2.4534125328063965\n",
      "epoch 3 batch 828 loss: 2.3882789611816406\n",
      "epoch 3 batch 829 loss: 2.3037712574005127\n",
      "epoch 3 batch 830 loss: 2.696803569793701\n",
      "epoch 3 batch 831 loss: 2.3784565925598145\n",
      "epoch 3 batch 832 loss: 2.5663580894470215\n",
      "epoch 3 batch 833 loss: 2.603518009185791\n",
      "epoch 3 batch 834 loss: 2.639516592025757\n",
      "epoch 3 batch 835 loss: 2.369715690612793\n",
      "epoch 3 batch 836 loss: 2.5633208751678467\n",
      "epoch 3 batch 837 loss: 2.5488088130950928\n",
      "epoch 3 batch 838 loss: 2.526803493499756\n",
      "epoch 3 batch 839 loss: 2.4129185676574707\n",
      "epoch 3 batch 840 loss: 2.2445123195648193\n",
      "epoch 3 batch 841 loss: 2.2819130420684814\n",
      "epoch 3 batch 842 loss: 2.4645092487335205\n",
      "epoch 3 batch 843 loss: 2.4277095794677734\n",
      "epoch 3 batch 844 loss: 2.240126132965088\n",
      "epoch 3 batch 845 loss: 2.3861513137817383\n",
      "epoch 3 batch 846 loss: 2.3552756309509277\n",
      "epoch 3 batch 847 loss: 2.7699873447418213\n",
      "epoch 3 batch 848 loss: 2.448373317718506\n",
      "epoch 3 batch 849 loss: 2.3166310787200928\n",
      "epoch 3 batch 850 loss: 2.512563705444336\n",
      "epoch 3 batch 851 loss: 2.375580310821533\n",
      "epoch 3 batch 852 loss: 2.330101728439331\n",
      "epoch 3 batch 853 loss: 2.374436855316162\n",
      "epoch 3 batch 854 loss: 2.6980161666870117\n",
      "epoch 3 batch 855 loss: 2.5024969577789307\n",
      "epoch 3 batch 856 loss: 2.3009698390960693\n",
      "epoch 3 batch 857 loss: 2.4171247482299805\n",
      "epoch 3 batch 858 loss: 2.4301626682281494\n",
      "epoch 3 batch 859 loss: 2.2010583877563477\n",
      "epoch 3 batch 860 loss: 2.4086999893188477\n",
      "epoch 3 batch 861 loss: 2.3522753715515137\n",
      "epoch 3 batch 862 loss: 2.2876646518707275\n",
      "epoch 3 batch 863 loss: 2.56113600730896\n",
      "epoch 3 batch 864 loss: 2.5753607749938965\n",
      "epoch 3 batch 865 loss: 2.5535101890563965\n",
      "epoch 3 batch 866 loss: 2.535369396209717\n",
      "epoch 3 batch 867 loss: 2.4980010986328125\n",
      "epoch 3 batch 868 loss: 2.3638219833374023\n",
      "epoch 3 batch 869 loss: 2.6259591579437256\n",
      "epoch 3 batch 870 loss: 2.280102014541626\n",
      "epoch 3 batch 871 loss: 2.4905242919921875\n",
      "epoch 3 batch 872 loss: 2.4492385387420654\n",
      "epoch 3 batch 873 loss: 2.280287265777588\n",
      "epoch 3 batch 874 loss: 2.413837432861328\n",
      "epoch 3 batch 875 loss: 2.327519416809082\n",
      "epoch 3 batch 876 loss: 2.2922751903533936\n",
      "epoch 3 batch 877 loss: 2.3218677043914795\n",
      "epoch 3 batch 878 loss: 2.4775846004486084\n",
      "epoch 3 batch 879 loss: 2.469656229019165\n",
      "epoch 3 batch 880 loss: 2.732166051864624\n",
      "epoch 3 batch 881 loss: 2.330822229385376\n",
      "epoch 3 batch 882 loss: 2.348541498184204\n",
      "epoch 3 batch 883 loss: 2.3201863765716553\n",
      "epoch 3 batch 884 loss: 2.803035259246826\n",
      "epoch 3 batch 885 loss: 2.3325142860412598\n",
      "epoch 3 batch 886 loss: 2.636507987976074\n",
      "epoch 3 batch 887 loss: 2.445167064666748\n",
      "epoch 3 batch 888 loss: 2.355588436126709\n",
      "epoch 3 batch 889 loss: 2.17629075050354\n",
      "epoch 3 batch 890 loss: 2.4421849250793457\n",
      "epoch 3 batch 891 loss: 2.2383553981781006\n",
      "epoch 3 batch 892 loss: 2.1996397972106934\n",
      "epoch 3 batch 893 loss: 2.256495714187622\n",
      "epoch 3 batch 894 loss: 2.646662473678589\n",
      "epoch 3 batch 895 loss: 2.157121181488037\n",
      "epoch 3 batch 896 loss: 2.3456735610961914\n",
      "epoch 3 batch 897 loss: 2.4655253887176514\n",
      "epoch 3 batch 898 loss: 2.305237054824829\n",
      "epoch 3 batch 899 loss: 2.3613343238830566\n",
      "epoch 3 batch 900 loss: 2.3519487380981445\n",
      "epoch 3 batch 901 loss: 2.5985934734344482\n",
      "epoch 3 batch 902 loss: 2.5512948036193848\n",
      "epoch 3 batch 903 loss: 2.362426519393921\n",
      "epoch 3 batch 904 loss: 2.358041286468506\n",
      "epoch 3 batch 905 loss: 2.5426154136657715\n",
      "epoch 3 batch 906 loss: 2.4519858360290527\n",
      "epoch 3 batch 907 loss: 2.639909267425537\n",
      "epoch 3 batch 908 loss: 2.371983289718628\n",
      "epoch 3 batch 909 loss: 2.428943157196045\n",
      "epoch 3 batch 910 loss: 2.3233695030212402\n",
      "epoch 3 batch 911 loss: 2.3141703605651855\n",
      "epoch 3 batch 912 loss: 2.5352110862731934\n",
      "epoch 3 batch 913 loss: 2.519451141357422\n",
      "epoch 3 batch 914 loss: 2.2134876251220703\n",
      "epoch 3 batch 915 loss: 2.3189139366149902\n",
      "epoch 3 batch 916 loss: 2.353395938873291\n",
      "epoch 3 batch 917 loss: 2.21829891204834\n",
      "epoch 3 batch 918 loss: 2.454258918762207\n",
      "epoch 3 batch 919 loss: 2.3402814865112305\n",
      "epoch 3 batch 920 loss: 2.21540904045105\n",
      "epoch 3 batch 921 loss: 2.9417290687561035\n",
      "epoch 3 batch 922 loss: 2.315220594406128\n",
      "epoch 3 batch 923 loss: 2.2656102180480957\n",
      "epoch 3 batch 924 loss: 2.3393194675445557\n",
      "epoch 3 batch 925 loss: 2.376953363418579\n",
      "epoch 3 batch 926 loss: 2.3769283294677734\n",
      "epoch 3 batch 927 loss: 2.4838390350341797\n",
      "epoch 3 batch 928 loss: 2.828278064727783\n",
      "epoch 3 batch 929 loss: 2.547930955886841\n",
      "epoch 3 batch 930 loss: 2.398193359375\n",
      "epoch 3 batch 931 loss: 2.344050407409668\n",
      "epoch 3 batch 932 loss: 2.361078977584839\n",
      "epoch 3 batch 933 loss: 2.4059934616088867\n",
      "epoch 3 batch 934 loss: 2.554413318634033\n",
      "epoch 3 batch 935 loss: 2.3934988975524902\n",
      "epoch 3 batch 936 loss: 2.442709445953369\n",
      "epoch 3 batch 937 loss: 2.112816333770752\n",
      "epoch 3 batch 938 loss: 2.395587205886841\n",
      "epoch 3 batch 939 loss: 2.5365233421325684\n",
      "epoch 3 batch 940 loss: 2.507129669189453\n",
      "epoch 3 batch 941 loss: 2.256653308868408\n",
      "epoch 3 batch 942 loss: 2.574523687362671\n",
      "epoch 3 batch 943 loss: 2.2302987575531006\n",
      "epoch 3 batch 944 loss: 2.2518978118896484\n",
      "epoch 3 batch 945 loss: 2.6133370399475098\n",
      "epoch 3 batch 946 loss: 2.586796760559082\n",
      "epoch 3 batch 947 loss: 2.3569939136505127\n",
      "epoch 3 batch 948 loss: 2.8049721717834473\n",
      "epoch 3 batch 949 loss: 2.5464391708374023\n",
      "epoch 3 batch 950 loss: 2.5017688274383545\n",
      "epoch 3 batch 951 loss: 2.4989376068115234\n",
      "epoch 3 batch 952 loss: 2.587836742401123\n",
      "epoch 3 batch 953 loss: 2.4981348514556885\n",
      "epoch 3 batch 954 loss: 2.2324135303497314\n",
      "epoch 3 batch 955 loss: 2.670074939727783\n",
      "epoch 3 batch 956 loss: 2.526183605194092\n",
      "epoch 3 batch 957 loss: 2.1692819595336914\n",
      "epoch 3 batch 958 loss: 2.358541488647461\n",
      "epoch 3 batch 959 loss: 2.259042978286743\n",
      "epoch 3 batch 960 loss: 2.4351322650909424\n",
      "epoch 3 batch 961 loss: 2.2718987464904785\n",
      "epoch 3 batch 962 loss: 2.415339946746826\n",
      "epoch 3 batch 963 loss: 2.26725697517395\n",
      "epoch 3 batch 964 loss: 2.3093621730804443\n",
      "epoch 3 batch 965 loss: 2.4467859268188477\n",
      "epoch 3 batch 966 loss: 2.2911970615386963\n",
      "epoch 3 batch 967 loss: 2.572770118713379\n",
      "epoch 3 batch 968 loss: 2.8096437454223633\n",
      "epoch 3 batch 969 loss: 2.4093523025512695\n",
      "epoch 3 batch 970 loss: 2.2181994915008545\n",
      "epoch 3 batch 971 loss: 2.5781314373016357\n",
      "epoch 3 batch 972 loss: 2.4189796447753906\n",
      "epoch 3 batch 973 loss: 2.4081177711486816\n",
      "epoch 3 batch 974 loss: 2.7769627571105957\n",
      "epoch 3 batch 975 loss: 2.5117440223693848\n",
      "epoch 3 batch 976 loss: 2.2434098720550537\n",
      "epoch 3 batch 977 loss: 2.6208324432373047\n",
      "epoch 3 batch 978 loss: 2.5194544792175293\n",
      "epoch 3 batch 979 loss: 2.4610486030578613\n",
      "epoch 3 batch 980 loss: 2.5176048278808594\n",
      "epoch 3 batch 981 loss: 2.2066879272460938\n",
      "epoch 3 batch 982 loss: 2.2535548210144043\n",
      "epoch 3 batch 983 loss: 2.5184311866760254\n",
      "epoch 3 batch 984 loss: 2.44987416267395\n",
      "epoch 3 batch 985 loss: 2.427495002746582\n",
      "epoch 3 batch 986 loss: 2.7469921112060547\n",
      "epoch 3 batch 987 loss: 2.2114572525024414\n",
      "epoch 3 batch 988 loss: 2.191929340362549\n",
      "epoch 3 batch 989 loss: 2.685049057006836\n",
      "epoch 3 batch 990 loss: 2.4645886421203613\n",
      "epoch 3 batch 991 loss: 2.3390626907348633\n",
      "epoch 3 batch 992 loss: 2.622908592224121\n",
      "epoch 3 batch 993 loss: 2.41642427444458\n",
      "epoch 3 batch 994 loss: 2.2787904739379883\n",
      "epoch 3 batch 995 loss: 2.223395347595215\n",
      "epoch 3 batch 996 loss: 2.6498546600341797\n",
      "epoch 3 batch 997 loss: 2.1028778553009033\n",
      "epoch 3 batch 998 loss: 2.5135421752929688\n",
      "epoch 3 batch 999 loss: 2.482869863510132\n",
      "epoch 3 batch 1000 loss: 2.4617717266082764\n",
      "epoch 3 batch 1001 loss: 2.627068042755127\n",
      "epoch 3 batch 1002 loss: 2.3694474697113037\n",
      "epoch 3 batch 1003 loss: 2.735417366027832\n",
      "epoch 3 batch 1004 loss: 2.193240165710449\n",
      "epoch 3 batch 1005 loss: 2.3999247550964355\n",
      "epoch 3 batch 1006 loss: 2.610854148864746\n",
      "epoch 3 batch 1007 loss: 2.1215648651123047\n",
      "epoch 3 batch 1008 loss: 2.531881332397461\n",
      "epoch 3 batch 1009 loss: 2.416795492172241\n",
      "epoch 3 batch 1010 loss: 2.423953056335449\n",
      "epoch 3 batch 1011 loss: 2.3744635581970215\n",
      "epoch 3 batch 1012 loss: 2.3817853927612305\n",
      "epoch 3 batch 1013 loss: 2.519451141357422\n",
      "epoch 3 batch 1014 loss: 2.3598716259002686\n",
      "epoch 3 batch 1015 loss: 2.5480008125305176\n",
      "epoch 3 batch 1016 loss: 2.501485824584961\n",
      "epoch 3 batch 1017 loss: 2.4168362617492676\n",
      "epoch 3 batch 1018 loss: 2.3814237117767334\n",
      "epoch 3 batch 1019 loss: 2.460265636444092\n",
      "epoch 3 batch 1020 loss: 2.677420139312744\n",
      "epoch 3 batch 1021 loss: 2.4512758255004883\n",
      "epoch 3 batch 1022 loss: 2.4970686435699463\n",
      "epoch 3 batch 1023 loss: 2.2577567100524902\n",
      "epoch 3 batch 1024 loss: 2.2490930557250977\n",
      "epoch 3 batch 1025 loss: 2.2558753490448\n",
      "epoch 3 batch 1026 loss: 2.295711040496826\n",
      "epoch 3 batch 1027 loss: 2.367309093475342\n",
      "epoch 3 batch 1028 loss: 2.4002933502197266\n",
      "epoch 3 batch 1029 loss: 2.449402332305908\n",
      "epoch 3 batch 1030 loss: 2.480196475982666\n",
      "epoch 3 batch 1031 loss: 2.4672060012817383\n",
      "epoch 3 batch 1032 loss: 2.4212403297424316\n",
      "epoch 3 batch 1033 loss: 2.2632126808166504\n",
      "epoch 3 batch 1034 loss: 2.3159677982330322\n",
      "epoch 3 batch 1035 loss: 2.2447071075439453\n",
      "epoch 3 batch 1036 loss: 2.6106958389282227\n",
      "epoch 3 batch 1037 loss: 2.7799410820007324\n",
      "epoch 3 batch 1038 loss: 2.2737576961517334\n",
      "epoch 3 batch 1039 loss: 2.524951934814453\n",
      "epoch 3 batch 1040 loss: 2.2426722049713135\n",
      "epoch 3 batch 1041 loss: 2.3557827472686768\n",
      "epoch 3 batch 1042 loss: 2.4323058128356934\n",
      "epoch 3 batch 1043 loss: 2.3284056186676025\n",
      "epoch 3 batch 1044 loss: 2.579789638519287\n",
      "epoch 3 batch 1045 loss: 2.665806531906128\n",
      "epoch 3 batch 1046 loss: 2.6688499450683594\n",
      "epoch 3 batch 1047 loss: 2.4784631729125977\n",
      "epoch 3 batch 1048 loss: 2.298555374145508\n",
      "epoch 3 batch 1049 loss: 2.242952346801758\n",
      "epoch 3 batch 1050 loss: 2.706127882003784\n",
      "epoch 3 batch 1051 loss: 2.7358627319335938\n",
      "epoch 3 batch 1052 loss: 2.4224348068237305\n",
      "epoch 3 batch 1053 loss: 2.3590707778930664\n",
      "epoch 3 batch 1054 loss: 2.610598087310791\n",
      "epoch 3 batch 1055 loss: 2.4086732864379883\n",
      "epoch 3 batch 1056 loss: 2.3424291610717773\n",
      "epoch 3 batch 1057 loss: 2.1360979080200195\n",
      "epoch 3 batch 1058 loss: 2.199186325073242\n",
      "epoch 3 batch 1059 loss: 2.401700019836426\n",
      "epoch 3 batch 1060 loss: 2.3122057914733887\n",
      "epoch 3 batch 1061 loss: 2.683795690536499\n",
      "epoch 3 batch 1062 loss: 2.5070574283599854\n",
      "epoch 3 batch 1063 loss: 2.3462581634521484\n",
      "epoch 3 batch 1064 loss: 2.379509449005127\n",
      "epoch 3 batch 1065 loss: 2.662949323654175\n",
      "epoch 3 batch 1066 loss: 2.3209292888641357\n",
      "epoch 3 batch 1067 loss: 2.3433079719543457\n",
      "epoch 3 batch 1068 loss: 2.409914016723633\n",
      "epoch 3 batch 1069 loss: 2.4118704795837402\n",
      "epoch 3 batch 1070 loss: 2.3824782371520996\n",
      "epoch 3 batch 1071 loss: 2.316774368286133\n",
      "epoch 3 batch 1072 loss: 2.1286473274230957\n",
      "epoch 3 batch 1073 loss: 2.495516777038574\n",
      "epoch 3 batch 1074 loss: 2.323437213897705\n",
      "epoch 3 batch 1075 loss: 2.3530640602111816\n",
      "epoch 3 batch 1076 loss: 2.352900981903076\n",
      "epoch 3 batch 1077 loss: 2.072981834411621\n",
      "epoch 3 batch 1078 loss: 2.129762887954712\n",
      "epoch 3 batch 1079 loss: 2.724874973297119\n",
      "epoch 3 batch 1080 loss: 2.0857701301574707\n",
      "epoch 3 batch 1081 loss: 2.5114119052886963\n",
      "epoch 3 batch 1082 loss: 2.406534194946289\n",
      "epoch 3 batch 1083 loss: 2.5011043548583984\n",
      "epoch 3 batch 1084 loss: 2.2229816913604736\n",
      "epoch 3 batch 1085 loss: 2.5119404792785645\n",
      "epoch 3 batch 1086 loss: 2.2914328575134277\n",
      "epoch 3 batch 1087 loss: 2.308433771133423\n",
      "epoch 3 batch 1088 loss: 2.2864439487457275\n",
      "epoch 3 batch 1089 loss: 2.481491804122925\n",
      "epoch 3 batch 1090 loss: 2.387976884841919\n",
      "epoch 3 batch 1091 loss: 2.730783462524414\n",
      "epoch 3 batch 1092 loss: 2.291576862335205\n",
      "epoch 3 batch 1093 loss: 2.5378732681274414\n",
      "epoch 3 batch 1094 loss: 2.509620428085327\n",
      "epoch 3 batch 1095 loss: 2.3490233421325684\n",
      "epoch 3 batch 1096 loss: 2.2988061904907227\n",
      "epoch 3 batch 1097 loss: 2.3063182830810547\n",
      "epoch 3 batch 1098 loss: 2.302560806274414\n",
      "epoch 3 batch 1099 loss: 2.3469462394714355\n",
      "epoch 3 batch 1100 loss: 2.4239206314086914\n",
      "epoch 3 batch 1101 loss: 2.388589382171631\n",
      "epoch 3 batch 1102 loss: 2.500183343887329\n",
      "epoch 3 batch 1103 loss: 2.5319037437438965\n",
      "epoch 3 batch 1104 loss: 2.3399791717529297\n",
      "epoch 3 batch 1105 loss: 2.4623265266418457\n",
      "epoch 3 batch 1106 loss: 2.0340657234191895\n",
      "epoch 3 batch 1107 loss: 2.630064010620117\n",
      "epoch 3 batch 1108 loss: 2.4792280197143555\n",
      "epoch 3 batch 1109 loss: 2.202859878540039\n",
      "epoch 3 batch 1110 loss: 2.1597213745117188\n",
      "epoch 3 batch 1111 loss: 2.161193370819092\n",
      "epoch 3 batch 1112 loss: 2.537281036376953\n",
      "epoch 3 batch 1113 loss: 2.64080810546875\n",
      "epoch 3 batch 1114 loss: 2.3684635162353516\n",
      "epoch 3 batch 1115 loss: 2.2617692947387695\n",
      "epoch 3 batch 1116 loss: 2.3673179149627686\n",
      "epoch 3 batch 1117 loss: 2.236499309539795\n",
      "epoch 3 batch 1118 loss: 2.22947359085083\n",
      "epoch 3 batch 1119 loss: 2.4860899448394775\n",
      "epoch 3 batch 1120 loss: 2.4296679496765137\n",
      "epoch 3 batch 1121 loss: 2.342341899871826\n",
      "epoch 3 batch 1122 loss: 2.3783323764801025\n",
      "epoch 3 batch 1123 loss: 2.4159841537475586\n",
      "epoch 3 batch 1124 loss: 2.3595008850097656\n",
      "epoch 3 batch 1125 loss: 2.384382486343384\n",
      "epoch 3 batch 1126 loss: 2.3561267852783203\n",
      "epoch 3 batch 1127 loss: 2.5206780433654785\n",
      "epoch 3 batch 1128 loss: 2.4065046310424805\n",
      "epoch 3 batch 1129 loss: 2.3286936283111572\n",
      "epoch 3 batch 1130 loss: 2.3653125762939453\n",
      "epoch 3 batch 1131 loss: 2.405566453933716\n",
      "epoch 3 batch 1132 loss: 2.3209691047668457\n",
      "epoch 3 batch 1133 loss: 2.56394624710083\n",
      "epoch 3 batch 1134 loss: 2.4887046813964844\n",
      "epoch 3 batch 1135 loss: 2.3315234184265137\n",
      "epoch 3 batch 1136 loss: 2.3975799083709717\n",
      "epoch 3 batch 1137 loss: 2.5282459259033203\n",
      "epoch 3 batch 1138 loss: 2.627262592315674\n",
      "epoch 3 batch 1139 loss: 2.393662929534912\n",
      "epoch 3 batch 1140 loss: 2.6879897117614746\n",
      "epoch 3 batch 1141 loss: 2.400999069213867\n",
      "epoch 3 batch 1142 loss: 2.4260010719299316\n",
      "epoch 3 batch 1143 loss: 2.2483320236206055\n",
      "epoch 3 batch 1144 loss: 2.580259323120117\n",
      "epoch 3 batch 1145 loss: 2.820110559463501\n",
      "epoch 3 batch 1146 loss: 2.4610371589660645\n",
      "epoch 3 batch 1147 loss: 2.403810501098633\n",
      "epoch 3 batch 1148 loss: 2.464902877807617\n",
      "epoch 3 batch 1149 loss: 2.3462347984313965\n",
      "epoch 3 batch 1150 loss: 2.2236666679382324\n",
      "epoch 3 batch 1151 loss: 2.2800910472869873\n",
      "epoch 3 batch 1152 loss: 2.449357509613037\n",
      "epoch 3 batch 1153 loss: 2.729835033416748\n",
      "epoch 3 batch 1154 loss: 2.435060977935791\n",
      "epoch 3 batch 1155 loss: 2.372337579727173\n",
      "epoch 3 batch 1156 loss: 2.274651050567627\n",
      "epoch 3 batch 1157 loss: 2.217629909515381\n",
      "epoch 3 batch 1158 loss: 2.5042312145233154\n",
      "epoch 3 batch 1159 loss: 2.29254150390625\n",
      "epoch 3 batch 1160 loss: 2.766545057296753\n",
      "epoch 3 batch 1161 loss: 2.206946849822998\n",
      "epoch 3 batch 1162 loss: 2.5993690490722656\n",
      "epoch 3 batch 1163 loss: 2.5889272689819336\n",
      "epoch 3 batch 1164 loss: 2.131044387817383\n",
      "epoch 3 batch 1165 loss: 2.474039077758789\n",
      "epoch 3 batch 1166 loss: 2.318761110305786\n",
      "epoch 3 batch 1167 loss: 2.0800018310546875\n",
      "epoch 3 batch 1168 loss: 2.379713535308838\n",
      "epoch 3 batch 1169 loss: 2.3448681831359863\n",
      "epoch 3 batch 1170 loss: 2.625581741333008\n",
      "epoch 3 batch 1171 loss: 2.273538827896118\n",
      "epoch 3 batch 1172 loss: 2.3503308296203613\n",
      "epoch 3 batch 1173 loss: 2.494040012359619\n",
      "epoch 3 batch 1174 loss: 2.3075647354125977\n",
      "epoch 3 batch 1175 loss: 2.624420166015625\n",
      "epoch 3 batch 1176 loss: 2.377204179763794\n",
      "epoch 3 batch 1177 loss: 2.481231689453125\n",
      "epoch 3 batch 1178 loss: 2.510012626647949\n",
      "epoch 3 batch 1179 loss: 2.290760040283203\n",
      "epoch 3 batch 1180 loss: 2.202369213104248\n",
      "epoch 3 batch 1181 loss: 2.450037956237793\n",
      "epoch 3 batch 1182 loss: 2.785090446472168\n",
      "epoch 3 batch 1183 loss: 2.546811580657959\n",
      "epoch 3 batch 1184 loss: 2.327796459197998\n",
      "epoch 3 batch 1185 loss: 2.1879498958587646\n",
      "epoch 3 batch 1186 loss: 2.397716999053955\n",
      "epoch 3 batch 1187 loss: 2.2043399810791016\n",
      "epoch 3 batch 1188 loss: 2.2368478775024414\n",
      "epoch 3 batch 1189 loss: 2.584230899810791\n",
      "epoch 3 batch 1190 loss: 2.584956407546997\n",
      "epoch 3 batch 1191 loss: 2.6006722450256348\n",
      "epoch 3 batch 1192 loss: 2.2717509269714355\n",
      "epoch 3 batch 1193 loss: 2.325162172317505\n",
      "epoch 3 batch 1194 loss: 2.6009373664855957\n",
      "epoch 3 batch 1195 loss: 2.449967861175537\n",
      "epoch 3 batch 1196 loss: 2.1918604373931885\n",
      "epoch 3 batch 1197 loss: 2.3822245597839355\n",
      "epoch 3 batch 1198 loss: 2.4204530715942383\n",
      "epoch 3 batch 1199 loss: 2.5954322814941406\n",
      "epoch 3 batch 1200 loss: 2.216911554336548\n",
      "epoch 3 batch 1201 loss: 2.748321533203125\n",
      "epoch 3 batch 1202 loss: 2.3165407180786133\n",
      "epoch 3 batch 1203 loss: 2.376957893371582\n",
      "epoch 3 batch 1204 loss: 2.7403969764709473\n",
      "epoch 3 batch 1205 loss: 2.2132973670959473\n",
      "epoch 3 batch 1206 loss: 2.6871142387390137\n",
      "epoch 3 batch 1207 loss: 2.2943263053894043\n",
      "epoch 3 batch 1208 loss: 2.343480348587036\n",
      "epoch 3 batch 1209 loss: 2.166616439819336\n",
      "epoch 3 batch 1210 loss: 2.3280420303344727\n",
      "epoch 3 batch 1211 loss: 2.4838995933532715\n",
      "epoch 3 batch 1212 loss: 2.6084470748901367\n",
      "epoch 3 batch 1213 loss: 2.51271390914917\n",
      "epoch 3 batch 1214 loss: 2.809215545654297\n",
      "epoch 3 batch 1215 loss: 2.267591953277588\n",
      "epoch 3 batch 1216 loss: 2.640613555908203\n",
      "epoch 3 batch 1217 loss: 2.3551368713378906\n",
      "epoch 3 batch 1218 loss: 2.421647548675537\n",
      "epoch 3 batch 1219 loss: 2.462451457977295\n",
      "epoch 3 batch 1220 loss: 2.2345094680786133\n",
      "epoch 3 batch 1221 loss: 2.400231122970581\n",
      "epoch 3 batch 1222 loss: 2.1579580307006836\n",
      "epoch 3 batch 1223 loss: 2.4099113941192627\n",
      "epoch 3 batch 1224 loss: 2.359532356262207\n",
      "epoch 3 batch 1225 loss: 2.3583240509033203\n",
      "epoch 3 batch 1226 loss: 2.673473358154297\n",
      "epoch 3 batch 1227 loss: 2.2746095657348633\n",
      "epoch 3 batch 1228 loss: 2.355675220489502\n",
      "epoch 3 batch 1229 loss: 2.37903094291687\n",
      "epoch 3 batch 1230 loss: 2.6241283416748047\n",
      "epoch 3 batch 1231 loss: 2.362135171890259\n",
      "epoch 3 batch 1232 loss: 2.166102409362793\n",
      "epoch 3 batch 1233 loss: 2.4728517532348633\n",
      "epoch 3 batch 1234 loss: 2.5224344730377197\n",
      "epoch 3 batch 1235 loss: 2.3657445907592773\n",
      "epoch 3 batch 1236 loss: 2.47365665435791\n",
      "epoch 3 batch 1237 loss: 2.52078914642334\n",
      "epoch 3 batch 1238 loss: 2.7904043197631836\n",
      "epoch 3 batch 1239 loss: 2.372209310531616\n",
      "epoch 3 batch 1240 loss: 2.267221689224243\n",
      "epoch 3 batch 1241 loss: 2.730600118637085\n",
      "epoch 3 batch 1242 loss: 2.327134370803833\n",
      "epoch 3 batch 1243 loss: 2.4725894927978516\n",
      "epoch 3 batch 1244 loss: 2.3817930221557617\n",
      "epoch 3 batch 1245 loss: 2.337639331817627\n",
      "epoch 3 batch 1246 loss: 2.436748504638672\n",
      "epoch 3 batch 1247 loss: 2.2763710021972656\n",
      "epoch 3 batch 1248 loss: 2.258695363998413\n",
      "epoch 3 batch 1249 loss: 2.6151652336120605\n",
      "epoch 3 batch 1250 loss: 2.4695873260498047\n",
      "epoch 3 batch 1251 loss: 2.1909193992614746\n",
      "epoch 3 batch 1252 loss: 2.4662108421325684\n",
      "epoch 3 batch 1253 loss: 2.3295655250549316\n",
      "epoch 3 batch 1254 loss: 2.5942118167877197\n",
      "epoch 3 batch 1255 loss: 2.765331268310547\n",
      "epoch 3 batch 1256 loss: 2.192533493041992\n",
      "epoch 3 batch 1257 loss: 2.675354480743408\n",
      "epoch 3 batch 1258 loss: 2.377598285675049\n",
      "epoch 3 batch 1259 loss: 2.2621161937713623\n",
      "epoch 3 batch 1260 loss: 2.4277892112731934\n",
      "epoch 3 batch 1261 loss: 2.1632328033447266\n",
      "epoch 3 batch 1262 loss: 2.6429617404937744\n",
      "epoch 3 batch 1263 loss: 2.2810165882110596\n",
      "epoch 3 batch 1264 loss: 2.478062629699707\n",
      "epoch 3 batch 1265 loss: 2.386253833770752\n",
      "epoch 3 batch 1266 loss: 2.4573843479156494\n",
      "epoch 3 batch 1267 loss: 2.7917587757110596\n",
      "epoch 3 batch 1268 loss: 2.3539633750915527\n",
      "epoch 3 batch 1269 loss: 2.184760093688965\n",
      "epoch 3 batch 1270 loss: 2.4109342098236084\n",
      "epoch 3 batch 1271 loss: 2.195052146911621\n",
      "epoch 3 batch 1272 loss: 2.3189854621887207\n",
      "epoch 3 batch 1273 loss: 2.5064563751220703\n",
      "epoch 3 batch 1274 loss: 2.5219240188598633\n",
      "epoch 3 batch 1275 loss: 2.3891639709472656\n",
      "epoch 3 batch 1276 loss: 2.1597108840942383\n",
      "epoch 3 batch 1277 loss: 2.272899627685547\n",
      "epoch 3 batch 1278 loss: 2.6342780590057373\n",
      "epoch 3 batch 1279 loss: 2.1702539920806885\n",
      "epoch 3 batch 1280 loss: 2.502906560897827\n",
      "epoch 3 batch 1281 loss: 2.2729973793029785\n",
      "epoch 3 batch 1282 loss: 2.5143494606018066\n",
      "epoch 3 batch 1283 loss: 2.3449411392211914\n",
      "epoch 3 batch 1284 loss: 2.501039505004883\n",
      "epoch 3 batch 1285 loss: 2.3803224563598633\n",
      "epoch 3 batch 1286 loss: 2.417647123336792\n",
      "epoch 3 batch 1287 loss: 2.173206090927124\n",
      "epoch 3 batch 1288 loss: 2.265209674835205\n",
      "epoch 3 batch 1289 loss: 2.281304359436035\n",
      "epoch 3 batch 1290 loss: 2.352449417114258\n",
      "epoch 3 batch 1291 loss: 2.3738465309143066\n",
      "epoch 3 batch 1292 loss: 2.1948084831237793\n",
      "epoch 3 batch 1293 loss: 2.6935534477233887\n",
      "epoch 3 batch 1294 loss: 2.364203929901123\n",
      "epoch 3 batch 1295 loss: 2.1791346073150635\n",
      "epoch 3 batch 1296 loss: 2.264291763305664\n",
      "epoch 3 batch 1297 loss: 2.689190626144409\n",
      "epoch 3 batch 1298 loss: 2.2530245780944824\n",
      "epoch 3 batch 1299 loss: 2.483135223388672\n",
      "epoch 3 batch 1300 loss: 2.19272518157959\n",
      "epoch 3 batch 1301 loss: 2.247918128967285\n",
      "epoch 3 batch 1302 loss: 2.2159082889556885\n",
      "epoch 3 batch 1303 loss: 2.507138252258301\n",
      "epoch 3 batch 1304 loss: 2.529284715652466\n",
      "epoch 3 batch 1305 loss: 2.3866074085235596\n",
      "epoch 3 batch 1306 loss: 2.429208993911743\n",
      "epoch 3 batch 1307 loss: 2.338289737701416\n",
      "epoch 3 batch 1308 loss: 2.333714008331299\n",
      "epoch 3 batch 1309 loss: 2.559643268585205\n",
      "epoch 3 batch 1310 loss: 2.3919005393981934\n",
      "epoch 3 batch 1311 loss: 2.7043519020080566\n",
      "epoch 3 batch 1312 loss: 2.4597184658050537\n",
      "epoch 3 batch 1313 loss: 2.3163254261016846\n",
      "epoch 3 batch 1314 loss: 2.2531864643096924\n",
      "epoch 3 batch 1315 loss: 2.74918270111084\n",
      "epoch 3 batch 1316 loss: 2.625176429748535\n",
      "epoch 3 batch 1317 loss: 2.454373598098755\n",
      "epoch 3 batch 1318 loss: 2.311539888381958\n",
      "epoch 3 batch 1319 loss: 2.4785635471343994\n",
      "epoch 3 batch 1320 loss: 2.377291202545166\n",
      "epoch 3 batch 1321 loss: 2.3971107006073\n",
      "epoch 3 batch 1322 loss: 2.3390815258026123\n",
      "epoch 3 batch 1323 loss: 2.7580583095550537\n",
      "epoch 3 batch 1324 loss: 2.1343674659729004\n",
      "epoch 3 batch 1325 loss: 2.450796127319336\n",
      "epoch 3 batch 1326 loss: 2.325411558151245\n",
      "epoch 3 batch 1327 loss: 2.6265993118286133\n",
      "epoch 3 batch 1328 loss: 2.2749457359313965\n",
      "epoch 3 batch 1329 loss: 2.7235493659973145\n",
      "epoch 3 batch 1330 loss: 2.610654354095459\n",
      "epoch 3 batch 1331 loss: 2.2896924018859863\n",
      "epoch 3 batch 1332 loss: 2.0903711318969727\n",
      "epoch 3 batch 1333 loss: 2.2371931076049805\n",
      "epoch 3 batch 1334 loss: 2.305954694747925\n",
      "epoch 3 batch 1335 loss: 2.5616819858551025\n",
      "epoch 3 batch 1336 loss: 2.215977191925049\n",
      "epoch 3 batch 1337 loss: 2.3435006141662598\n",
      "epoch 3 batch 1338 loss: 2.6172971725463867\n",
      "epoch 3 batch 1339 loss: 2.3653998374938965\n",
      "epoch 3 batch 1340 loss: 2.8694987297058105\n",
      "epoch 3 batch 1341 loss: 2.4310355186462402\n",
      "epoch 3 batch 1342 loss: 2.3589179515838623\n",
      "epoch 3 batch 1343 loss: 2.375880241394043\n",
      "epoch 3 batch 1344 loss: 2.3361940383911133\n",
      "epoch 3 batch 1345 loss: 2.4746339321136475\n",
      "epoch 3 batch 1346 loss: 2.2062697410583496\n",
      "epoch 3 batch 1347 loss: 2.4644224643707275\n",
      "epoch 3 batch 1348 loss: 2.569688320159912\n",
      "epoch 3 batch 1349 loss: 2.5280849933624268\n",
      "epoch 3 batch 1350 loss: 2.5997025966644287\n",
      "epoch 3 batch 1351 loss: 2.3604300022125244\n",
      "epoch 3 batch 1352 loss: 2.6003222465515137\n",
      "epoch 3 batch 1353 loss: 2.5599327087402344\n",
      "epoch 3 batch 1354 loss: 2.405648708343506\n",
      "epoch 3 batch 1355 loss: 2.646474838256836\n",
      "epoch 3 batch 1356 loss: 2.2529244422912598\n",
      "epoch 3 batch 1357 loss: 2.397024631500244\n",
      "epoch 3 batch 1358 loss: 2.5988612174987793\n",
      "epoch 3 batch 1359 loss: 2.5750436782836914\n",
      "epoch 3 batch 1360 loss: 2.406609535217285\n",
      "epoch 3 batch 1361 loss: 2.578822135925293\n",
      "epoch 3 batch 1362 loss: 2.5457701683044434\n",
      "epoch 3 batch 1363 loss: 2.496912717819214\n",
      "epoch 3 batch 1364 loss: 2.436032295227051\n",
      "epoch 3 batch 1365 loss: 2.41219425201416\n",
      "epoch 3 batch 1366 loss: 2.660989284515381\n",
      "epoch 3 batch 1367 loss: 2.195113182067871\n",
      "epoch 3 batch 1368 loss: 2.682575225830078\n",
      "epoch 3 batch 1369 loss: 2.2868380546569824\n",
      "epoch 3 batch 1370 loss: 2.3512091636657715\n",
      "epoch 3 batch 1371 loss: 2.6053483486175537\n",
      "epoch 3 batch 1372 loss: 2.33125638961792\n",
      "epoch 3 batch 1373 loss: 2.613368034362793\n",
      "epoch 3 batch 1374 loss: 2.403717517852783\n",
      "epoch 3 batch 1375 loss: 2.507552146911621\n",
      "epoch 3 batch 1376 loss: 2.3757293224334717\n",
      "epoch 3 batch 1377 loss: 2.249293804168701\n",
      "epoch 3 batch 1378 loss: 2.665919780731201\n",
      "epoch 3 batch 1379 loss: 2.621875762939453\n",
      "epoch 3 batch 1380 loss: 2.460153102874756\n",
      "epoch 3 batch 1381 loss: 2.6126670837402344\n",
      "epoch 3 batch 1382 loss: 2.2570977210998535\n",
      "epoch 3 batch 1383 loss: 2.3138465881347656\n",
      "epoch 3 batch 1384 loss: 2.3199708461761475\n",
      "epoch 3 batch 1385 loss: 2.2859320640563965\n",
      "epoch 3 batch 1386 loss: 2.1663622856140137\n",
      "epoch 3 batch 1387 loss: 2.59787654876709\n",
      "epoch 3 batch 1388 loss: 2.297280788421631\n",
      "epoch 3 batch 1389 loss: 2.328561305999756\n",
      "epoch 3 batch 1390 loss: 2.5569639205932617\n",
      "epoch 3 batch 1391 loss: 2.507347822189331\n",
      "epoch 3 batch 1392 loss: 2.3254010677337646\n",
      "epoch 3 batch 1393 loss: 2.5548477172851562\n",
      "epoch 3 batch 1394 loss: 2.7501869201660156\n",
      "epoch 3 batch 1395 loss: 2.3643927574157715\n",
      "epoch 3 batch 1396 loss: 2.2305784225463867\n",
      "epoch 3 batch 1397 loss: 2.400390625\n",
      "epoch 3 batch 1398 loss: 2.6209192276000977\n",
      "epoch 3 batch 1399 loss: 2.285524845123291\n",
      "epoch 3 batch 1400 loss: 2.3175172805786133\n",
      "epoch 3 batch 1401 loss: 2.556346893310547\n",
      "epoch 3 batch 1402 loss: 2.3186607360839844\n",
      "epoch 3 batch 1403 loss: 2.435182571411133\n",
      "epoch 3 batch 1404 loss: 2.269886016845703\n",
      "epoch 3 batch 1405 loss: 2.2754034996032715\n",
      "epoch 3 batch 1406 loss: 2.359372615814209\n",
      "epoch 3 batch 1407 loss: 2.419881582260132\n",
      "epoch 3 batch 1408 loss: 2.457926034927368\n",
      "epoch 3 batch 1409 loss: 2.4178595542907715\n",
      "epoch 3 batch 1410 loss: 2.2350220680236816\n",
      "epoch 3 batch 1411 loss: 2.650749444961548\n",
      "epoch 3 batch 1412 loss: 2.4318692684173584\n",
      "epoch 3 batch 1413 loss: 2.5513100624084473\n",
      "epoch 3 batch 1414 loss: 2.582810401916504\n",
      "epoch 3 batch 1415 loss: 2.510780096054077\n",
      "epoch 3 batch 1416 loss: 2.524249792098999\n",
      "epoch 3 batch 1417 loss: 2.224025011062622\n",
      "epoch 3 batch 1418 loss: 2.6704883575439453\n",
      "epoch 3 batch 1419 loss: 2.775186538696289\n",
      "epoch 3 batch 1420 loss: 2.2581253051757812\n",
      "epoch 3 batch 1421 loss: 2.3070178031921387\n",
      "epoch 3 batch 1422 loss: 2.6827869415283203\n",
      "epoch 3 batch 1423 loss: 2.2803945541381836\n",
      "epoch 3 batch 1424 loss: 2.4228429794311523\n",
      "epoch 3 batch 1425 loss: 2.5250353813171387\n",
      "epoch 3 batch 1426 loss: 2.548098564147949\n",
      "epoch 3 batch 1427 loss: 2.145742893218994\n",
      "epoch 3 batch 1428 loss: 2.430680274963379\n",
      "epoch 3 batch 1429 loss: 2.4924628734588623\n",
      "epoch 3 batch 1430 loss: 2.535271167755127\n",
      "epoch 3 batch 1431 loss: 2.297372341156006\n",
      "epoch 3 batch 1432 loss: 2.5947842597961426\n",
      "epoch 3 batch 1433 loss: 2.51283597946167\n",
      "epoch 3 batch 1434 loss: 2.3147144317626953\n",
      "epoch 3 batch 1435 loss: 2.6358070373535156\n",
      "epoch 3 batch 1436 loss: 2.2921571731567383\n",
      "epoch 3 batch 1437 loss: 2.3255486488342285\n",
      "epoch 3 batch 1438 loss: 2.5668420791625977\n",
      "epoch 3 batch 1439 loss: 2.490428924560547\n",
      "epoch 3 batch 1440 loss: 2.4081273078918457\n",
      "epoch 3 batch 1441 loss: 2.222165584564209\n",
      "epoch 3 batch 1442 loss: 2.118882179260254\n",
      "epoch 3 batch 1443 loss: 2.79130220413208\n",
      "epoch 3 batch 1444 loss: 2.5008461475372314\n",
      "epoch 3 batch 1445 loss: 2.4675168991088867\n",
      "epoch 3 batch 1446 loss: 2.3604495525360107\n",
      "epoch 3 batch 1447 loss: 2.4917263984680176\n",
      "epoch 3 batch 1448 loss: 2.8233227729797363\n",
      "epoch 3 batch 1449 loss: 2.4517011642456055\n",
      "epoch 3 batch 1450 loss: 2.41166090965271\n",
      "epoch 3 batch 1451 loss: 2.7326622009277344\n",
      "epoch 3 batch 1452 loss: 2.552971363067627\n",
      "epoch 3 batch 1453 loss: 2.32374906539917\n",
      "epoch 3 batch 1454 loss: 2.6798744201660156\n",
      "epoch 3 batch 1455 loss: 2.4200010299682617\n",
      "epoch 3 batch 1456 loss: 2.7476847171783447\n",
      "epoch 3 batch 1457 loss: 2.375514030456543\n",
      "epoch 3 batch 1458 loss: 2.438326358795166\n",
      "epoch 3 batch 1459 loss: 2.2000961303710938\n",
      "epoch 3 batch 1460 loss: 2.409123420715332\n",
      "epoch 3 batch 1461 loss: 2.633371591567993\n",
      "epoch 3 batch 1462 loss: 2.571002960205078\n",
      "epoch 3 batch 1463 loss: 2.2908997535705566\n",
      "epoch 3 batch 1464 loss: 2.615203380584717\n",
      "epoch 3 batch 1465 loss: 2.232088804244995\n",
      "epoch 3 batch 1466 loss: 2.2851319313049316\n",
      "epoch 3 batch 1467 loss: 2.6567800045013428\n",
      "epoch 3 batch 1468 loss: 2.2382142543792725\n",
      "epoch 3 batch 1469 loss: 2.3824260234832764\n",
      "epoch 3 batch 1470 loss: 2.4292073249816895\n",
      "epoch 3 batch 1471 loss: 2.4356303215026855\n",
      "epoch 3 batch 1472 loss: 2.312286615371704\n",
      "epoch 3 batch 1473 loss: 2.603278875350952\n",
      "epoch 3 batch 1474 loss: 2.4236929416656494\n",
      "epoch 3 batch 1475 loss: 2.8239779472351074\n",
      "epoch 3 batch 1476 loss: 2.349684238433838\n",
      "epoch 3 batch 1477 loss: 2.5762085914611816\n",
      "epoch 3 batch 1478 loss: 2.274940252304077\n",
      "epoch 3 batch 1479 loss: 2.222282648086548\n",
      "epoch 3 batch 1480 loss: 2.5510172843933105\n",
      "epoch 3 batch 1481 loss: 2.2065823078155518\n",
      "epoch 3 batch 1482 loss: 2.3754305839538574\n",
      "epoch 3 batch 1483 loss: 2.271918296813965\n",
      "epoch 3 batch 1484 loss: 2.3440470695495605\n",
      "epoch 3 batch 1485 loss: 2.2819182872772217\n",
      "epoch 3 batch 1486 loss: 2.5395560264587402\n",
      "epoch 3 batch 1487 loss: 2.4740047454833984\n",
      "epoch 3 batch 1488 loss: 2.481001377105713\n",
      "epoch 3 batch 1489 loss: 2.484236717224121\n",
      "epoch 3 batch 1490 loss: 2.2212541103363037\n",
      "epoch 3 batch 1491 loss: 2.4884846210479736\n",
      "epoch 3 batch 1492 loss: 2.3350541591644287\n",
      "epoch 3 batch 1493 loss: 2.472376585006714\n",
      "epoch 3 batch 1494 loss: 2.2907981872558594\n",
      "epoch 3 batch 1495 loss: 2.4019112586975098\n",
      "epoch 3 batch 1496 loss: 2.4990475177764893\n",
      "epoch 3 batch 1497 loss: 2.3804712295532227\n",
      "epoch 3 batch 1498 loss: 2.41477632522583\n",
      "epoch 3 batch 1499 loss: 2.5466206073760986\n",
      "epoch 3 batch 1500 loss: 2.168506383895874\n",
      "epoch 3 batch 1501 loss: 2.4266934394836426\n",
      "epoch 3 batch 1502 loss: 2.402529716491699\n",
      "epoch 3 batch 1503 loss: 2.2887086868286133\n",
      "epoch 3 batch 1504 loss: 2.310854911804199\n",
      "epoch 3 batch 1505 loss: 2.4270057678222656\n",
      "epoch 3 batch 1506 loss: 2.442418336868286\n",
      "epoch 3 batch 1507 loss: 2.49812912940979\n",
      "epoch 3 batch 1508 loss: 2.521451473236084\n",
      "epoch 3 batch 1509 loss: 2.4049124717712402\n",
      "epoch 3 batch 1510 loss: 2.8660452365875244\n",
      "epoch 3 batch 1511 loss: 2.3307814598083496\n",
      "epoch 3 batch 1512 loss: 2.4918618202209473\n",
      "epoch 3 batch 1513 loss: 2.307593822479248\n",
      "epoch 3 batch 1514 loss: 2.336620807647705\n",
      "epoch 3 batch 1515 loss: 2.402473211288452\n",
      "epoch 3 batch 1516 loss: 2.800084114074707\n",
      "epoch 3 batch 1517 loss: 2.587334632873535\n",
      "epoch 3 batch 1518 loss: 2.244948387145996\n",
      "epoch 3 batch 1519 loss: 2.680438756942749\n",
      "epoch 3 batch 1520 loss: 2.5955169200897217\n",
      "epoch 3 batch 1521 loss: 2.3394908905029297\n",
      "epoch 3 batch 1522 loss: 2.3548693656921387\n",
      "epoch 3 batch 1523 loss: 2.430957317352295\n",
      "epoch 3 batch 1524 loss: 2.561464548110962\n",
      "epoch 3 batch 1525 loss: 2.2225375175476074\n",
      "epoch 3 batch 1526 loss: 2.2231907844543457\n",
      "epoch 3 batch 1527 loss: 2.5213232040405273\n",
      "epoch 3 batch 1528 loss: 2.269927978515625\n",
      "epoch 3 batch 1529 loss: 2.3189332485198975\n",
      "epoch 3 batch 1530 loss: 2.4121789932250977\n",
      "epoch 3 batch 1531 loss: 2.440488815307617\n",
      "epoch 3 batch 1532 loss: 2.4843668937683105\n",
      "epoch 3 batch 1533 loss: 2.22973895072937\n",
      "epoch 3 batch 1534 loss: 2.4408812522888184\n",
      "epoch 3 batch 1535 loss: 2.46201753616333\n",
      "epoch 3 batch 1536 loss: 2.2990565299987793\n",
      "epoch 3 batch 1537 loss: 2.509608507156372\n",
      "epoch 3 batch 1538 loss: 2.478215456008911\n",
      "epoch 3 batch 1539 loss: 2.2658839225769043\n",
      "epoch 3 batch 1540 loss: 2.764132022857666\n",
      "epoch 3 batch 1541 loss: 2.4717111587524414\n",
      "epoch 3 batch 1542 loss: 2.3869338035583496\n",
      "epoch 3 batch 1543 loss: 2.1960296630859375\n",
      "epoch 3 batch 1544 loss: 2.1857147216796875\n",
      "epoch 3 batch 1545 loss: 2.3965766429901123\n",
      "epoch 3 batch 1546 loss: 2.23875093460083\n",
      "epoch 3 batch 1547 loss: 2.356529474258423\n",
      "epoch 3 batch 1548 loss: 2.255737781524658\n",
      "epoch 3 batch 1549 loss: 2.4151580333709717\n",
      "epoch 3 batch 1550 loss: 2.3345448970794678\n",
      "epoch 3 batch 1551 loss: 2.298694610595703\n",
      "epoch 3 batch 1552 loss: 2.192793607711792\n",
      "epoch 3 batch 1553 loss: 2.7150518894195557\n",
      "epoch 3 batch 1554 loss: 2.6211228370666504\n",
      "epoch 3 batch 1555 loss: 2.4680614471435547\n",
      "epoch 3 batch 1556 loss: 2.5645689964294434\n",
      "epoch 3 batch 1557 loss: 2.4134764671325684\n",
      "epoch 3 batch 1558 loss: 2.868206024169922\n",
      "epoch 3 batch 1559 loss: 2.4513964653015137\n",
      "epoch 3 batch 1560 loss: 2.509704113006592\n",
      "epoch 3 batch 1561 loss: 2.4767415523529053\n",
      "epoch 3 batch 1562 loss: 2.631941080093384\n",
      "epoch 3 batch 1563 loss: 2.4865059852600098\n",
      "epoch 3 batch 1564 loss: 2.4205644130706787\n",
      "epoch 3 batch 1565 loss: 2.7449419498443604\n",
      "epoch 3 batch 1566 loss: 2.58693265914917\n",
      "epoch 3 batch 1567 loss: 2.618089199066162\n",
      "epoch 3 batch 1568 loss: 2.3382434844970703\n",
      "epoch 3 batch 1569 loss: 2.450568437576294\n",
      "epoch 3 batch 1570 loss: 2.8034377098083496\n",
      "epoch 3 batch 1571 loss: 2.5880165100097656\n",
      "epoch 3 batch 1572 loss: 2.3576433658599854\n",
      "epoch 3 batch 1573 loss: 2.2983641624450684\n",
      "epoch 3 batch 1574 loss: 2.3439059257507324\n",
      "epoch 3 batch 1575 loss: 2.558218002319336\n",
      "epoch 3 batch 1576 loss: 2.4328017234802246\n",
      "epoch 3 batch 1577 loss: 2.3717331886291504\n",
      "epoch 3 batch 1578 loss: 2.471695899963379\n",
      "epoch 3 batch 1579 loss: 2.2205662727355957\n",
      "epoch 3 batch 1580 loss: 2.442728042602539\n",
      "epoch 3 batch 1581 loss: 2.3416457176208496\n",
      "epoch 3 batch 1582 loss: 2.387707471847534\n",
      "epoch 3 batch 1583 loss: 2.3279054164886475\n",
      "epoch 3 batch 1584 loss: 2.4814229011535645\n",
      "epoch 3 batch 1585 loss: 2.54144024848938\n",
      "epoch 3 batch 1586 loss: 2.248227119445801\n",
      "epoch 3 batch 1587 loss: 2.600402593612671\n",
      "epoch 3 batch 1588 loss: 2.368321657180786\n",
      "epoch 3 batch 1589 loss: 2.2743120193481445\n",
      "epoch 3 batch 1590 loss: 2.4217162132263184\n",
      "epoch 3 batch 1591 loss: 2.2773756980895996\n",
      "epoch 3 batch 1592 loss: 2.216771125793457\n",
      "epoch 3 batch 1593 loss: 2.404123306274414\n",
      "epoch 3 batch 1594 loss: 2.3682875633239746\n",
      "epoch 3 batch 1595 loss: 2.7480499744415283\n",
      "epoch 3 batch 1596 loss: 2.3626856803894043\n",
      "epoch 3 batch 1597 loss: 2.376265048980713\n",
      "epoch 3 batch 1598 loss: 2.349576234817505\n",
      "epoch 3 batch 1599 loss: 2.1344895362854004\n",
      "epoch 3 batch 1600 loss: 2.1970245838165283\n",
      "epoch 3 batch 1601 loss: 2.4437918663024902\n",
      "epoch 3 batch 1602 loss: 2.3117737770080566\n",
      "epoch 3 batch 1603 loss: 2.2966511249542236\n",
      "epoch 3 batch 1604 loss: 2.346858024597168\n",
      "epoch 3 batch 1605 loss: 2.438213348388672\n",
      "epoch 3 batch 1606 loss: 2.2783894538879395\n",
      "epoch 3 batch 1607 loss: 2.3261852264404297\n",
      "epoch 3 batch 1608 loss: 2.40805721282959\n",
      "epoch 3 batch 1609 loss: 2.232809066772461\n",
      "epoch 3 batch 1610 loss: 2.4183201789855957\n",
      "epoch 3 batch 1611 loss: 2.518343448638916\n",
      "epoch 3 batch 1612 loss: 2.5321879386901855\n",
      "epoch 3 batch 1613 loss: 2.3408865928649902\n",
      "epoch 3 batch 1614 loss: 2.408022880554199\n",
      "epoch 3 batch 1615 loss: 2.5827858448028564\n",
      "epoch 3 batch 1616 loss: 2.4319753646850586\n",
      "epoch 3 batch 1617 loss: 2.506796360015869\n",
      "epoch 3 batch 1618 loss: 2.2577691078186035\n",
      "epoch 3 batch 1619 loss: 2.6126413345336914\n",
      "epoch 3 batch 1620 loss: 2.500290632247925\n",
      "epoch 3 batch 1621 loss: 2.168673515319824\n",
      "epoch 3 batch 1622 loss: 2.352753162384033\n",
      "epoch 3 batch 1623 loss: 2.4499316215515137\n",
      "epoch 3 batch 1624 loss: 2.5587539672851562\n",
      "epoch 3 batch 1625 loss: 2.7333974838256836\n",
      "epoch 3 batch 1626 loss: 2.331507682800293\n",
      "epoch 3 batch 1627 loss: 2.4046695232391357\n",
      "epoch 3 batch 1628 loss: 2.710641384124756\n",
      "epoch 3 batch 1629 loss: 2.4018077850341797\n",
      "epoch 3 batch 1630 loss: 2.7213125228881836\n",
      "epoch 3 batch 1631 loss: 2.544898748397827\n",
      "epoch 3 batch 1632 loss: 2.2796902656555176\n",
      "epoch 3 batch 1633 loss: 2.328913927078247\n",
      "epoch 3 batch 1634 loss: 2.7236180305480957\n",
      "epoch 3 batch 1635 loss: 2.249494791030884\n",
      "epoch 3 batch 1636 loss: 2.395508289337158\n",
      "epoch 3 batch 1637 loss: 2.3636534214019775\n",
      "epoch 3 batch 1638 loss: 2.7261528968811035\n",
      "epoch 3 batch 1639 loss: 2.1547138690948486\n",
      "epoch 3 batch 1640 loss: 2.2504796981811523\n",
      "epoch 3 batch 1641 loss: 2.6533398628234863\n",
      "epoch 3 batch 1642 loss: 2.599555730819702\n",
      "epoch 3 batch 1643 loss: 2.619539737701416\n",
      "epoch 3 batch 1644 loss: 2.32871675491333\n",
      "epoch 3 batch 1645 loss: 2.4780092239379883\n",
      "epoch 3 batch 1646 loss: 2.3264129161834717\n",
      "epoch 3 batch 1647 loss: 2.558671474456787\n",
      "epoch 3 batch 1648 loss: 2.364945888519287\n",
      "epoch 3 batch 1649 loss: 2.2307868003845215\n",
      "epoch 3 batch 1650 loss: 2.492551326751709\n",
      "epoch 3 batch 1651 loss: 2.2094805240631104\n",
      "epoch 3 batch 1652 loss: 2.3001084327697754\n",
      "epoch 3 batch 1653 loss: 2.2240262031555176\n",
      "epoch 3 batch 1654 loss: 2.61029052734375\n",
      "epoch 3 batch 1655 loss: 2.319693088531494\n",
      "epoch 3 batch 1656 loss: 2.2136552333831787\n",
      "epoch 3 batch 1657 loss: 2.4818191528320312\n",
      "epoch 3 batch 1658 loss: 2.484508514404297\n",
      "epoch 3 batch 1659 loss: 2.5698161125183105\n",
      "epoch 3 batch 1660 loss: 2.3917667865753174\n",
      "epoch 3 batch 1661 loss: 2.3797545433044434\n",
      "epoch 3 batch 1662 loss: 2.625706672668457\n",
      "epoch 3 batch 1663 loss: 2.3447446823120117\n",
      "epoch 3 batch 1664 loss: 2.2330493927001953\n",
      "epoch 3 batch 1665 loss: 2.346736192703247\n",
      "epoch 3 batch 1666 loss: 2.748899221420288\n",
      "epoch 3 batch 1667 loss: 2.338547706604004\n",
      "epoch 3 batch 1668 loss: 2.529560089111328\n",
      "epoch 3 batch 1669 loss: 2.212102174758911\n",
      "epoch 3 batch 1670 loss: 2.537961959838867\n",
      "epoch 3 batch 1671 loss: 2.244858503341675\n",
      "epoch 3 batch 1672 loss: 2.5487353801727295\n",
      "epoch 3 batch 1673 loss: 2.541104555130005\n",
      "epoch 3 batch 1674 loss: 2.1715035438537598\n",
      "epoch 3 batch 1675 loss: 2.508920192718506\n",
      "epoch 3 batch 1676 loss: 2.423320770263672\n",
      "epoch 3 batch 1677 loss: 2.349752902984619\n",
      "epoch 3 batch 1678 loss: 2.4753150939941406\n",
      "epoch 3 batch 1679 loss: 2.2026777267456055\n",
      "epoch 3 batch 1680 loss: 2.332732915878296\n",
      "epoch 3 batch 1681 loss: 2.378648042678833\n",
      "epoch 3 batch 1682 loss: 2.2829813957214355\n",
      "epoch 3 batch 1683 loss: 2.4779398441314697\n",
      "epoch 3 batch 1684 loss: 2.186570167541504\n",
      "epoch 3 batch 1685 loss: 2.4277000427246094\n",
      "epoch 3 batch 1686 loss: 2.4306187629699707\n",
      "epoch 3 batch 1687 loss: 2.7941439151763916\n",
      "epoch 3 batch 1688 loss: 2.4636054039001465\n",
      "epoch 3 batch 1689 loss: 2.4417881965637207\n",
      "epoch 3 batch 1690 loss: 2.661696434020996\n",
      "epoch 3 batch 1691 loss: 2.4502854347229004\n",
      "epoch 3 batch 1692 loss: 2.1741437911987305\n",
      "epoch 3 batch 1693 loss: 2.520667552947998\n",
      "epoch 3 batch 1694 loss: 2.4402475357055664\n",
      "epoch 3 batch 1695 loss: 2.4595298767089844\n",
      "epoch 3 batch 1696 loss: 2.596672534942627\n",
      "epoch 3 batch 1697 loss: 2.5813186168670654\n",
      "epoch 3 batch 1698 loss: 2.2975211143493652\n",
      "epoch 3 batch 1699 loss: 2.3958234786987305\n",
      "epoch 3 batch 1700 loss: 2.2306013107299805\n",
      "epoch 3 batch 1701 loss: 2.4260120391845703\n",
      "epoch 3 batch 1702 loss: 2.517780303955078\n",
      "epoch 3 batch 1703 loss: 2.871354103088379\n",
      "epoch 3 batch 1704 loss: 2.2630486488342285\n",
      "epoch 3 batch 1705 loss: 2.3567705154418945\n",
      "epoch 3 batch 1706 loss: 2.4430994987487793\n",
      "epoch 3 batch 1707 loss: 2.529172897338867\n",
      "epoch 3 batch 1708 loss: 2.1744813919067383\n",
      "epoch 3 batch 1709 loss: 2.4381422996520996\n",
      "epoch 3 batch 1710 loss: 2.1732563972473145\n",
      "epoch 3 batch 1711 loss: 2.4474732875823975\n",
      "epoch 3 batch 1712 loss: 2.4902453422546387\n",
      "epoch 3 batch 1713 loss: 2.7141828536987305\n",
      "epoch 3 batch 1714 loss: 2.336277484893799\n",
      "epoch 3 batch 1715 loss: 2.4284791946411133\n",
      "epoch 3 batch 1716 loss: 2.4023962020874023\n",
      "epoch 3 batch 1717 loss: 2.429612636566162\n",
      "epoch 3 batch 1718 loss: 2.3231496810913086\n",
      "epoch 3 batch 1719 loss: 2.371386766433716\n",
      "epoch 3 batch 1720 loss: 2.227114677429199\n",
      "epoch 3 batch 1721 loss: 2.4508073329925537\n",
      "epoch 3 batch 1722 loss: 2.2661542892456055\n",
      "epoch 3 batch 1723 loss: 2.293768882751465\n",
      "epoch 3 batch 1724 loss: 2.499505043029785\n",
      "epoch 3 batch 1725 loss: 2.3446669578552246\n",
      "epoch 3 batch 1726 loss: 2.6729445457458496\n",
      "epoch 3 batch 1727 loss: 2.376939535140991\n",
      "epoch 3 batch 1728 loss: 2.2681169509887695\n",
      "epoch 3 batch 1729 loss: 2.371763229370117\n",
      "epoch 3 batch 1730 loss: 2.564971446990967\n",
      "epoch 3 batch 1731 loss: 2.7885000705718994\n",
      "epoch 3 batch 1732 loss: 2.513754367828369\n",
      "epoch 3 batch 1733 loss: 2.6157126426696777\n",
      "epoch 3 batch 1734 loss: 2.35225510597229\n",
      "epoch 3 batch 1735 loss: 2.3311727046966553\n",
      "epoch 3 batch 1736 loss: 2.4108166694641113\n",
      "epoch 3 batch 1737 loss: 2.4241995811462402\n",
      "epoch 3 batch 1738 loss: 2.5794930458068848\n",
      "epoch 3 batch 1739 loss: 2.5388617515563965\n",
      "epoch 3 batch 1740 loss: 2.6758081912994385\n",
      "epoch 3 batch 1741 loss: 2.4308838844299316\n",
      "epoch 3 batch 1742 loss: 2.647470474243164\n",
      "epoch 3 batch 1743 loss: 2.322720527648926\n",
      "epoch 3 batch 1744 loss: 2.452495574951172\n",
      "epoch 3 batch 1745 loss: 2.2425756454467773\n",
      "epoch 3 batch 1746 loss: 2.415024518966675\n",
      "epoch 3 batch 1747 loss: 2.2310941219329834\n",
      "epoch 3 batch 1748 loss: 2.5608503818511963\n",
      "epoch 3 batch 1749 loss: 2.6329386234283447\n",
      "epoch 3 batch 1750 loss: 2.5853519439697266\n",
      "epoch 3 batch 1751 loss: 2.4604032039642334\n",
      "epoch 3 batch 1752 loss: 2.62229061126709\n",
      "epoch 3 batch 1753 loss: 2.3483641147613525\n",
      "epoch 3 batch 1754 loss: 2.4176292419433594\n",
      "epoch 3 batch 1755 loss: 2.5655219554901123\n",
      "epoch 3 batch 1756 loss: 2.3127965927124023\n",
      "epoch 3 batch 1757 loss: 2.5540122985839844\n",
      "epoch 3 batch 1758 loss: 2.274019241333008\n",
      "epoch 3 batch 1759 loss: 2.297287940979004\n",
      "epoch 3 batch 1760 loss: 2.342500686645508\n",
      "epoch 3 batch 1761 loss: 2.062675714492798\n",
      "epoch 3 batch 1762 loss: 2.4958815574645996\n",
      "epoch 3 batch 1763 loss: 2.5558624267578125\n",
      "epoch 3 batch 1764 loss: 2.683516025543213\n",
      "epoch 3 batch 1765 loss: 2.39188289642334\n",
      "epoch 3 batch 1766 loss: 2.752892255783081\n",
      "epoch 3 batch 1767 loss: 2.417557716369629\n",
      "epoch 3 batch 1768 loss: 2.7671308517456055\n",
      "epoch 3 batch 1769 loss: 2.4047553539276123\n",
      "epoch 3 batch 1770 loss: 2.1663241386413574\n",
      "epoch 3 batch 1771 loss: 2.329611301422119\n",
      "epoch 3 batch 1772 loss: 2.2530980110168457\n",
      "epoch 3 batch 1773 loss: 2.4838805198669434\n",
      "epoch 3 batch 1774 loss: 2.4078736305236816\n",
      "epoch 3 batch 1775 loss: 2.2397055625915527\n",
      "epoch 3 batch 1776 loss: 2.450380802154541\n",
      "epoch 3 batch 1777 loss: 2.1613245010375977\n",
      "epoch 3 batch 1778 loss: 2.4143173694610596\n",
      "epoch 3 batch 1779 loss: 2.2684078216552734\n",
      "epoch 3 batch 1780 loss: 2.2576441764831543\n",
      "epoch 3 batch 1781 loss: 2.4477758407592773\n",
      "epoch 3 batch 1782 loss: 2.280485153198242\n",
      "epoch 3 batch 1783 loss: 2.5488762855529785\n",
      "epoch 3 batch 1784 loss: 2.1585373878479004\n",
      "epoch 3 batch 1785 loss: 2.744330883026123\n",
      "epoch 3 batch 1786 loss: 2.3221659660339355\n",
      "epoch 3 batch 1787 loss: 2.39345645904541\n",
      "epoch 3 batch 1788 loss: 2.422919988632202\n",
      "epoch 3 batch 1789 loss: 2.5913898944854736\n",
      "epoch 3 batch 1790 loss: 2.3610806465148926\n",
      "epoch 3 batch 1791 loss: 2.3268203735351562\n",
      "epoch 3 batch 1792 loss: 2.6748924255371094\n",
      "epoch 3 batch 1793 loss: 2.3513646125793457\n",
      "epoch 3 batch 1794 loss: 2.5398762226104736\n",
      "epoch 3 batch 1795 loss: 2.4421257972717285\n",
      "epoch 3 batch 1796 loss: 2.550729513168335\n",
      "epoch 3 batch 1797 loss: 2.4185636043548584\n",
      "epoch 3 batch 1798 loss: 2.529670476913452\n",
      "epoch 3 batch 1799 loss: 2.2696800231933594\n",
      "epoch 3 batch 1800 loss: 2.562638759613037\n",
      "epoch 3 batch 1801 loss: 2.3583078384399414\n",
      "epoch 3 batch 1802 loss: 2.520653247833252\n",
      "epoch 3 batch 1803 loss: 2.4727602005004883\n",
      "epoch 3 batch 1804 loss: 2.3674120903015137\n",
      "epoch 3 batch 1805 loss: 2.491779327392578\n",
      "epoch 3 batch 1806 loss: 2.3032469749450684\n",
      "epoch 3 batch 1807 loss: 2.4926013946533203\n",
      "epoch 3 batch 1808 loss: 2.360050678253174\n",
      "epoch 3 batch 1809 loss: 2.410202980041504\n",
      "epoch 3 batch 1810 loss: 2.5684573650360107\n",
      "epoch 3 batch 1811 loss: 2.227184295654297\n",
      "epoch 3 batch 1812 loss: 2.1813974380493164\n",
      "epoch 3 batch 1813 loss: 2.5457115173339844\n",
      "epoch 3 batch 1814 loss: 2.3425426483154297\n",
      "epoch 3 batch 1815 loss: 2.271318197250366\n",
      "epoch 3 batch 1816 loss: 2.541220188140869\n",
      "epoch 3 batch 1817 loss: 2.3592443466186523\n",
      "epoch 3 batch 1818 loss: 2.1797566413879395\n",
      "epoch 3 batch 1819 loss: 2.233431816101074\n",
      "epoch 3 batch 1820 loss: 2.339238405227661\n",
      "epoch 3 batch 1821 loss: 2.460038185119629\n",
      "epoch 3 batch 1822 loss: 2.3239645957946777\n",
      "epoch 3 batch 1823 loss: 2.5592336654663086\n",
      "epoch 3 batch 1824 loss: 2.4935667514801025\n",
      "epoch 3 batch 1825 loss: 2.2138094902038574\n",
      "epoch 3 batch 1826 loss: 2.5678505897521973\n",
      "epoch 3 batch 1827 loss: 2.4255361557006836\n",
      "epoch 3 batch 1828 loss: 2.393087148666382\n",
      "epoch 3 batch 1829 loss: 2.5419387817382812\n",
      "epoch 3 batch 1830 loss: 2.6085729598999023\n",
      "epoch 3 batch 1831 loss: 2.1653871536254883\n",
      "epoch 3 batch 1832 loss: 2.50726056098938\n",
      "epoch 3 batch 1833 loss: 2.837972402572632\n",
      "epoch 3 batch 1834 loss: 2.3318848609924316\n",
      "epoch 3 batch 1835 loss: 2.4560775756835938\n",
      "epoch 3 batch 1836 loss: 2.3772573471069336\n",
      "epoch 3 batch 1837 loss: 2.597822427749634\n",
      "epoch 3 batch 1838 loss: 2.546532154083252\n",
      "epoch 3 batch 1839 loss: 2.4549312591552734\n",
      "epoch 3 batch 1840 loss: 2.2445755004882812\n",
      "epoch 3 batch 1841 loss: 2.4231810569763184\n",
      "epoch 3 batch 1842 loss: 2.3482861518859863\n",
      "epoch 3 batch 1843 loss: 2.454315423965454\n",
      "epoch 3 batch 1844 loss: 2.419344663619995\n",
      "epoch 3 batch 1845 loss: 2.504504919052124\n",
      "epoch 3 batch 1846 loss: 2.488156795501709\n",
      "epoch 3 batch 1847 loss: 2.479343891143799\n",
      "epoch 3 batch 1848 loss: 2.507838249206543\n",
      "epoch 3 batch 1849 loss: 2.5438222885131836\n",
      "epoch 3 batch 1850 loss: 2.3707876205444336\n",
      "epoch 3 batch 1851 loss: 2.532360553741455\n",
      "epoch 3 batch 1852 loss: 2.495941638946533\n",
      "epoch 3 batch 1853 loss: 2.4052674770355225\n",
      "epoch 3 batch 1854 loss: 2.653082847595215\n",
      "epoch 3 batch 1855 loss: 2.445699691772461\n",
      "epoch 3 batch 1856 loss: 2.502501964569092\n",
      "epoch 3 batch 1857 loss: 2.5477519035339355\n",
      "epoch 3 batch 1858 loss: 2.263707160949707\n",
      "epoch 3 batch 1859 loss: 2.6044020652770996\n",
      "epoch 3 batch 1860 loss: 2.114867925643921\n",
      "epoch 3 batch 1861 loss: 2.382305145263672\n",
      "epoch 3 batch 1862 loss: 2.606391429901123\n",
      "epoch 3 batch 1863 loss: 2.3557612895965576\n",
      "epoch 3 batch 1864 loss: 2.1673648357391357\n",
      "epoch 3 batch 1865 loss: 2.2961156368255615\n",
      "epoch 3 batch 1866 loss: 2.251469850540161\n",
      "epoch 3 batch 1867 loss: 2.314689874649048\n",
      "epoch 3 batch 1868 loss: 2.2701525688171387\n",
      "epoch 3 batch 1869 loss: 2.441514015197754\n",
      "epoch 3 batch 1870 loss: 2.396568536758423\n",
      "epoch 3 batch 1871 loss: 2.1529881954193115\n",
      "epoch 3 batch 1872 loss: 2.4889020919799805\n",
      "epoch 3 batch 1873 loss: 2.286510467529297\n",
      "epoch 3 batch 1874 loss: 2.460482597351074\n",
      "epoch 3 batch 1875 loss: 2.441606044769287\n",
      "epoch 3 batch 1876 loss: 2.410130500793457\n",
      "epoch 3 batch 1877 loss: 2.446249485015869\n",
      "epoch 3 batch 1878 loss: 2.3263959884643555\n",
      "epoch 3 batch 1879 loss: 2.2298946380615234\n",
      "epoch 3 batch 1880 loss: 2.4140796661376953\n",
      "epoch 3 batch 1881 loss: 2.5120105743408203\n",
      "epoch 3 batch 1882 loss: 2.1600890159606934\n",
      "epoch 3 batch 1883 loss: 2.4436709880828857\n",
      "epoch 3 batch 1884 loss: 2.6495261192321777\n",
      "epoch 3 batch 1885 loss: 2.227303981781006\n",
      "epoch 3 batch 1886 loss: 2.2624073028564453\n",
      "epoch 3 batch 1887 loss: 2.613966464996338\n",
      "epoch 3 batch 1888 loss: 2.2522573471069336\n",
      "epoch 3 batch 1889 loss: 2.574871063232422\n",
      "epoch 3 batch 1890 loss: 2.2273659706115723\n",
      "epoch 3 batch 1891 loss: 2.4586474895477295\n",
      "epoch 3 batch 1892 loss: 2.3646953105926514\n",
      "epoch 3 batch 1893 loss: 2.263890266418457\n",
      "epoch 3 batch 1894 loss: 2.159198760986328\n",
      "epoch 3 batch 1895 loss: 2.606826066970825\n",
      "epoch 3 batch 1896 loss: 2.376697063446045\n",
      "epoch 3 batch 1897 loss: 2.343348503112793\n",
      "epoch 3 batch 1898 loss: 2.572073221206665\n",
      "epoch 3 batch 1899 loss: 2.3669309616088867\n",
      "epoch 3 batch 1900 loss: 2.4343791007995605\n",
      "epoch 3 batch 1901 loss: 2.3975605964660645\n",
      "epoch 3 batch 1902 loss: 2.375309944152832\n",
      "epoch 3 batch 1903 loss: 2.2380762100219727\n",
      "epoch 3 batch 1904 loss: 2.390730381011963\n",
      "epoch 3 batch 1905 loss: 2.325242519378662\n",
      "epoch 3 batch 1906 loss: 2.3341143131256104\n",
      "epoch 3 batch 1907 loss: 2.62729549407959\n",
      "epoch 3 batch 1908 loss: 2.233569622039795\n",
      "epoch 3 batch 1909 loss: 2.330033779144287\n",
      "epoch 3 batch 1910 loss: 2.3340516090393066\n",
      "epoch 3 batch 1911 loss: 2.369630813598633\n",
      "epoch 3 batch 1912 loss: 2.1524155139923096\n",
      "epoch 3 batch 1913 loss: 2.3922924995422363\n",
      "epoch 3 batch 1914 loss: 2.2349812984466553\n",
      "epoch 3 batch 1915 loss: 2.144847869873047\n",
      "epoch 3 batch 1916 loss: 2.276521682739258\n",
      "epoch 3 batch 1917 loss: 2.6644716262817383\n",
      "epoch 3 batch 1918 loss: 2.3777928352355957\n",
      "epoch 3 batch 1919 loss: 2.526700019836426\n",
      "epoch 3 batch 1920 loss: 2.4254021644592285\n",
      "epoch 3 batch 1921 loss: 2.498441696166992\n",
      "epoch 3 batch 1922 loss: 2.294719934463501\n",
      "epoch 3 batch 1923 loss: 2.6129026412963867\n",
      "epoch 3 batch 1924 loss: 2.2485523223876953\n",
      "epoch 3 batch 1925 loss: 2.428863286972046\n",
      "epoch 3 batch 1926 loss: 2.3359458446502686\n",
      "epoch 3 batch 1927 loss: 2.4539966583251953\n",
      "epoch 3 batch 1928 loss: 2.426605463027954\n",
      "epoch 3 batch 1929 loss: 2.386479377746582\n",
      "epoch 3 batch 1930 loss: 2.1644842624664307\n",
      "epoch 3 batch 1931 loss: 2.499439239501953\n",
      "epoch 3 batch 1932 loss: 2.2357707023620605\n",
      "epoch 3 batch 1933 loss: 2.707674264907837\n",
      "epoch 3 batch 1934 loss: 2.5083959102630615\n",
      "epoch 3 batch 1935 loss: 2.837028741836548\n",
      "epoch 3 batch 1936 loss: 2.344736099243164\n",
      "epoch 3 batch 1937 loss: 2.902777671813965\n",
      "epoch 3 batch 1938 loss: 2.703559398651123\n",
      "epoch 3 batch 1939 loss: 2.5335943698883057\n",
      "epoch 3 batch 1940 loss: 2.5727651119232178\n",
      "epoch 3 batch 1941 loss: 2.222620725631714\n",
      "epoch 3 batch 1942 loss: 2.377600908279419\n",
      "epoch 3 batch 1943 loss: 2.390007972717285\n",
      "epoch 3 batch 1944 loss: 2.3288795948028564\n",
      "epoch 3 batch 1945 loss: 2.3701953887939453\n",
      "epoch 3 batch 1946 loss: 2.341543197631836\n",
      "epoch 3 batch 1947 loss: 2.415149450302124\n",
      "epoch 3 batch 1948 loss: 2.370910167694092\n",
      "epoch 3 batch 1949 loss: 2.570324420928955\n",
      "epoch 3 batch 1950 loss: 2.7076573371887207\n",
      "epoch 3 batch 1951 loss: 2.3757505416870117\n",
      "epoch 3 batch 1952 loss: 2.3923492431640625\n",
      "epoch 3 batch 1953 loss: 2.4835190773010254\n",
      "epoch 3 batch 1954 loss: 2.3573966026306152\n",
      "epoch 3 batch 1955 loss: 2.3299989700317383\n",
      "epoch 3 batch 1956 loss: 2.348878860473633\n",
      "epoch 3 batch 1957 loss: 2.3751678466796875\n",
      "epoch 3 batch 1958 loss: 2.410937786102295\n",
      "epoch 3 batch 1959 loss: 2.663458824157715\n",
      "epoch 3 batch 1960 loss: 2.575878858566284\n",
      "epoch 3 batch 1961 loss: 2.43595814704895\n",
      "epoch 3 batch 1962 loss: 2.3837523460388184\n",
      "epoch 3 batch 1963 loss: 2.5224781036376953\n",
      "epoch 3 batch 1964 loss: 2.448265552520752\n",
      "epoch 3 batch 1965 loss: 2.4291367530822754\n",
      "epoch 3 batch 1966 loss: 2.8385438919067383\n",
      "epoch 3 batch 1967 loss: 2.643955945968628\n",
      "epoch 3 batch 1968 loss: 2.4016926288604736\n",
      "epoch 3 batch 1969 loss: 2.3759613037109375\n",
      "epoch 3 batch 1970 loss: 2.2622499465942383\n",
      "epoch 3 batch 1971 loss: 2.3552544116973877\n",
      "epoch 3 batch 1972 loss: 2.2445602416992188\n",
      "epoch 3 batch 1973 loss: 2.5730695724487305\n",
      "epoch 3 batch 1974 loss: 2.4334592819213867\n",
      "epoch 3 batch 1975 loss: 2.6059834957122803\n",
      "epoch 3 batch 1976 loss: 2.49747896194458\n",
      "epoch 3 batch 1977 loss: 2.4139389991760254\n",
      "epoch 3 batch 1978 loss: 1.9866694211959839\n",
      "epoch 3 batch 1979 loss: 2.5006847381591797\n",
      "epoch 3 batch 1980 loss: 2.3991265296936035\n",
      "epoch 3 batch 1981 loss: 2.3651046752929688\n",
      "epoch 3 batch 1982 loss: 2.463977098464966\n",
      "epoch 3 batch 1983 loss: 2.4012866020202637\n",
      "epoch 3 batch 1984 loss: 2.229079246520996\n",
      "epoch 3 batch 1985 loss: 2.5908074378967285\n",
      "epoch 3 batch 1986 loss: 2.188789129257202\n",
      "epoch 3 batch 1987 loss: 2.660994529724121\n",
      "epoch 3 batch 1988 loss: 2.3989758491516113\n",
      "epoch 3 batch 1989 loss: 2.821086883544922\n",
      "epoch 3 batch 1990 loss: 2.3933606147766113\n",
      "epoch 3 batch 1991 loss: 2.0816216468811035\n",
      "epoch 3 batch 1992 loss: 2.6199791431427\n",
      "epoch 3 batch 1993 loss: 2.328627586364746\n",
      "epoch 3 batch 1994 loss: 2.5566959381103516\n",
      "epoch 3 batch 1995 loss: 2.3812484741210938\n",
      "epoch 3 batch 1996 loss: 2.333897590637207\n",
      "epoch 3 batch 1997 loss: 2.2475972175598145\n",
      "epoch 3 batch 1998 loss: 2.3039603233337402\n",
      "epoch 3 batch 1999 loss: 2.2301225662231445\n",
      "epoch 3 batch 2000 loss: 2.324465274810791\n",
      "epoch 3 batch 2001 loss: 2.3793370723724365\n",
      "epoch 3 batch 2002 loss: 2.725085735321045\n",
      "epoch 3 batch 2003 loss: 2.655447483062744\n",
      "epoch 3 batch 2004 loss: 2.183897018432617\n",
      "epoch 3 batch 2005 loss: 2.2970950603485107\n",
      "epoch 3 batch 2006 loss: 2.4718663692474365\n",
      "epoch 3 batch 2007 loss: 2.1515355110168457\n",
      "epoch 3 batch 2008 loss: 2.399076461791992\n",
      "epoch 3 batch 2009 loss: 2.2644097805023193\n",
      "epoch 3 batch 2010 loss: 2.2848899364471436\n",
      "epoch 3 batch 2011 loss: 2.369779109954834\n",
      "epoch 3 batch 2012 loss: 2.206953525543213\n",
      "epoch 3 batch 2013 loss: 2.371504783630371\n",
      "epoch 3 batch 2014 loss: 2.2390973567962646\n",
      "epoch 3 batch 2015 loss: 2.3881680965423584\n",
      "epoch 3 batch 2016 loss: 2.6031336784362793\n",
      "epoch 3 batch 2017 loss: 2.439880132675171\n",
      "epoch 3 batch 2018 loss: 2.343970537185669\n",
      "epoch 3 batch 2019 loss: 2.5672812461853027\n",
      "epoch 3 batch 2020 loss: 2.259805679321289\n",
      "epoch 3 batch 2021 loss: 2.430650234222412\n",
      "epoch 3 batch 2022 loss: 2.448662757873535\n",
      "epoch 3 batch 2023 loss: 2.6435585021972656\n",
      "epoch 3 batch 2024 loss: 2.4423224925994873\n",
      "epoch 3 batch 2025 loss: 2.452052593231201\n",
      "epoch 3 batch 2026 loss: 2.1779422760009766\n",
      "epoch 3 batch 2027 loss: 2.410170078277588\n",
      "epoch 3 batch 2028 loss: 2.3085527420043945\n",
      "epoch 3 batch 2029 loss: 2.3972508907318115\n",
      "epoch 3 batch 2030 loss: 2.540081024169922\n",
      "epoch 3 batch 2031 loss: 2.540283203125\n",
      "epoch 3 batch 2032 loss: 2.2678539752960205\n",
      "epoch 3 batch 2033 loss: 2.4537556171417236\n",
      "epoch 3 batch 2034 loss: 2.203871250152588\n",
      "epoch 3 batch 2035 loss: 2.2547805309295654\n",
      "epoch 3 batch 2036 loss: 2.237628936767578\n",
      "epoch 3 batch 2037 loss: 2.29828143119812\n",
      "epoch 3 batch 2038 loss: 2.5768017768859863\n",
      "epoch 3 batch 2039 loss: 2.2284486293792725\n",
      "epoch 3 batch 2040 loss: 2.2429375648498535\n",
      "epoch 3 batch 2041 loss: 2.441422939300537\n",
      "epoch 3 batch 2042 loss: 2.211759090423584\n",
      "epoch 3 batch 2043 loss: 2.1936726570129395\n",
      "epoch 3 batch 2044 loss: 2.312244176864624\n",
      "epoch 3 batch 2045 loss: 2.475337505340576\n",
      "epoch 3 batch 2046 loss: 2.4168925285339355\n",
      "epoch 3 batch 2047 loss: 2.513127088546753\n",
      "epoch 3 batch 2048 loss: 2.1819005012512207\n",
      "epoch 3 batch 2049 loss: 2.4696459770202637\n",
      "epoch 3 batch 2050 loss: 2.4867520332336426\n",
      "epoch 3 batch 2051 loss: 2.3228964805603027\n",
      "epoch 3 batch 2052 loss: 2.3622350692749023\n",
      "epoch 3 batch 2053 loss: 2.5287435054779053\n",
      "epoch 3 batch 2054 loss: 2.5176522731781006\n",
      "epoch 3 batch 2055 loss: 2.69033145904541\n",
      "epoch 3 batch 2056 loss: 2.394679069519043\n",
      "epoch 3 batch 2057 loss: 2.5663411617279053\n",
      "epoch 3 batch 2058 loss: 2.3730788230895996\n",
      "epoch 3 batch 2059 loss: 2.1420092582702637\n",
      "epoch 3 batch 2060 loss: 2.111877202987671\n",
      "epoch 3 batch 2061 loss: 2.440122604370117\n",
      "epoch 3 batch 2062 loss: 2.509716510772705\n",
      "epoch 3 batch 2063 loss: 2.564326286315918\n",
      "epoch 3 batch 2064 loss: 2.3935904502868652\n",
      "epoch 3 batch 2065 loss: 2.412196159362793\n",
      "epoch 3 batch 2066 loss: 2.269221782684326\n",
      "epoch 3 batch 2067 loss: 2.5178604125976562\n",
      "epoch 3 batch 2068 loss: 2.4942383766174316\n",
      "epoch 3 batch 2069 loss: 2.3652448654174805\n",
      "epoch 3 batch 2070 loss: 2.380913257598877\n",
      "epoch 3 batch 2071 loss: 2.5167253017425537\n",
      "epoch 3 batch 2072 loss: 2.1756489276885986\n",
      "epoch 3 batch 2073 loss: 2.428832530975342\n",
      "epoch 3 batch 2074 loss: 2.2672743797302246\n",
      "epoch 3 batch 2075 loss: 2.3805127143859863\n",
      "epoch 3 batch 2076 loss: 2.2240819931030273\n",
      "epoch 3 batch 2077 loss: 2.2608795166015625\n",
      "epoch 3 batch 2078 loss: 2.8115196228027344\n",
      "epoch 3 batch 2079 loss: 2.4168224334716797\n",
      "epoch 3 batch 2080 loss: 2.473052501678467\n",
      "epoch 3 batch 2081 loss: 2.229612350463867\n",
      "epoch 3 batch 2082 loss: 2.451322555541992\n",
      "epoch 3 batch 2083 loss: 2.443222999572754\n",
      "epoch 3 batch 2084 loss: 2.328711986541748\n",
      "epoch 3 batch 2085 loss: 2.3901724815368652\n",
      "epoch 3 batch 2086 loss: 2.2541134357452393\n",
      "epoch 3 batch 2087 loss: 2.2967329025268555\n",
      "epoch 3 batch 2088 loss: 2.467801570892334\n",
      "epoch 3 batch 2089 loss: 2.2609193325042725\n",
      "epoch 3 batch 2090 loss: 2.1922032833099365\n",
      "epoch 3 batch 2091 loss: 2.1196694374084473\n",
      "epoch 3 batch 2092 loss: 2.256714344024658\n",
      "epoch 3 batch 2093 loss: 2.3646600246429443\n",
      "epoch 3 batch 2094 loss: 2.4418203830718994\n",
      "epoch 3 batch 2095 loss: 2.2277002334594727\n",
      "epoch 3 batch 2096 loss: 2.298086166381836\n",
      "epoch 3 batch 2097 loss: 2.1144065856933594\n",
      "epoch 3 batch 2098 loss: 2.6006031036376953\n",
      "epoch 3 batch 2099 loss: 2.370772361755371\n",
      "epoch 3 batch 2100 loss: 2.517824172973633\n",
      "epoch 3 batch 2101 loss: 2.605954170227051\n",
      "epoch 3 batch 2102 loss: 2.4856364727020264\n",
      "epoch 3 batch 2103 loss: 2.3549184799194336\n",
      "epoch 3 batch 2104 loss: 2.479116439819336\n",
      "epoch 3 batch 2105 loss: 2.3491506576538086\n",
      "epoch 3 batch 2106 loss: 2.398951768875122\n",
      "epoch 3 batch 2107 loss: 2.206696033477783\n",
      "epoch 3 batch 2108 loss: 2.757979393005371\n",
      "epoch 3 batch 2109 loss: 2.4163107872009277\n",
      "epoch 3 batch 2110 loss: 2.4902663230895996\n",
      "epoch 3 batch 2111 loss: 2.174628257751465\n",
      "epoch 3 batch 2112 loss: 2.7117648124694824\n",
      "epoch 3 batch 2113 loss: 2.4438817501068115\n",
      "epoch 3 batch 2114 loss: 2.4270737171173096\n",
      "epoch 3 batch 2115 loss: 2.5944933891296387\n",
      "epoch 3 batch 2116 loss: 2.4393301010131836\n",
      "epoch 3 batch 2117 loss: 2.4241037368774414\n",
      "epoch 3 batch 2118 loss: 2.374755382537842\n",
      "epoch 3 batch 2119 loss: 2.3505210876464844\n",
      "epoch 3 batch 2120 loss: 2.4290409088134766\n",
      "epoch 3 batch 2121 loss: 2.476785659790039\n",
      "epoch 3 batch 2122 loss: 2.352665424346924\n",
      "epoch 3 batch 2123 loss: 2.4251973628997803\n",
      "epoch 3 batch 2124 loss: 2.3748106956481934\n",
      "epoch 3 batch 2125 loss: 2.3203999996185303\n",
      "epoch 3 batch 2126 loss: 2.2472915649414062\n",
      "epoch 3 batch 2127 loss: 2.5840773582458496\n",
      "epoch 3 batch 2128 loss: 2.5262532234191895\n",
      "epoch 3 batch 2129 loss: 2.4354498386383057\n",
      "epoch 3 batch 2130 loss: 2.6539952754974365\n",
      "epoch 3 batch 2131 loss: 2.3659780025482178\n",
      "epoch 3 batch 2132 loss: 2.2456085681915283\n",
      "epoch 3 batch 2133 loss: 2.5020554065704346\n",
      "epoch 3 batch 2134 loss: 2.554168224334717\n",
      "epoch 3 batch 2135 loss: 2.158832311630249\n",
      "epoch 3 batch 2136 loss: 2.4571516513824463\n",
      "epoch 3 batch 2137 loss: 2.2705283164978027\n",
      "epoch 3 batch 2138 loss: 2.300748586654663\n",
      "epoch 3 batch 2139 loss: 2.3216445446014404\n",
      "epoch 3 batch 2140 loss: 2.4559879302978516\n",
      "epoch 3 batch 2141 loss: 2.3091650009155273\n",
      "epoch 3 batch 2142 loss: 2.279186248779297\n",
      "epoch 3 batch 2143 loss: 2.5152649879455566\n",
      "epoch 3 batch 2144 loss: 2.4237256050109863\n",
      "epoch 3 batch 2145 loss: 2.215553045272827\n",
      "epoch 3 batch 2146 loss: 2.2371578216552734\n",
      "epoch 3 batch 2147 loss: 2.424747943878174\n",
      "epoch 3 batch 2148 loss: 2.4331092834472656\n",
      "epoch 3 batch 2149 loss: 2.4762685298919678\n",
      "epoch 3 batch 2150 loss: 2.314107894897461\n",
      "epoch 3 batch 2151 loss: 2.7434144020080566\n",
      "epoch 3 batch 2152 loss: 2.529064893722534\n",
      "epoch 3 batch 2153 loss: 2.5014004707336426\n",
      "epoch 3 batch 2154 loss: 2.450003147125244\n",
      "epoch 3 batch 2155 loss: 2.2959253787994385\n",
      "epoch 3 batch 2156 loss: 2.295513391494751\n",
      "epoch 3 batch 2157 loss: 2.6680102348327637\n",
      "epoch 3 batch 2158 loss: 2.2599575519561768\n",
      "epoch 3 batch 2159 loss: 2.2215285301208496\n",
      "epoch 3 batch 2160 loss: 2.2648091316223145\n",
      "epoch 3 batch 2161 loss: 2.3598384857177734\n",
      "epoch 3 batch 2162 loss: 2.4025750160217285\n",
      "epoch 3 batch 2163 loss: 2.658731460571289\n",
      "epoch 3 batch 2164 loss: 2.568787097930908\n",
      "epoch 3 batch 2165 loss: 2.3954763412475586\n",
      "epoch 3 batch 2166 loss: 2.4272027015686035\n",
      "epoch 3 batch 2167 loss: 2.4699203968048096\n",
      "epoch 3 batch 2168 loss: 2.3495445251464844\n",
      "epoch 3 batch 2169 loss: 2.5215959548950195\n",
      "epoch 3 batch 2170 loss: 2.316087484359741\n",
      "epoch 3 batch 2171 loss: 2.3484439849853516\n",
      "epoch 3 batch 2172 loss: 2.684614419937134\n",
      "epoch 3 batch 2173 loss: 2.3787989616394043\n",
      "epoch 3 batch 2174 loss: 2.295790433883667\n",
      "epoch 3 batch 2175 loss: 2.3178577423095703\n",
      "epoch 3 batch 2176 loss: 2.3659653663635254\n",
      "epoch 3 batch 2177 loss: 2.4094696044921875\n",
      "epoch 3 batch 2178 loss: 2.5211493968963623\n",
      "epoch 3 batch 2179 loss: 2.2246408462524414\n",
      "epoch 3 batch 2180 loss: 2.4928817749023438\n",
      "epoch 3 batch 2181 loss: 2.2746481895446777\n",
      "epoch 3 batch 2182 loss: 2.238011598587036\n",
      "epoch 3 batch 2183 loss: 2.324679136276245\n",
      "epoch 3 batch 2184 loss: 2.3233144283294678\n",
      "epoch 3 batch 2185 loss: 2.622864246368408\n",
      "epoch 3 batch 2186 loss: 2.5534443855285645\n",
      "epoch 3 batch 2187 loss: 2.3027775287628174\n",
      "epoch 3 batch 2188 loss: 2.434053421020508\n",
      "epoch 3 batch 2189 loss: 2.5188674926757812\n",
      "epoch 3 batch 2190 loss: 2.538605213165283\n",
      "epoch 3 batch 2191 loss: 2.3603343963623047\n",
      "epoch 3 batch 2192 loss: 2.1293983459472656\n",
      "epoch 3 batch 2193 loss: 2.6007983684539795\n",
      "epoch 3 batch 2194 loss: 2.584822177886963\n",
      "epoch 3 batch 2195 loss: 2.433535099029541\n",
      "epoch 3 batch 2196 loss: 2.415292263031006\n",
      "epoch 3 batch 2197 loss: 2.3349223136901855\n",
      "epoch 3 batch 2198 loss: 2.4407949447631836\n",
      "epoch 3 batch 2199 loss: 2.3687057495117188\n",
      "epoch 3 batch 2200 loss: 2.4367411136627197\n",
      "epoch 3 batch 2201 loss: 2.300612211227417\n",
      "epoch 3 batch 2202 loss: 2.3998022079467773\n",
      "epoch 3 batch 2203 loss: 2.2103590965270996\n",
      "epoch 3 batch 2204 loss: 2.4205281734466553\n",
      "epoch 3 batch 2205 loss: 2.500495195388794\n",
      "epoch 3 batch 2206 loss: 2.2040669918060303\n",
      "epoch 3 batch 2207 loss: 2.6010303497314453\n",
      "epoch 3 batch 2208 loss: 2.368328332901001\n",
      "epoch 3 batch 2209 loss: 2.373744010925293\n",
      "epoch 3 batch 2210 loss: 2.2892520427703857\n",
      "epoch 3 batch 2211 loss: 2.635845899581909\n",
      "epoch 3 batch 2212 loss: 2.36376953125\n",
      "epoch 3 batch 2213 loss: 2.8528175354003906\n",
      "epoch 3 batch 2214 loss: 2.283139228820801\n",
      "epoch 3 batch 2215 loss: 2.2556264400482178\n",
      "epoch 3 batch 2216 loss: 2.5889854431152344\n",
      "epoch 3 batch 2217 loss: 2.622098684310913\n",
      "epoch 3 batch 2218 loss: 2.4040417671203613\n",
      "epoch 3 batch 2219 loss: 2.2766966819763184\n",
      "epoch 3 batch 2220 loss: 2.3470160961151123\n",
      "epoch 3 batch 2221 loss: 2.4098100662231445\n",
      "epoch 3 batch 2222 loss: 2.2114980220794678\n",
      "epoch 3 batch 2223 loss: 2.177671194076538\n",
      "epoch 3 batch 2224 loss: 2.499189615249634\n",
      "epoch 3 batch 2225 loss: 2.322122573852539\n",
      "epoch 3 batch 2226 loss: 2.3553476333618164\n",
      "epoch 3 batch 2227 loss: 2.258397102355957\n",
      "epoch 3 batch 2228 loss: 2.2458367347717285\n",
      "epoch 3 batch 2229 loss: 2.256225824356079\n",
      "epoch 3 batch 2230 loss: 2.1887948513031006\n",
      "epoch 3 batch 2231 loss: 2.1574058532714844\n",
      "epoch 3 batch 2232 loss: 2.253614902496338\n",
      "epoch 3 batch 2233 loss: 2.4054081439971924\n",
      "epoch 3 batch 2234 loss: 2.339120388031006\n",
      "epoch 3 batch 2235 loss: 2.2997515201568604\n",
      "epoch 3 batch 2236 loss: 2.5719826221466064\n",
      "epoch 3 batch 2237 loss: 2.204334259033203\n",
      "epoch 3 batch 2238 loss: 2.4367315769195557\n",
      "epoch 3 batch 2239 loss: 2.5522921085357666\n",
      "epoch 3 batch 2240 loss: 2.481292247772217\n",
      "epoch 3 batch 2241 loss: 2.4937589168548584\n",
      "epoch 3 batch 2242 loss: 2.5566940307617188\n",
      "epoch 3 batch 2243 loss: 2.389261484146118\n",
      "epoch 3 batch 2244 loss: 2.4530746936798096\n",
      "epoch 3 batch 2245 loss: 2.397446393966675\n",
      "epoch 3 batch 2246 loss: 2.431732654571533\n",
      "epoch 3 batch 2247 loss: 2.6920652389526367\n",
      "epoch 3 batch 2248 loss: 2.1076807975769043\n",
      "epoch 3 batch 2249 loss: 2.463747501373291\n",
      "epoch 3 batch 2250 loss: 2.2816340923309326\n",
      "epoch 3 batch 2251 loss: 2.2908730506896973\n",
      "epoch 3 batch 2252 loss: 2.445185422897339\n",
      "epoch 3 batch 2253 loss: 2.408816337585449\n",
      "epoch 3 batch 2254 loss: 2.386152982711792\n",
      "epoch 3 batch 2255 loss: 2.462970733642578\n",
      "epoch 3 batch 2256 loss: 2.5451500415802\n",
      "epoch 3 batch 2257 loss: 2.4338364601135254\n",
      "epoch 3 batch 2258 loss: 2.2879624366760254\n",
      "epoch 3 batch 2259 loss: 2.3397696018218994\n",
      "epoch 3 batch 2260 loss: 2.633930206298828\n",
      "epoch 3 batch 2261 loss: 2.3970956802368164\n",
      "epoch 3 batch 2262 loss: 2.2099673748016357\n",
      "epoch 3 batch 2263 loss: 2.5504701137542725\n",
      "epoch 3 batch 2264 loss: 2.4622814655303955\n",
      "epoch 3 batch 2265 loss: 2.526226043701172\n",
      "epoch 3 batch 2266 loss: 2.2493605613708496\n",
      "epoch 3 batch 2267 loss: 2.3872599601745605\n",
      "epoch 3 batch 2268 loss: 2.4056668281555176\n",
      "epoch 3 batch 2269 loss: 2.3617966175079346\n",
      "epoch 3 batch 2270 loss: 2.3070068359375\n",
      "epoch 3 batch 2271 loss: 2.282794713973999\n",
      "epoch 3 batch 2272 loss: 2.4023184776306152\n",
      "epoch 3 batch 2273 loss: 2.311037063598633\n",
      "epoch 3 batch 2274 loss: 2.2893009185791016\n",
      "epoch 3 batch 2275 loss: 2.4084486961364746\n",
      "epoch 3 batch 2276 loss: 2.4564805030822754\n",
      "epoch 3 batch 2277 loss: 2.499448776245117\n",
      "epoch 3 batch 2278 loss: 2.272495985031128\n",
      "epoch 3 batch 2279 loss: 2.352964401245117\n",
      "epoch 3 batch 2280 loss: 2.5305733680725098\n",
      "epoch 3 batch 2281 loss: 2.6930036544799805\n",
      "epoch 3 batch 2282 loss: 2.45288348197937\n",
      "epoch 3 batch 2283 loss: 2.392312526702881\n",
      "epoch 3 batch 2284 loss: 2.343879222869873\n",
      "epoch 3 batch 2285 loss: 2.3933558464050293\n",
      "epoch 3 batch 2286 loss: 2.2444896697998047\n",
      "epoch 3 batch 2287 loss: 2.4597344398498535\n",
      "epoch 3 batch 2288 loss: 2.5850884914398193\n",
      "epoch 3 batch 2289 loss: 2.6624581813812256\n",
      "epoch 3 batch 2290 loss: 2.3226537704467773\n",
      "epoch 3 batch 2291 loss: 2.8333230018615723\n",
      "epoch 3 batch 2292 loss: 2.5101704597473145\n",
      "epoch 3 batch 2293 loss: 2.3961801528930664\n",
      "epoch 3 batch 2294 loss: 2.506197929382324\n",
      "epoch 3 batch 2295 loss: 2.2822036743164062\n",
      "epoch 3 batch 2296 loss: 2.373605728149414\n",
      "epoch 3 batch 2297 loss: 2.6821725368499756\n",
      "epoch 3 batch 2298 loss: 2.3412210941314697\n",
      "epoch 3 batch 2299 loss: 2.1258037090301514\n",
      "epoch 3 batch 2300 loss: 2.1633760929107666\n",
      "epoch 3 batch 2301 loss: 2.521226406097412\n",
      "epoch 3 batch 2302 loss: 2.2967891693115234\n",
      "epoch 3 batch 2303 loss: 2.7304224967956543\n",
      "epoch 3 batch 2304 loss: 2.2457802295684814\n",
      "epoch 3 batch 2305 loss: 2.5262231826782227\n",
      "epoch 3 batch 2306 loss: 2.205080509185791\n",
      "epoch 3 batch 2307 loss: 2.1368093490600586\n",
      "epoch 3 batch 2308 loss: 2.307128429412842\n",
      "epoch 3 batch 2309 loss: 2.3905866146087646\n",
      "epoch 3 batch 2310 loss: 2.5799970626831055\n",
      "epoch 3 batch 2311 loss: 2.4806575775146484\n",
      "epoch 3 batch 2312 loss: 2.290764570236206\n",
      "epoch 3 batch 2313 loss: 2.3011410236358643\n",
      "epoch 3 batch 2314 loss: 2.4979963302612305\n",
      "epoch 3 batch 2315 loss: 2.3136699199676514\n",
      "epoch 3 batch 2316 loss: 2.284377336502075\n",
      "epoch 3 batch 2317 loss: 2.505319118499756\n",
      "epoch 3 batch 2318 loss: 2.46551513671875\n",
      "epoch 3 batch 2319 loss: 2.2587950229644775\n",
      "epoch 3 batch 2320 loss: 2.2880730628967285\n",
      "epoch 3 batch 2321 loss: 2.534019947052002\n",
      "epoch 3 batch 2322 loss: 2.5537686347961426\n",
      "epoch 3 batch 2323 loss: 2.4984099864959717\n",
      "epoch 3 batch 2324 loss: 2.3139610290527344\n",
      "epoch 3 batch 2325 loss: 2.878458023071289\n",
      "epoch 3 batch 2326 loss: 2.156050205230713\n",
      "epoch 3 batch 2327 loss: 2.5261454582214355\n",
      "epoch 3 batch 2328 loss: 2.2224512100219727\n",
      "epoch 3 batch 2329 loss: 2.451793670654297\n",
      "epoch 3 batch 2330 loss: 2.5453314781188965\n",
      "epoch 3 batch 2331 loss: 2.1866068840026855\n",
      "epoch 3 batch 2332 loss: 2.269221782684326\n",
      "epoch 3 batch 2333 loss: 2.5168063640594482\n",
      "epoch 3 batch 2334 loss: 2.4175143241882324\n",
      "epoch 3 batch 2335 loss: 2.3720946311950684\n",
      "epoch 3 batch 2336 loss: 2.546365261077881\n",
      "epoch 3 batch 2337 loss: 2.343123197555542\n",
      "epoch 3 batch 2338 loss: 2.4947338104248047\n",
      "epoch 3 batch 2339 loss: 2.311553478240967\n",
      "epoch 3 batch 2340 loss: 2.329054832458496\n",
      "epoch 3 batch 2341 loss: 2.4503633975982666\n",
      "epoch 3 batch 2342 loss: 2.3411660194396973\n",
      "epoch 3 batch 2343 loss: 2.3141891956329346\n",
      "epoch 3 batch 2344 loss: 2.2606277465820312\n",
      "epoch 3 batch 2345 loss: 2.2979822158813477\n",
      "epoch 3 batch 2346 loss: 2.382354736328125\n",
      "epoch 3 batch 2347 loss: 2.2014365196228027\n",
      "epoch 3 batch 2348 loss: 2.2105188369750977\n",
      "epoch 3 batch 2349 loss: 2.3521032333374023\n",
      "epoch 3 batch 2350 loss: 2.199906349182129\n",
      "epoch 3 batch 2351 loss: 2.2428226470947266\n",
      "epoch 3 batch 2352 loss: 2.5553267002105713\n",
      "epoch 3 batch 2353 loss: 2.357422113418579\n",
      "epoch 3 batch 2354 loss: 2.6981048583984375\n",
      "epoch 3 batch 2355 loss: 2.2894418239593506\n",
      "epoch 3 batch 2356 loss: 2.2069714069366455\n",
      "epoch 3 batch 2357 loss: 2.3783018589019775\n",
      "epoch 3 batch 2358 loss: 2.4079079627990723\n",
      "epoch 3 batch 2359 loss: 2.3865175247192383\n",
      "epoch 3 batch 2360 loss: 2.7455215454101562\n",
      "epoch 3 batch 2361 loss: 2.2262122631073\n",
      "epoch 3 batch 2362 loss: 2.5459213256835938\n",
      "epoch 3 batch 2363 loss: 2.6106762886047363\n",
      "epoch 3 batch 2364 loss: 2.3041067123413086\n",
      "epoch 3 batch 2365 loss: 2.3514089584350586\n",
      "epoch 3 batch 2366 loss: 2.2468981742858887\n",
      "epoch 3 batch 2367 loss: 2.3061842918395996\n",
      "epoch 3 batch 2368 loss: 2.4108099937438965\n",
      "epoch 3 batch 2369 loss: 2.400681972503662\n",
      "epoch 3 batch 2370 loss: 2.226983070373535\n",
      "epoch 3 batch 2371 loss: 2.7682652473449707\n",
      "epoch 3 batch 2372 loss: 2.4379830360412598\n",
      "epoch 3 batch 2373 loss: 2.245457649230957\n",
      "epoch 3 batch 2374 loss: 2.2829809188842773\n",
      "epoch 3 batch 2375 loss: 2.4305195808410645\n",
      "epoch 3 batch 2376 loss: 2.3381779193878174\n",
      "epoch 3 batch 2377 loss: 2.2650575637817383\n",
      "epoch 3 batch 2378 loss: 2.2253577709198\n",
      "epoch 3 batch 2379 loss: 2.2812135219573975\n",
      "epoch 3 batch 2380 loss: 2.2013745307922363\n",
      "epoch 3 batch 2381 loss: 2.338383197784424\n",
      "epoch 3 batch 2382 loss: 2.4431324005126953\n",
      "epoch 3 batch 2383 loss: 2.375356674194336\n",
      "epoch 3 batch 2384 loss: 2.195120334625244\n",
      "epoch 3 batch 2385 loss: 2.1206369400024414\n",
      "epoch 3 batch 2386 loss: 2.348639488220215\n",
      "epoch 3 batch 2387 loss: 2.715935707092285\n",
      "epoch 3 batch 2388 loss: 2.5799641609191895\n",
      "epoch 3 batch 2389 loss: 2.4993057250976562\n",
      "epoch 3 batch 2390 loss: 2.270359992980957\n",
      "epoch 3 batch 2391 loss: 2.4187381267547607\n",
      "epoch 3 batch 2392 loss: 2.332451343536377\n",
      "epoch 3 batch 2393 loss: 2.3894824981689453\n",
      "epoch 3 batch 2394 loss: 2.4845499992370605\n",
      "epoch 3 batch 2395 loss: 2.553231954574585\n",
      "epoch 3 batch 2396 loss: 2.6214447021484375\n",
      "epoch 3 batch 2397 loss: 2.244152069091797\n",
      "epoch 3 batch 2398 loss: 2.129852294921875\n",
      "epoch 3 batch 2399 loss: 2.5405960083007812\n",
      "epoch 3 batch 2400 loss: 2.4721570014953613\n",
      "epoch 3 batch 2401 loss: 2.381289482116699\n",
      "epoch 3 batch 2402 loss: 2.595823049545288\n",
      "epoch 3 batch 2403 loss: 2.3391239643096924\n",
      "epoch 3 batch 2404 loss: 2.3803110122680664\n",
      "epoch 3 batch 2405 loss: 2.496182441711426\n",
      "epoch 3 batch 2406 loss: 2.3295459747314453\n",
      "epoch 3 batch 2407 loss: 2.5729355812072754\n",
      "epoch 3 batch 2408 loss: 2.798433780670166\n",
      "epoch 3 batch 2409 loss: 2.242846727371216\n",
      "epoch 3 batch 2410 loss: 2.339289903640747\n",
      "epoch 3 batch 2411 loss: 2.529796838760376\n",
      "epoch 3 batch 2412 loss: 2.3126344680786133\n",
      "epoch 3 batch 2413 loss: 2.575490951538086\n",
      "epoch 3 batch 2414 loss: 2.4884095191955566\n",
      "epoch 3 batch 2415 loss: 2.482311725616455\n",
      "epoch 3 batch 2416 loss: 2.816739559173584\n",
      "epoch 3 batch 2417 loss: 2.3161780834198\n",
      "epoch 3 batch 2418 loss: 2.0229873657226562\n",
      "epoch 3 batch 2419 loss: 2.3294131755828857\n",
      "epoch 3 batch 2420 loss: 2.4852561950683594\n",
      "epoch 3 batch 2421 loss: 2.3453238010406494\n",
      "epoch 3 batch 2422 loss: 2.5467453002929688\n",
      "epoch 3 batch 2423 loss: 2.4898788928985596\n",
      "epoch 3 batch 2424 loss: 2.574409008026123\n",
      "epoch 3 batch 2425 loss: 2.366708278656006\n",
      "epoch 3 batch 2426 loss: 2.534472942352295\n",
      "epoch 3 batch 2427 loss: 2.1779942512512207\n",
      "epoch 3 batch 2428 loss: 2.3376171588897705\n",
      "epoch 3 batch 2429 loss: 2.433708667755127\n",
      "epoch 3 batch 2430 loss: 2.428157329559326\n",
      "epoch 3 batch 2431 loss: 2.2845702171325684\n",
      "epoch 3 batch 2432 loss: 2.2069921493530273\n",
      "epoch 3 batch 2433 loss: 2.0866219997406006\n",
      "epoch 3 batch 2434 loss: 2.6002862453460693\n",
      "epoch 3 batch 2435 loss: 2.124988079071045\n",
      "epoch 3 batch 2436 loss: 2.711360216140747\n",
      "epoch 3 batch 2437 loss: 2.14935564994812\n",
      "epoch 3 batch 2438 loss: 2.3480000495910645\n",
      "epoch 3 batch 2439 loss: 2.2658767700195312\n",
      "epoch 3 batch 2440 loss: 2.3531088829040527\n",
      "epoch 3 batch 2441 loss: 2.2565956115722656\n",
      "epoch 3 batch 2442 loss: 2.399895668029785\n",
      "epoch 3 batch 2443 loss: 2.343111991882324\n",
      "epoch 3 batch 2444 loss: 2.6339197158813477\n",
      "epoch 3 batch 2445 loss: 2.3846628665924072\n",
      "epoch 3 batch 2446 loss: 2.5233373641967773\n",
      "epoch 3 batch 2447 loss: 2.579639196395874\n",
      "epoch 3 batch 2448 loss: 2.7560646533966064\n",
      "epoch 3 batch 2449 loss: 2.2968881130218506\n",
      "epoch 3 batch 2450 loss: 2.3882062435150146\n",
      "epoch 3 batch 2451 loss: 2.398366689682007\n",
      "epoch 3 batch 2452 loss: 2.5491719245910645\n",
      "epoch 3 batch 2453 loss: 2.4307007789611816\n",
      "epoch 3 batch 2454 loss: 2.1393837928771973\n",
      "epoch 3 batch 2455 loss: 2.442605972290039\n",
      "epoch 3 batch 2456 loss: 2.503227949142456\n",
      "epoch 3 batch 2457 loss: 2.6126294136047363\n",
      "epoch 3 batch 2458 loss: 2.244532346725464\n",
      "epoch 3 batch 2459 loss: 2.3262147903442383\n",
      "epoch 3 batch 2460 loss: 2.507075309753418\n",
      "epoch 3 batch 2461 loss: 2.263294219970703\n",
      "epoch 3 batch 2462 loss: 2.527147054672241\n",
      "epoch 3 batch 2463 loss: 2.4257771968841553\n",
      "epoch 3 batch 2464 loss: 2.2589564323425293\n",
      "epoch 3 batch 2465 loss: 2.1589841842651367\n",
      "epoch 3 batch 2466 loss: 2.2435927391052246\n",
      "epoch 3 batch 2467 loss: 2.4131174087524414\n",
      "epoch 3 batch 2468 loss: 2.8274238109588623\n",
      "epoch 3 batch 2469 loss: 2.647355079650879\n",
      "epoch 3 batch 2470 loss: 2.3585658073425293\n",
      "epoch 3 batch 2471 loss: 2.30544114112854\n",
      "epoch 3 batch 2472 loss: 2.30645489692688\n",
      "epoch 3 batch 2473 loss: 2.3007211685180664\n",
      "epoch 3 batch 2474 loss: 2.279106616973877\n",
      "epoch 3 batch 2475 loss: 2.162142038345337\n",
      "epoch 3 batch 2476 loss: 2.214784622192383\n",
      "epoch 3 batch 2477 loss: 2.1262400150299072\n",
      "epoch 3 batch 2478 loss: 2.372403383255005\n",
      "epoch 3 batch 2479 loss: 2.457418918609619\n",
      "epoch 3 batch 2480 loss: 2.228062152862549\n",
      "epoch 3 batch 2481 loss: 2.4174859523773193\n",
      "epoch 3 batch 2482 loss: 2.2092387676239014\n",
      "epoch 3 batch 2483 loss: 2.200925350189209\n",
      "epoch 3 batch 2484 loss: 2.254401683807373\n",
      "epoch 3 batch 2485 loss: 2.2082254886627197\n",
      "epoch 3 batch 2486 loss: 2.2960917949676514\n",
      "epoch 3 batch 2487 loss: 2.1555209159851074\n",
      "epoch 3 batch 2488 loss: 2.244380474090576\n",
      "epoch 3 batch 2489 loss: 2.2235069274902344\n",
      "epoch 3 batch 2490 loss: 2.2979469299316406\n",
      "epoch 3 batch 2491 loss: 2.409965753555298\n",
      "epoch 3 batch 2492 loss: 2.5709757804870605\n",
      "epoch 3 batch 2493 loss: 2.364612102508545\n",
      "epoch 3 batch 2494 loss: 2.323090076446533\n",
      "epoch 3 batch 2495 loss: 2.4206013679504395\n",
      "epoch 3 batch 2496 loss: 2.26115083694458\n",
      "epoch 3 batch 2497 loss: 2.437242269515991\n",
      "epoch 3 batch 2498 loss: 2.513395309448242\n",
      "epoch 3 batch 2499 loss: 2.523149013519287\n",
      "epoch 3 batch 2500 loss: 2.2550954818725586\n",
      "epoch 3 batch 2501 loss: 2.3655805587768555\n",
      "epoch 3 batch 2502 loss: 2.2677385807037354\n",
      "epoch 3 batch 2503 loss: 2.5708580017089844\n",
      "epoch 3 batch 2504 loss: 2.478060483932495\n",
      "epoch 3 batch 2505 loss: 2.2263615131378174\n",
      "epoch 3 batch 2506 loss: 2.3149538040161133\n",
      "epoch 3 batch 2507 loss: 2.3502707481384277\n",
      "epoch 3 batch 2508 loss: 2.249248743057251\n",
      "epoch 3 batch 2509 loss: 2.390488624572754\n",
      "epoch 3 batch 2510 loss: 2.4088175296783447\n",
      "epoch 3 batch 2511 loss: 2.391831874847412\n",
      "epoch 3 batch 2512 loss: 2.395775318145752\n",
      "epoch 3 batch 2513 loss: 2.2698211669921875\n",
      "epoch 3 batch 2514 loss: 2.305515766143799\n",
      "epoch 3 batch 2515 loss: 2.300668716430664\n",
      "epoch 3 batch 2516 loss: 2.308150053024292\n",
      "epoch 3 batch 2517 loss: 2.3756258487701416\n",
      "epoch 3 batch 2518 loss: 2.3279685974121094\n",
      "epoch 3 batch 2519 loss: 2.395594358444214\n",
      "epoch 3 batch 2520 loss: 2.484982967376709\n",
      "epoch 3 batch 2521 loss: 2.403712511062622\n",
      "epoch 3 batch 2522 loss: 2.360123634338379\n",
      "epoch 3 batch 2523 loss: 2.3159446716308594\n",
      "epoch 3 batch 2524 loss: 2.1217916011810303\n",
      "epoch 3 batch 2525 loss: 2.598444700241089\n",
      "epoch 3 batch 2526 loss: 2.5908288955688477\n",
      "epoch 3 batch 2527 loss: 2.332808494567871\n",
      "epoch 3 batch 2528 loss: 2.3425607681274414\n",
      "epoch 3 batch 2529 loss: 2.3294758796691895\n",
      "epoch 3 batch 2530 loss: 2.0613973140716553\n",
      "epoch 3 batch 2531 loss: 2.2066056728363037\n",
      "epoch 3 batch 2532 loss: 2.312562942504883\n",
      "epoch 3 batch 2533 loss: 2.2351996898651123\n",
      "epoch 3 batch 2534 loss: 2.4021966457366943\n",
      "epoch 3 batch 2535 loss: 2.139944314956665\n",
      "epoch 3 batch 2536 loss: 2.102497100830078\n",
      "epoch 3 batch 2537 loss: 2.2743537425994873\n",
      "epoch 3 batch 2538 loss: 2.4470529556274414\n",
      "epoch 3 batch 2539 loss: 2.0480008125305176\n",
      "epoch 3 batch 2540 loss: 2.216456890106201\n",
      "epoch 3 batch 2541 loss: 2.467294692993164\n",
      "epoch 3 batch 2542 loss: 2.3570427894592285\n",
      "epoch 3 batch 2543 loss: 2.329073905944824\n",
      "epoch 3 batch 2544 loss: 2.4391767978668213\n",
      "epoch 3 batch 2545 loss: 2.639551877975464\n",
      "epoch 3 batch 2546 loss: 2.361142635345459\n",
      "epoch 3 batch 2547 loss: 2.2766780853271484\n",
      "epoch 3 batch 2548 loss: 2.4066433906555176\n",
      "epoch 3 batch 2549 loss: 2.7038180828094482\n",
      "epoch 3 batch 2550 loss: 2.703191041946411\n",
      "epoch 3 batch 2551 loss: 2.439030647277832\n",
      "epoch 3 batch 2552 loss: 2.512573480606079\n",
      "epoch 3 batch 2553 loss: 2.540790319442749\n",
      "epoch 3 batch 2554 loss: 2.2593436241149902\n",
      "epoch 3 batch 2555 loss: 2.3185219764709473\n",
      "epoch 3 batch 2556 loss: 2.22352933883667\n",
      "epoch 3 batch 2557 loss: 2.331369400024414\n",
      "epoch 3 batch 2558 loss: 2.329113483428955\n",
      "epoch 3 batch 2559 loss: 2.4666080474853516\n",
      "epoch 3 batch 2560 loss: 2.2383434772491455\n",
      "epoch 3 batch 2561 loss: 2.382181167602539\n",
      "epoch 3 batch 2562 loss: 2.616551399230957\n",
      "epoch 3 batch 2563 loss: 2.306680679321289\n",
      "epoch 3 batch 2564 loss: 2.4783668518066406\n",
      "epoch 3 batch 2565 loss: 2.4782795906066895\n",
      "epoch 3 batch 2566 loss: 2.33954119682312\n",
      "epoch 3 batch 2567 loss: 2.4837758541107178\n",
      "epoch 3 batch 2568 loss: 2.6974542140960693\n",
      "epoch 3 batch 2569 loss: 2.11287522315979\n",
      "epoch 3 batch 2570 loss: 2.4897592067718506\n",
      "epoch 3 batch 2571 loss: 2.394232749938965\n",
      "epoch 3 batch 2572 loss: 2.37819242477417\n",
      "epoch 3 batch 2573 loss: 2.5213847160339355\n",
      "epoch 3 batch 2574 loss: 2.0272140502929688\n",
      "epoch 3 batch 2575 loss: 2.0525412559509277\n",
      "epoch 3 batch 2576 loss: 2.2841925621032715\n",
      "epoch 3 batch 2577 loss: 2.363614559173584\n",
      "epoch 3 batch 2578 loss: 2.2498669624328613\n",
      "epoch 3 batch 2579 loss: 2.6124117374420166\n",
      "epoch 3 batch 2580 loss: 2.2972517013549805\n",
      "epoch 3 batch 2581 loss: 2.319089889526367\n",
      "epoch 3 batch 2582 loss: 2.192798137664795\n",
      "epoch 3 batch 2583 loss: 2.3021373748779297\n",
      "epoch 3 batch 2584 loss: 2.3802623748779297\n",
      "epoch 3 batch 2585 loss: 2.5890538692474365\n",
      "epoch 3 batch 2586 loss: 2.1822619438171387\n",
      "epoch 3 batch 2587 loss: 2.4761297702789307\n",
      "epoch 3 batch 2588 loss: 2.2918026447296143\n",
      "epoch 3 batch 2589 loss: 2.317082405090332\n",
      "epoch 3 batch 2590 loss: 2.5354151725769043\n",
      "epoch 3 batch 2591 loss: 2.314305305480957\n",
      "epoch 3 batch 2592 loss: 2.736457586288452\n",
      "epoch 3 batch 2593 loss: 2.6349010467529297\n",
      "epoch 3 batch 2594 loss: 2.133284330368042\n",
      "epoch 3 batch 2595 loss: 2.372401475906372\n",
      "epoch 3 batch 2596 loss: 2.5204620361328125\n",
      "epoch 3 batch 2597 loss: 2.495429515838623\n",
      "epoch 3 batch 2598 loss: 2.361280918121338\n",
      "epoch 3 batch 2599 loss: 2.797969341278076\n",
      "epoch 3 batch 2600 loss: 2.4856913089752197\n",
      "epoch 3 batch 2601 loss: 2.3996260166168213\n",
      "epoch 3 batch 2602 loss: 2.362800359725952\n",
      "epoch 3 batch 2603 loss: 2.3859026432037354\n",
      "epoch 3 batch 2604 loss: 2.4586291313171387\n",
      "epoch 3 batch 2605 loss: 2.2398126125335693\n",
      "epoch 3 batch 2606 loss: 2.539008140563965\n",
      "epoch 3 batch 2607 loss: 2.372803211212158\n",
      "epoch 3 batch 2608 loss: 2.3382914066314697\n",
      "epoch 3 batch 2609 loss: 2.6211307048797607\n",
      "epoch 3 batch 2610 loss: 2.39654541015625\n",
      "epoch 3 batch 2611 loss: 2.394923210144043\n",
      "epoch 3 batch 2612 loss: 2.522495746612549\n",
      "epoch 3 batch 2613 loss: 2.4771814346313477\n",
      "epoch 3 batch 2614 loss: 2.358497142791748\n",
      "epoch 3 batch 2615 loss: 2.452180862426758\n",
      "epoch 3 batch 2616 loss: 2.3230152130126953\n",
      "epoch 3 batch 2617 loss: 2.550510883331299\n",
      "epoch 3 batch 2618 loss: 2.4524407386779785\n",
      "epoch 3 batch 2619 loss: 2.332862138748169\n",
      "epoch 3 batch 2620 loss: 2.4689254760742188\n",
      "epoch 3 batch 2621 loss: 2.374955654144287\n",
      "epoch 3 batch 2622 loss: 2.740922451019287\n",
      "epoch 3 batch 2623 loss: 2.162099599838257\n",
      "epoch 3 batch 2624 loss: 2.4599416255950928\n",
      "epoch 3 batch 2625 loss: 2.673046112060547\n",
      "epoch 3 batch 2626 loss: 2.185525417327881\n",
      "epoch 3 batch 2627 loss: 2.235321521759033\n",
      "epoch 3 batch 2628 loss: 2.196579933166504\n",
      "epoch 3 batch 2629 loss: 2.4396495819091797\n",
      "epoch 3 batch 2630 loss: 2.521498203277588\n",
      "epoch 3 batch 2631 loss: 2.1075119972229004\n",
      "epoch 3 batch 2632 loss: 2.2006444931030273\n",
      "epoch 3 batch 2633 loss: 2.2137317657470703\n",
      "epoch 3 batch 2634 loss: 2.269068479537964\n",
      "epoch 3 batch 2635 loss: 2.2841591835021973\n",
      "epoch 3 batch 2636 loss: 2.4627087116241455\n",
      "epoch 3 batch 2637 loss: 2.2157769203186035\n",
      "epoch 3 batch 2638 loss: 2.4110913276672363\n",
      "epoch 3 batch 2639 loss: 2.555621385574341\n",
      "epoch 3 batch 2640 loss: 2.473369836807251\n",
      "epoch 3 batch 2641 loss: 2.4151391983032227\n",
      "epoch 3 batch 2642 loss: 2.440084218978882\n",
      "epoch 3 batch 2643 loss: 2.5261034965515137\n",
      "epoch 3 batch 2644 loss: 2.4402644634246826\n",
      "epoch 3 batch 2645 loss: 2.2989463806152344\n",
      "epoch 3 batch 2646 loss: 2.3281476497650146\n",
      "epoch 3 batch 2647 loss: 2.158388614654541\n",
      "epoch 3 batch 2648 loss: 2.5209007263183594\n",
      "epoch 3 batch 2649 loss: 2.7408816814422607\n",
      "epoch 3 batch 2650 loss: 2.307068347930908\n",
      "epoch 3 batch 2651 loss: 2.3034965991973877\n",
      "epoch 3 batch 2652 loss: 2.3953957557678223\n",
      "epoch 3 batch 2653 loss: 2.462113857269287\n",
      "epoch 3 batch 2654 loss: 2.3807225227355957\n",
      "epoch 3 batch 2655 loss: 2.381459951400757\n",
      "epoch 3 batch 2656 loss: 2.468895196914673\n",
      "epoch 3 batch 2657 loss: 2.406057834625244\n",
      "epoch 3 batch 2658 loss: 2.395824909210205\n",
      "epoch 3 batch 2659 loss: 2.138807773590088\n",
      "epoch 3 batch 2660 loss: 2.529444694519043\n",
      "epoch 3 batch 2661 loss: 2.1938469409942627\n",
      "epoch 3 batch 2662 loss: 2.3755860328674316\n",
      "epoch 3 batch 2663 loss: 2.4766197204589844\n",
      "epoch 3 batch 2664 loss: 2.313490152359009\n",
      "epoch 3 batch 2665 loss: 2.6566953659057617\n",
      "epoch 3 batch 2666 loss: 2.325399875640869\n",
      "epoch 3 batch 2667 loss: 2.3648922443389893\n",
      "epoch 3 batch 2668 loss: 2.4299840927124023\n",
      "epoch 3 batch 2669 loss: 2.2038774490356445\n",
      "epoch 3 batch 2670 loss: 2.3750641345977783\n",
      "epoch 3 batch 2671 loss: 2.201387405395508\n",
      "epoch 3 batch 2672 loss: 2.473808765411377\n",
      "epoch 3 batch 2673 loss: 2.2339906692504883\n",
      "epoch 3 batch 2674 loss: 2.66605281829834\n",
      "epoch 3 batch 2675 loss: 2.588435649871826\n",
      "epoch 3 batch 2676 loss: 2.478938579559326\n",
      "epoch 3 batch 2677 loss: 2.4384169578552246\n",
      "epoch 3 batch 2678 loss: 2.443997859954834\n",
      "epoch 3 batch 2679 loss: 2.772474765777588\n",
      "epoch 3 batch 2680 loss: 2.3166537284851074\n",
      "epoch 3 batch 2681 loss: 2.5026495456695557\n",
      "epoch 3 batch 2682 loss: 2.2596523761749268\n",
      "epoch 3 batch 2683 loss: 2.607555866241455\n",
      "epoch 3 batch 2684 loss: 2.196808099746704\n",
      "epoch 3 batch 2685 loss: 2.215823173522949\n",
      "epoch 3 batch 2686 loss: 2.1858935356140137\n",
      "epoch 3 batch 2687 loss: 2.4113411903381348\n",
      "epoch 3 batch 2688 loss: 2.3530936241149902\n",
      "epoch 3 batch 2689 loss: 2.5441079139709473\n",
      "epoch 3 batch 2690 loss: 2.208733558654785\n",
      "epoch 3 batch 2691 loss: 2.2927210330963135\n",
      "epoch 3 batch 2692 loss: 2.4446425437927246\n",
      "epoch 3 batch 2693 loss: 2.268075466156006\n",
      "epoch 3 batch 2694 loss: 2.5106759071350098\n",
      "epoch 3 batch 2695 loss: 2.2821874618530273\n",
      "epoch 3 batch 2696 loss: 2.6862034797668457\n",
      "epoch 3 batch 2697 loss: 2.211137294769287\n",
      "epoch 3 batch 2698 loss: 2.4037346839904785\n",
      "epoch 3 batch 2699 loss: 2.687004327774048\n",
      "epoch 3 batch 2700 loss: 2.561697483062744\n",
      "epoch 3 batch 2701 loss: 2.323671340942383\n",
      "epoch 3 batch 2702 loss: 2.511627674102783\n",
      "epoch 3 batch 2703 loss: 2.375913143157959\n",
      "epoch 3 batch 2704 loss: 2.3726933002471924\n",
      "epoch 3 batch 2705 loss: 2.2500100135803223\n",
      "epoch 3 batch 2706 loss: 2.4165284633636475\n",
      "epoch 3 batch 2707 loss: 2.3653974533081055\n",
      "epoch 3 batch 2708 loss: 2.3653314113616943\n",
      "epoch 3 batch 2709 loss: 2.677894353866577\n",
      "epoch 3 batch 2710 loss: 2.295752763748169\n",
      "epoch 3 batch 2711 loss: 2.3170504570007324\n",
      "epoch 3 batch 2712 loss: 2.648348808288574\n",
      "epoch 3 batch 2713 loss: 2.3928661346435547\n",
      "epoch 3 batch 2714 loss: 2.480069637298584\n",
      "epoch 3 batch 2715 loss: 2.431368589401245\n",
      "epoch 3 batch 2716 loss: 2.4309024810791016\n",
      "epoch 3 batch 2717 loss: 2.3658218383789062\n",
      "epoch 3 batch 2718 loss: 2.4165191650390625\n",
      "epoch 3 batch 2719 loss: 2.4977657794952393\n",
      "epoch 3 batch 2720 loss: 2.37654972076416\n",
      "epoch 3 batch 2721 loss: 2.205695152282715\n",
      "epoch 3 batch 2722 loss: 2.378030300140381\n",
      "epoch 3 batch 2723 loss: 2.494616985321045\n",
      "epoch 3 batch 2724 loss: 2.400779962539673\n",
      "epoch 3 batch 2725 loss: 2.4598493576049805\n",
      "epoch 3 batch 2726 loss: 2.4570670127868652\n",
      "epoch 3 batch 2727 loss: 2.5187675952911377\n",
      "epoch 3 batch 2728 loss: 2.2465062141418457\n",
      "epoch 3 batch 2729 loss: 2.129671335220337\n",
      "epoch 3 batch 2730 loss: 2.411254405975342\n",
      "epoch 3 batch 2731 loss: 2.046083450317383\n",
      "epoch 3 batch 2732 loss: 2.4897494316101074\n",
      "epoch 3 batch 2733 loss: 2.3416924476623535\n",
      "epoch 3 batch 2734 loss: 2.4078588485717773\n",
      "epoch 3 batch 2735 loss: 2.4075357913970947\n",
      "epoch 3 batch 2736 loss: 2.4527971744537354\n",
      "epoch 3 batch 2737 loss: 2.354027271270752\n",
      "epoch 3 batch 2738 loss: 2.655381917953491\n",
      "epoch 3 batch 2739 loss: 2.4351773262023926\n",
      "epoch 3 batch 2740 loss: 2.3667216300964355\n",
      "epoch 3 batch 2741 loss: 2.1748647689819336\n",
      "epoch 3 batch 2742 loss: 2.4000229835510254\n",
      "epoch 3 batch 2743 loss: 2.174150228500366\n",
      "epoch 3 batch 2744 loss: 2.379399538040161\n",
      "epoch 3 batch 2745 loss: 2.3354310989379883\n",
      "epoch 3 batch 2746 loss: 2.2464542388916016\n",
      "epoch 3 batch 2747 loss: 2.407369613647461\n",
      "epoch 3 batch 2748 loss: 2.322223663330078\n",
      "epoch 3 batch 2749 loss: 2.3758201599121094\n",
      "epoch 3 batch 2750 loss: 2.245882272720337\n",
      "epoch 3 batch 2751 loss: 2.4003920555114746\n",
      "epoch 3 batch 2752 loss: 2.42824125289917\n",
      "epoch 3 batch 2753 loss: 2.378871440887451\n",
      "epoch 3 batch 2754 loss: 2.2921414375305176\n",
      "epoch 3 batch 2755 loss: 2.69944167137146\n",
      "epoch 3 batch 2756 loss: 2.19553804397583\n",
      "epoch 3 batch 2757 loss: 2.234581470489502\n",
      "epoch 3 batch 2758 loss: 2.330866575241089\n",
      "epoch 3 batch 2759 loss: 2.256267786026001\n",
      "epoch 3 batch 2760 loss: 2.183260917663574\n",
      "epoch 3 batch 2761 loss: 2.3730080127716064\n",
      "epoch 3 batch 2762 loss: 2.2873239517211914\n",
      "epoch 3 batch 2763 loss: 2.6600184440612793\n",
      "epoch 3 batch 2764 loss: 2.403169631958008\n",
      "epoch 3 batch 2765 loss: 2.2858550548553467\n",
      "epoch 3 batch 2766 loss: 2.367382049560547\n",
      "epoch 3 batch 2767 loss: 2.242305278778076\n",
      "epoch 3 batch 2768 loss: 2.2238364219665527\n",
      "epoch 3 batch 2769 loss: 2.2661523818969727\n",
      "epoch 3 batch 2770 loss: 2.529088020324707\n",
      "epoch 3 batch 2771 loss: 2.419964075088501\n",
      "epoch 3 batch 2772 loss: 2.4365649223327637\n",
      "epoch 3 batch 2773 loss: 2.2013602256774902\n",
      "epoch 3 batch 2774 loss: 2.3906173706054688\n",
      "epoch 3 batch 2775 loss: 2.444537878036499\n",
      "epoch 3 batch 2776 loss: 2.385481834411621\n",
      "epoch 3 batch 2777 loss: 2.609654426574707\n",
      "epoch 3 batch 2778 loss: 2.275606155395508\n",
      "epoch 3 batch 2779 loss: 2.297070026397705\n",
      "epoch 3 batch 2780 loss: 2.410482406616211\n",
      "epoch 3 batch 2781 loss: 2.611973285675049\n",
      "epoch 3 batch 2782 loss: 2.421290636062622\n",
      "epoch 3 batch 2783 loss: 2.4491710662841797\n",
      "epoch 3 batch 2784 loss: 2.3327455520629883\n",
      "epoch 3 batch 2785 loss: 2.114861488342285\n",
      "epoch 3 batch 2786 loss: 2.4601128101348877\n",
      "epoch 3 batch 2787 loss: 2.374671220779419\n",
      "epoch 3 batch 2788 loss: 2.2123124599456787\n",
      "epoch 3 batch 2789 loss: 2.3354129791259766\n",
      "epoch 3 batch 2790 loss: 2.217573642730713\n",
      "epoch 3 batch 2791 loss: 2.378201961517334\n",
      "epoch 3 batch 2792 loss: 2.46529483795166\n",
      "epoch 3 batch 2793 loss: 2.4433140754699707\n",
      "epoch 3 batch 2794 loss: 2.497096538543701\n",
      "epoch 3 batch 2795 loss: 2.5642902851104736\n",
      "epoch 3 batch 2796 loss: 2.3403451442718506\n",
      "epoch 3 batch 2797 loss: 2.32639217376709\n",
      "epoch 3 batch 2798 loss: 2.507625102996826\n",
      "epoch 3 batch 2799 loss: 2.463566780090332\n",
      "epoch 3 batch 2800 loss: 2.245225429534912\n",
      "epoch 3 batch 2801 loss: 2.1082983016967773\n",
      "epoch 3 batch 2802 loss: 2.594945192337036\n",
      "epoch 3 batch 2803 loss: 2.3351991176605225\n",
      "epoch 3 batch 2804 loss: 2.362299680709839\n",
      "epoch 3 batch 2805 loss: 2.2631921768188477\n",
      "epoch 3 batch 2806 loss: 2.314774513244629\n",
      "epoch 3 batch 2807 loss: 2.3446433544158936\n",
      "epoch 3 batch 2808 loss: 2.3845531940460205\n",
      "epoch 3 batch 2809 loss: 2.236722707748413\n",
      "epoch 3 batch 2810 loss: 2.315423011779785\n",
      "epoch 3 batch 2811 loss: 2.8616325855255127\n",
      "epoch 3 batch 2812 loss: 2.2467596530914307\n",
      "epoch 3 batch 2813 loss: 2.3628368377685547\n",
      "epoch 3 batch 2814 loss: 2.464179277420044\n",
      "epoch 3 batch 2815 loss: 2.5063912868499756\n",
      "epoch 3 batch 2816 loss: 2.354017734527588\n",
      "epoch 3 batch 2817 loss: 2.2730913162231445\n",
      "epoch 3 batch 2818 loss: 2.189356803894043\n",
      "epoch 3 batch 2819 loss: 2.085111379623413\n",
      "epoch 3 batch 2820 loss: 2.574714183807373\n",
      "epoch 3 batch 2821 loss: 2.3792567253112793\n",
      "epoch 3 batch 2822 loss: 2.0861635208129883\n",
      "epoch 3 batch 2823 loss: 2.321215867996216\n",
      "epoch 3 batch 2824 loss: 2.40975284576416\n",
      "epoch 3 batch 2825 loss: 2.3171563148498535\n",
      "epoch 3 batch 2826 loss: 2.3388209342956543\n",
      "epoch 3 batch 2827 loss: 2.4410576820373535\n",
      "epoch 3 batch 2828 loss: 2.3815619945526123\n",
      "epoch 3 batch 2829 loss: 2.340080738067627\n",
      "epoch 3 batch 2830 loss: 2.465070962905884\n",
      "epoch 3 batch 2831 loss: 2.487098217010498\n",
      "epoch 3 batch 2832 loss: 2.079742431640625\n",
      "epoch 3 batch 2833 loss: 2.3461880683898926\n",
      "epoch 3 batch 2834 loss: 3.001741409301758\n",
      "epoch 3 batch 2835 loss: 2.3699450492858887\n",
      "epoch 3 batch 2836 loss: 2.3550610542297363\n",
      "epoch 3 batch 2837 loss: 2.5502071380615234\n",
      "epoch 3 batch 2838 loss: 2.5493288040161133\n",
      "epoch 3 batch 2839 loss: 2.430983066558838\n",
      "epoch 3 batch 2840 loss: 2.761016607284546\n",
      "epoch 3 batch 2841 loss: 2.2349205017089844\n",
      "epoch 3 batch 2842 loss: 2.4599618911743164\n",
      "epoch 3 batch 2843 loss: 2.518758773803711\n",
      "epoch 3 batch 2844 loss: 2.441068172454834\n",
      "epoch 3 batch 2845 loss: 2.313553810119629\n",
      "epoch 3 batch 2846 loss: 2.4472427368164062\n",
      "epoch 3 batch 2847 loss: 2.234440326690674\n",
      "epoch 3 batch 2848 loss: 2.3003766536712646\n",
      "epoch 3 batch 2849 loss: 2.4289469718933105\n",
      "epoch 3 batch 2850 loss: 2.081998348236084\n",
      "epoch 3 batch 2851 loss: 2.2108898162841797\n",
      "epoch 3 batch 2852 loss: 2.2564594745635986\n",
      "epoch 3 batch 2853 loss: 2.2151925563812256\n",
      "epoch 3 batch 2854 loss: 2.486501693725586\n",
      "epoch 3 batch 2855 loss: 2.6478891372680664\n",
      "epoch 3 batch 2856 loss: 2.2453274726867676\n",
      "epoch 3 batch 2857 loss: 2.23213791847229\n",
      "epoch 3 batch 2858 loss: 2.570657253265381\n",
      "epoch 3 batch 2859 loss: 2.246391773223877\n",
      "epoch 3 batch 2860 loss: 2.3888790607452393\n",
      "epoch 3 batch 2861 loss: 2.2909321784973145\n",
      "epoch 3 batch 2862 loss: 2.377720832824707\n",
      "epoch 3 batch 2863 loss: 2.3738303184509277\n",
      "epoch 3 batch 2864 loss: 2.2836241722106934\n",
      "epoch 3 batch 2865 loss: 2.4346389770507812\n",
      "epoch 3 batch 2866 loss: 2.2058653831481934\n",
      "epoch 3 batch 2867 loss: 2.4216535091400146\n",
      "epoch 3 batch 2868 loss: 2.2258718013763428\n",
      "epoch 3 batch 2869 loss: 2.4999003410339355\n",
      "epoch 3 batch 2870 loss: 2.255892038345337\n",
      "epoch 3 batch 2871 loss: 2.595425844192505\n",
      "epoch 3 batch 2872 loss: 2.224555730819702\n",
      "epoch 3 batch 2873 loss: 2.337195873260498\n",
      "epoch 3 batch 2874 loss: 2.469666004180908\n",
      "epoch 3 batch 2875 loss: 2.0130138397216797\n",
      "epoch 3 batch 2876 loss: 2.3259828090667725\n",
      "epoch 3 batch 2877 loss: 2.348384380340576\n",
      "epoch 3 batch 2878 loss: 2.529069423675537\n",
      "epoch 3 batch 2879 loss: 2.2854456901550293\n",
      "epoch 3 batch 2880 loss: 2.494626045227051\n",
      "epoch 3 batch 2881 loss: 2.3912296295166016\n",
      "epoch 3 batch 2882 loss: 2.43652606010437\n",
      "epoch 3 batch 2883 loss: 2.319884777069092\n",
      "epoch 3 batch 2884 loss: 2.310971260070801\n",
      "epoch 3 batch 2885 loss: 2.6619091033935547\n",
      "epoch 3 batch 2886 loss: 2.4969866275787354\n",
      "epoch 3 batch 2887 loss: 2.2771096229553223\n",
      "epoch 3 batch 2888 loss: 2.69582200050354\n",
      "epoch 3 batch 2889 loss: 2.2605104446411133\n",
      "epoch 3 batch 2890 loss: 2.35858416557312\n",
      "epoch 3 batch 2891 loss: 2.189326286315918\n",
      "epoch 3 batch 2892 loss: 2.291905164718628\n",
      "epoch 3 batch 2893 loss: 2.543574333190918\n",
      "epoch 3 batch 2894 loss: 2.360456705093384\n",
      "epoch 3 batch 2895 loss: 2.3182480335235596\n",
      "epoch 3 batch 2896 loss: 2.326209306716919\n",
      "epoch 3 batch 2897 loss: 2.3267159461975098\n",
      "epoch 3 batch 2898 loss: 2.296360969543457\n",
      "epoch 3 batch 2899 loss: 2.392582416534424\n",
      "epoch 3 batch 2900 loss: 2.3829376697540283\n",
      "epoch 3 batch 2901 loss: 2.4574809074401855\n",
      "epoch 3 batch 2902 loss: 2.3485965728759766\n",
      "epoch 3 batch 2903 loss: 2.2983670234680176\n",
      "epoch 3 batch 2904 loss: 2.427382707595825\n",
      "epoch 3 batch 2905 loss: 2.350276470184326\n",
      "epoch 3 batch 2906 loss: 2.286698341369629\n",
      "epoch 3 batch 2907 loss: 2.387146472930908\n",
      "epoch 3 batch 2908 loss: 2.3172338008880615\n",
      "epoch 3 batch 2909 loss: 2.18249249458313\n",
      "epoch 3 batch 2910 loss: 2.355720043182373\n",
      "epoch 3 batch 2911 loss: 2.3677306175231934\n",
      "epoch 3 batch 2912 loss: 2.679945945739746\n",
      "epoch 3 batch 2913 loss: 2.257636308670044\n",
      "epoch 3 batch 2914 loss: 2.4719882011413574\n",
      "epoch 3 batch 2915 loss: 2.1194000244140625\n",
      "epoch 3 batch 2916 loss: 2.198659896850586\n",
      "epoch 3 batch 2917 loss: 2.5147716999053955\n",
      "epoch 3 batch 2918 loss: 2.3561949729919434\n",
      "epoch 3 batch 2919 loss: 2.5049338340759277\n",
      "epoch 3 batch 2920 loss: 2.603914260864258\n",
      "epoch 3 batch 2921 loss: 2.2429628372192383\n",
      "epoch 3 batch 2922 loss: 2.3081109523773193\n",
      "epoch 3 batch 2923 loss: 2.1349830627441406\n",
      "epoch 3 batch 2924 loss: 2.301525115966797\n",
      "epoch 3 batch 2925 loss: 2.219952344894409\n",
      "epoch 3 batch 2926 loss: 2.2182064056396484\n",
      "epoch 3 batch 2927 loss: 2.263835906982422\n",
      "epoch 3 batch 2928 loss: 2.528083324432373\n",
      "epoch 3 batch 2929 loss: 2.3662309646606445\n",
      "epoch 3 batch 2930 loss: 2.5753893852233887\n",
      "epoch 3 batch 2931 loss: 2.2731075286865234\n",
      "epoch 3 batch 2932 loss: 2.2839088439941406\n",
      "epoch 3 batch 2933 loss: 2.5789835453033447\n",
      "epoch 3 batch 2934 loss: 2.432553291320801\n",
      "epoch 3 batch 2935 loss: 2.262425422668457\n",
      "epoch 3 batch 2936 loss: 2.518406867980957\n",
      "epoch 3 batch 2937 loss: 2.2070698738098145\n",
      "epoch 3 batch 2938 loss: 2.281142234802246\n",
      "epoch 3 batch 2939 loss: 2.453749179840088\n",
      "epoch 3 batch 2940 loss: 2.2621045112609863\n",
      "epoch 3 batch 2941 loss: 2.4697952270507812\n",
      "epoch 3 batch 2942 loss: 2.3375134468078613\n",
      "epoch 3 batch 2943 loss: 2.5480635166168213\n",
      "epoch 3 batch 2944 loss: 2.1657261848449707\n",
      "epoch 3 batch 2945 loss: 2.277092456817627\n",
      "epoch 3 batch 2946 loss: 2.2407312393188477\n",
      "epoch 3 batch 2947 loss: 2.672799587249756\n",
      "epoch 3 batch 2948 loss: 2.3351809978485107\n",
      "epoch 3 batch 2949 loss: 2.2409305572509766\n",
      "epoch 3 batch 2950 loss: 2.5759596824645996\n",
      "epoch 3 batch 2951 loss: 2.2985951900482178\n",
      "epoch 3 batch 2952 loss: 2.4423885345458984\n",
      "epoch 3 batch 2953 loss: 2.3839292526245117\n",
      "epoch 3 batch 2954 loss: 2.2171969413757324\n",
      "epoch 3 batch 2955 loss: 2.3216211795806885\n",
      "epoch 3 batch 2956 loss: 2.4103617668151855\n",
      "epoch 3 batch 2957 loss: 2.296973943710327\n",
      "epoch 3 batch 2958 loss: 2.6906397342681885\n",
      "epoch 3 batch 2959 loss: 2.6394851207733154\n",
      "epoch 3 batch 2960 loss: 2.3738179206848145\n",
      "epoch 3 batch 2961 loss: 2.3597018718719482\n",
      "epoch 3 batch 2962 loss: 2.7990615367889404\n",
      "epoch 3 batch 2963 loss: 2.432420253753662\n",
      "epoch 3 batch 2964 loss: 2.3712146282196045\n",
      "epoch 3 batch 2965 loss: 2.5212903022766113\n",
      "epoch 3 batch 2966 loss: 2.210150718688965\n",
      "epoch 3 batch 2967 loss: 2.371858596801758\n",
      "epoch 3 batch 2968 loss: 2.369621753692627\n",
      "epoch 3 batch 2969 loss: 2.35526704788208\n",
      "epoch 3 batch 2970 loss: 2.4175548553466797\n",
      "epoch 3 batch 2971 loss: 2.5148978233337402\n",
      "epoch 3 batch 2972 loss: 2.248812675476074\n",
      "epoch 3 batch 2973 loss: 2.333547592163086\n",
      "epoch 3 batch 2974 loss: 2.6237354278564453\n",
      "epoch 3 batch 2975 loss: 2.4481678009033203\n",
      "epoch 3 batch 2976 loss: 2.341146469116211\n",
      "epoch 3 batch 2977 loss: 2.531048536300659\n",
      "epoch 3 batch 2978 loss: 2.165955066680908\n",
      "epoch 3 batch 2979 loss: 2.5419740676879883\n",
      "epoch 3 batch 2980 loss: 2.3608620166778564\n",
      "epoch 3 batch 2981 loss: 2.153862476348877\n",
      "epoch 3 batch 2982 loss: 2.2596776485443115\n",
      "epoch 3 batch 2983 loss: 2.319345235824585\n",
      "epoch 3 batch 2984 loss: 2.216391086578369\n",
      "epoch 3 batch 2985 loss: 2.2333555221557617\n",
      "epoch 3 batch 2986 loss: 2.232360363006592\n",
      "epoch 3 batch 2987 loss: 2.4653878211975098\n",
      "epoch 3 batch 2988 loss: 2.2348337173461914\n",
      "epoch 3 batch 2989 loss: 2.4854578971862793\n",
      "epoch 3 batch 2990 loss: 2.2099266052246094\n",
      "epoch 3 batch 2991 loss: 2.1572861671447754\n",
      "epoch 3 batch 2992 loss: 2.276338577270508\n",
      "epoch 3 batch 2993 loss: 2.2639784812927246\n",
      "epoch 3 batch 2994 loss: 2.23460054397583\n",
      "epoch 3 batch 2995 loss: 2.4332480430603027\n",
      "epoch 3 batch 2996 loss: 2.5551066398620605\n",
      "epoch 3 batch 2997 loss: 2.275679588317871\n",
      "epoch 3 batch 2998 loss: 2.4914021492004395\n",
      "epoch 3 batch 2999 loss: 2.271113872528076\n",
      "epoch 3 batch 3000 loss: 2.1635420322418213\n",
      "epoch 3 batch 3001 loss: 2.5149738788604736\n",
      "epoch 3 batch 3002 loss: 2.30595064163208\n",
      "epoch 3 batch 3003 loss: 2.5897090435028076\n",
      "epoch 3 batch 3004 loss: 2.586243152618408\n",
      "epoch 3 batch 3005 loss: 2.242377281188965\n",
      "epoch 3 batch 3006 loss: 2.5087647438049316\n",
      "epoch 3 batch 3007 loss: 2.2057406902313232\n",
      "epoch 3 batch 3008 loss: 2.243433952331543\n",
      "epoch 3 batch 3009 loss: 2.337418556213379\n",
      "epoch 3 batch 3010 loss: 2.2805070877075195\n",
      "epoch 3 batch 3011 loss: 2.208883047103882\n",
      "epoch 3 batch 3012 loss: 2.2761142253875732\n",
      "epoch 3 batch 3013 loss: 2.339743137359619\n",
      "epoch 3 batch 3014 loss: 2.3034114837646484\n",
      "epoch 3 batch 3015 loss: 2.3059298992156982\n",
      "epoch 3 batch 3016 loss: 2.2279157638549805\n",
      "epoch 3 batch 3017 loss: 2.359654664993286\n",
      "epoch 3 batch 3018 loss: 2.765690803527832\n",
      "epoch 3 batch 3019 loss: 2.166165590286255\n",
      "epoch 3 batch 3020 loss: 2.3752312660217285\n",
      "epoch 3 batch 3021 loss: 2.373605251312256\n",
      "epoch 3 batch 3022 loss: 2.32936429977417\n",
      "epoch 3 batch 3023 loss: 2.1356279850006104\n",
      "epoch 3 batch 3024 loss: 2.4607043266296387\n",
      "epoch 3 batch 3025 loss: 2.130125045776367\n",
      "epoch 3 batch 3026 loss: 2.153717517852783\n",
      "epoch 3 batch 3027 loss: 2.446197509765625\n",
      "epoch 3 batch 3028 loss: 2.2422657012939453\n",
      "epoch 3 batch 3029 loss: 2.347503900527954\n",
      "epoch 3 batch 3030 loss: 2.521559476852417\n",
      "epoch 3 batch 3031 loss: 2.2963967323303223\n",
      "epoch 3 batch 3032 loss: 2.421024799346924\n",
      "epoch 3 batch 3033 loss: 2.2019147872924805\n",
      "epoch 3 batch 3034 loss: 2.4662704467773438\n",
      "epoch 3 batch 3035 loss: 2.328385353088379\n",
      "epoch 3 batch 3036 loss: 2.6000328063964844\n",
      "epoch 3 batch 3037 loss: 2.4439430236816406\n",
      "epoch 3 batch 3038 loss: 2.0832300186157227\n",
      "epoch 3 batch 3039 loss: 2.2226927280426025\n",
      "epoch 3 batch 3040 loss: 2.68778920173645\n",
      "epoch 3 batch 3041 loss: 2.4211792945861816\n",
      "epoch 3 batch 3042 loss: 2.377363681793213\n",
      "epoch 3 batch 3043 loss: 2.462717056274414\n",
      "epoch 3 batch 3044 loss: 2.339390277862549\n",
      "epoch 3 batch 3045 loss: 2.7016963958740234\n",
      "epoch 3 batch 3046 loss: 2.410210371017456\n",
      "epoch 3 batch 3047 loss: 2.4573018550872803\n",
      "epoch 3 batch 3048 loss: 2.207923412322998\n",
      "epoch 3 batch 3049 loss: 2.405299663543701\n",
      "epoch 3 batch 3050 loss: 2.2799556255340576\n",
      "epoch 3 batch 3051 loss: 2.0822629928588867\n",
      "epoch 3 batch 3052 loss: 2.324406385421753\n",
      "epoch 3 batch 3053 loss: 2.3510630130767822\n",
      "epoch 3 batch 3054 loss: 2.44320011138916\n",
      "epoch 3 batch 3055 loss: 2.389519691467285\n",
      "epoch 3 batch 3056 loss: 2.3156094551086426\n",
      "epoch 3 batch 3057 loss: 2.3626646995544434\n",
      "epoch 3 batch 3058 loss: 2.3087964057922363\n",
      "epoch 3 batch 3059 loss: 2.277322769165039\n",
      "epoch 3 batch 3060 loss: 2.194183349609375\n",
      "epoch 3 batch 3061 loss: 2.169950008392334\n",
      "epoch 3 batch 3062 loss: 2.30133056640625\n",
      "epoch 3 batch 3063 loss: 2.4524049758911133\n",
      "epoch 3 batch 3064 loss: 2.56663179397583\n",
      "epoch 3 batch 3065 loss: 2.296846866607666\n",
      "epoch 3 batch 3066 loss: 2.2927656173706055\n",
      "epoch 3 batch 3067 loss: 2.4458224773406982\n",
      "epoch 3 batch 3068 loss: 2.314441204071045\n",
      "epoch 3 batch 3069 loss: 2.434889078140259\n",
      "epoch 3 batch 3070 loss: 2.3910317420959473\n",
      "epoch 3 batch 3071 loss: 2.1263699531555176\n",
      "epoch 3 batch 3072 loss: 2.452327013015747\n",
      "epoch 3 batch 3073 loss: 2.41902756690979\n",
      "epoch 3 batch 3074 loss: 2.2932229042053223\n",
      "epoch 3 batch 3075 loss: 2.2889561653137207\n",
      "epoch 3 batch 3076 loss: 2.5053277015686035\n",
      "epoch 3 batch 3077 loss: 2.3698248863220215\n",
      "epoch 3 batch 3078 loss: 2.471925735473633\n",
      "epoch 3 batch 3079 loss: 2.5675413608551025\n",
      "epoch 3 batch 3080 loss: 2.32562518119812\n",
      "epoch 3 batch 3081 loss: 2.3245956897735596\n",
      "epoch 3 batch 3082 loss: 2.3752593994140625\n",
      "epoch 3 batch 3083 loss: 2.702785015106201\n",
      "epoch 3 batch 3084 loss: 2.3915634155273438\n",
      "epoch 3 batch 3085 loss: 2.1616573333740234\n",
      "epoch 3 batch 3086 loss: 2.334970474243164\n",
      "epoch 3 batch 3087 loss: 2.2614870071411133\n",
      "epoch 3 batch 3088 loss: 2.376603126525879\n",
      "epoch 3 batch 3089 loss: 2.432048797607422\n",
      "epoch 3 batch 3090 loss: 2.3875648975372314\n",
      "epoch 3 batch 3091 loss: 2.363363265991211\n",
      "epoch 3 batch 3092 loss: 2.4082822799682617\n",
      "epoch 3 batch 3093 loss: 2.614814043045044\n",
      "epoch 3 batch 3094 loss: 2.161602020263672\n",
      "epoch 3 batch 3095 loss: 2.6534318923950195\n",
      "epoch 3 batch 3096 loss: 2.328394889831543\n",
      "epoch 3 batch 3097 loss: 2.4748823642730713\n",
      "epoch 3 batch 3098 loss: 2.318936586380005\n",
      "epoch 3 batch 3099 loss: 2.6816234588623047\n",
      "epoch 3 batch 3100 loss: 2.459644317626953\n",
      "epoch 3 batch 3101 loss: 2.4694998264312744\n",
      "epoch 3 batch 3102 loss: 2.2495856285095215\n",
      "epoch 3 batch 3103 loss: 2.5460739135742188\n",
      "epoch 3 batch 3104 loss: 2.4193246364593506\n",
      "epoch 3 batch 3105 loss: 2.444613456726074\n",
      "epoch 3 batch 3106 loss: 2.4900832176208496\n",
      "epoch 3 batch 3107 loss: 2.2914490699768066\n",
      "epoch 3 batch 3108 loss: 2.5822839736938477\n",
      "epoch 3 batch 3109 loss: 2.1169416904449463\n",
      "epoch 3 batch 3110 loss: 2.322429656982422\n",
      "epoch 3 batch 3111 loss: 2.066915988922119\n",
      "epoch 3 batch 3112 loss: 2.2142608165740967\n",
      "epoch 3 batch 3113 loss: 2.3806865215301514\n",
      "epoch 3 batch 3114 loss: 2.498366117477417\n",
      "epoch 3 batch 3115 loss: 2.358935832977295\n",
      "epoch 3 batch 3116 loss: 2.386547565460205\n",
      "epoch 3 batch 3117 loss: 2.3092238903045654\n",
      "epoch 3 batch 3118 loss: 2.493473529815674\n",
      "epoch 3 batch 3119 loss: 2.3303842544555664\n",
      "epoch 3 batch 3120 loss: 2.544398307800293\n",
      "epoch 3 batch 3121 loss: 2.2882699966430664\n",
      "epoch 3 batch 3122 loss: 2.356809616088867\n",
      "epoch 3 batch 3123 loss: 2.566458225250244\n",
      "epoch 3 batch 3124 loss: 2.3965625762939453\n",
      "epoch loss: 2.417132318229675\n",
      "epoch 4 batch 0 loss: 2.438720464706421\n",
      "epoch 4 batch 1 loss: 2.4242055416107178\n",
      "epoch 4 batch 2 loss: 2.114997386932373\n",
      "epoch 4 batch 3 loss: 2.0167222023010254\n",
      "epoch 4 batch 4 loss: 2.3586246967315674\n",
      "epoch 4 batch 5 loss: 2.3200550079345703\n",
      "epoch 4 batch 6 loss: 2.4033894538879395\n",
      "epoch 4 batch 7 loss: 2.3883631229400635\n",
      "epoch 4 batch 8 loss: 2.44797945022583\n",
      "epoch 4 batch 9 loss: 2.083137035369873\n",
      "epoch 4 batch 10 loss: 2.360074996948242\n",
      "epoch 4 batch 11 loss: 2.493068218231201\n",
      "epoch 4 batch 12 loss: 2.414783477783203\n",
      "epoch 4 batch 13 loss: 2.2367680072784424\n",
      "epoch 4 batch 14 loss: 2.325540781021118\n",
      "epoch 4 batch 15 loss: 2.331679105758667\n",
      "epoch 4 batch 16 loss: 2.3822569847106934\n",
      "epoch 4 batch 17 loss: 2.3674545288085938\n",
      "epoch 4 batch 18 loss: 2.2728347778320312\n",
      "epoch 4 batch 19 loss: 2.121180534362793\n",
      "epoch 4 batch 20 loss: 2.217041492462158\n",
      "epoch 4 batch 21 loss: 2.2900915145874023\n",
      "epoch 4 batch 22 loss: 2.4533395767211914\n",
      "epoch 4 batch 23 loss: 2.337949752807617\n",
      "epoch 4 batch 24 loss: 2.5722246170043945\n",
      "epoch 4 batch 25 loss: 2.1066675186157227\n",
      "epoch 4 batch 26 loss: 2.601196527481079\n",
      "epoch 4 batch 27 loss: 2.219205856323242\n",
      "epoch 4 batch 28 loss: 2.481100559234619\n",
      "epoch 4 batch 29 loss: 2.3375773429870605\n",
      "epoch 4 batch 30 loss: 2.466437339782715\n",
      "epoch 4 batch 31 loss: 2.6338143348693848\n",
      "epoch 4 batch 32 loss: 2.3542351722717285\n",
      "epoch 4 batch 33 loss: 2.260636806488037\n",
      "epoch 4 batch 34 loss: 2.8115224838256836\n",
      "epoch 4 batch 35 loss: 2.271982192993164\n",
      "epoch 4 batch 36 loss: 2.6736319065093994\n",
      "epoch 4 batch 37 loss: 2.3723361492156982\n",
      "epoch 4 batch 38 loss: 2.4706172943115234\n",
      "epoch 4 batch 39 loss: 2.232821464538574\n",
      "epoch 4 batch 40 loss: 2.404883623123169\n",
      "epoch 4 batch 41 loss: 2.4599242210388184\n",
      "epoch 4 batch 42 loss: 2.2268319129943848\n",
      "epoch 4 batch 43 loss: 2.4412920475006104\n",
      "epoch 4 batch 44 loss: 2.68743896484375\n",
      "epoch 4 batch 45 loss: 2.361602306365967\n",
      "epoch 4 batch 46 loss: 2.3580832481384277\n",
      "epoch 4 batch 47 loss: 2.370178461074829\n",
      "epoch 4 batch 48 loss: 2.4602160453796387\n",
      "epoch 4 batch 49 loss: 2.373539924621582\n",
      "epoch 4 batch 50 loss: 2.499202251434326\n",
      "epoch 4 batch 51 loss: 2.2333476543426514\n",
      "epoch 4 batch 52 loss: 2.400841474533081\n",
      "epoch 4 batch 53 loss: 2.370577335357666\n",
      "epoch 4 batch 54 loss: 2.511080265045166\n",
      "epoch 4 batch 55 loss: 2.5775318145751953\n",
      "epoch 4 batch 56 loss: 2.4877982139587402\n",
      "epoch 4 batch 57 loss: 2.290823459625244\n",
      "epoch 4 batch 58 loss: 2.577589988708496\n",
      "epoch 4 batch 59 loss: 2.4290966987609863\n",
      "epoch 4 batch 60 loss: 2.373380422592163\n",
      "epoch 4 batch 61 loss: 2.273792028427124\n",
      "epoch 4 batch 62 loss: 2.180685043334961\n",
      "epoch 4 batch 63 loss: 2.1632306575775146\n",
      "epoch 4 batch 64 loss: 2.325822114944458\n",
      "epoch 4 batch 65 loss: 2.323523998260498\n",
      "epoch 4 batch 66 loss: 2.401031017303467\n",
      "epoch 4 batch 67 loss: 2.0881435871124268\n",
      "epoch 4 batch 68 loss: 2.588613510131836\n",
      "epoch 4 batch 69 loss: 2.476397752761841\n",
      "epoch 4 batch 70 loss: 2.40806245803833\n",
      "epoch 4 batch 71 loss: 2.253129482269287\n",
      "epoch 4 batch 72 loss: 2.685741424560547\n",
      "epoch 4 batch 73 loss: 2.3601913452148438\n",
      "epoch 4 batch 74 loss: 2.430182933807373\n",
      "epoch 4 batch 75 loss: 2.3650119304656982\n",
      "epoch 4 batch 76 loss: 2.3404974937438965\n",
      "epoch 4 batch 77 loss: 2.480005979537964\n",
      "epoch 4 batch 78 loss: 2.0020012855529785\n",
      "epoch 4 batch 79 loss: 2.4968409538269043\n",
      "epoch 4 batch 80 loss: 2.3602044582366943\n",
      "epoch 4 batch 81 loss: 2.053877353668213\n",
      "epoch 4 batch 82 loss: 2.6043453216552734\n",
      "epoch 4 batch 83 loss: 2.3079733848571777\n",
      "epoch 4 batch 84 loss: 2.2663724422454834\n",
      "epoch 4 batch 85 loss: 2.339191198348999\n",
      "epoch 4 batch 86 loss: 2.518186569213867\n",
      "epoch 4 batch 87 loss: 2.2085938453674316\n",
      "epoch 4 batch 88 loss: 2.1577086448669434\n",
      "epoch 4 batch 89 loss: 2.405087471008301\n",
      "epoch 4 batch 90 loss: 2.320394992828369\n",
      "epoch 4 batch 91 loss: 2.2559969425201416\n",
      "epoch 4 batch 92 loss: 2.2561757564544678\n",
      "epoch 4 batch 93 loss: 2.2209908962249756\n",
      "epoch 4 batch 94 loss: 2.655149221420288\n",
      "epoch 4 batch 95 loss: 2.375525951385498\n",
      "epoch 4 batch 96 loss: 2.405353546142578\n",
      "epoch 4 batch 97 loss: 2.2768702507019043\n",
      "epoch 4 batch 98 loss: 2.4531989097595215\n",
      "epoch 4 batch 99 loss: 2.3557541370391846\n",
      "epoch 4 batch 100 loss: 2.538100481033325\n",
      "epoch 4 batch 101 loss: 2.264981746673584\n",
      "epoch 4 batch 102 loss: 2.1872146129608154\n",
      "epoch 4 batch 103 loss: 2.4546031951904297\n",
      "epoch 4 batch 104 loss: 2.639937400817871\n",
      "epoch 4 batch 105 loss: 2.170624017715454\n",
      "epoch 4 batch 106 loss: 2.1570093631744385\n",
      "epoch 4 batch 107 loss: 2.1353282928466797\n",
      "epoch 4 batch 108 loss: 2.3406214714050293\n",
      "epoch 4 batch 109 loss: 2.3552000522613525\n",
      "epoch 4 batch 110 loss: 2.281043529510498\n",
      "epoch 4 batch 111 loss: 2.586099147796631\n",
      "epoch 4 batch 112 loss: 2.297499656677246\n",
      "epoch 4 batch 113 loss: 2.355518341064453\n",
      "epoch 4 batch 114 loss: 2.274501323699951\n",
      "epoch 4 batch 115 loss: 2.485297679901123\n",
      "epoch 4 batch 116 loss: 2.6165552139282227\n",
      "epoch 4 batch 117 loss: 2.232430934906006\n",
      "epoch 4 batch 118 loss: 2.095224380493164\n",
      "epoch 4 batch 119 loss: 2.285547971725464\n",
      "epoch 4 batch 120 loss: 2.6758265495300293\n",
      "epoch 4 batch 121 loss: 2.2890963554382324\n",
      "epoch 4 batch 122 loss: 2.1043050289154053\n",
      "epoch 4 batch 123 loss: 2.337996482849121\n",
      "epoch 4 batch 124 loss: 2.311760902404785\n",
      "epoch 4 batch 125 loss: 2.5090951919555664\n",
      "epoch 4 batch 126 loss: 2.266925811767578\n",
      "epoch 4 batch 127 loss: 2.3137497901916504\n",
      "epoch 4 batch 128 loss: 2.314953088760376\n",
      "epoch 4 batch 129 loss: 2.300478458404541\n",
      "epoch 4 batch 130 loss: 2.24558162689209\n",
      "epoch 4 batch 131 loss: 2.2298593521118164\n",
      "epoch 4 batch 132 loss: 2.309250593185425\n",
      "epoch 4 batch 133 loss: 2.353729248046875\n",
      "epoch 4 batch 134 loss: 2.2795469760894775\n",
      "epoch 4 batch 135 loss: 2.2330527305603027\n",
      "epoch 4 batch 136 loss: 2.156360387802124\n",
      "epoch 4 batch 137 loss: 2.3791069984436035\n",
      "epoch 4 batch 138 loss: 2.224609613418579\n",
      "epoch 4 batch 139 loss: 2.2643299102783203\n",
      "epoch 4 batch 140 loss: 2.4425225257873535\n",
      "epoch 4 batch 141 loss: 2.3888018131256104\n",
      "epoch 4 batch 142 loss: 2.216508388519287\n",
      "epoch 4 batch 143 loss: 2.2484312057495117\n",
      "epoch 4 batch 144 loss: 2.1150355339050293\n",
      "epoch 4 batch 145 loss: 2.3670787811279297\n",
      "epoch 4 batch 146 loss: 2.284946918487549\n",
      "epoch 4 batch 147 loss: 2.7189838886260986\n",
      "epoch 4 batch 148 loss: 2.508368968963623\n",
      "epoch 4 batch 149 loss: 2.599480628967285\n",
      "epoch 4 batch 150 loss: 2.3698911666870117\n",
      "epoch 4 batch 151 loss: 2.380760431289673\n",
      "epoch 4 batch 152 loss: 2.537463426589966\n",
      "epoch 4 batch 153 loss: 2.4072108268737793\n",
      "epoch 4 batch 154 loss: 2.428980827331543\n",
      "epoch 4 batch 155 loss: 2.4679336547851562\n",
      "epoch 4 batch 156 loss: 2.313790798187256\n",
      "epoch 4 batch 157 loss: 2.3404486179351807\n",
      "epoch 4 batch 158 loss: 2.469040870666504\n",
      "epoch 4 batch 159 loss: 2.2538130283355713\n",
      "epoch 4 batch 160 loss: 2.083282709121704\n",
      "epoch 4 batch 161 loss: 2.321491003036499\n",
      "epoch 4 batch 162 loss: 2.4816250801086426\n",
      "epoch 4 batch 163 loss: 2.439603328704834\n",
      "epoch 4 batch 164 loss: 2.3246634006500244\n",
      "epoch 4 batch 165 loss: 2.281743049621582\n",
      "epoch 4 batch 166 loss: 2.31982421875\n",
      "epoch 4 batch 167 loss: 2.2832789421081543\n",
      "epoch 4 batch 168 loss: 2.400111675262451\n",
      "epoch 4 batch 169 loss: 2.399122953414917\n",
      "epoch 4 batch 170 loss: 2.5107741355895996\n",
      "epoch 4 batch 171 loss: 2.2475433349609375\n",
      "epoch 4 batch 172 loss: 2.2214903831481934\n",
      "epoch 4 batch 173 loss: 2.2758474349975586\n",
      "epoch 4 batch 174 loss: 2.1534547805786133\n",
      "epoch 4 batch 175 loss: 2.337528705596924\n",
      "epoch 4 batch 176 loss: 2.4121716022491455\n",
      "epoch 4 batch 177 loss: 2.4698567390441895\n",
      "epoch 4 batch 178 loss: 2.3424177169799805\n",
      "epoch 4 batch 179 loss: 2.358454942703247\n",
      "epoch 4 batch 180 loss: 2.4460020065307617\n",
      "epoch 4 batch 181 loss: 2.4088215827941895\n",
      "epoch 4 batch 182 loss: 2.2572431564331055\n",
      "epoch 4 batch 183 loss: 2.375274658203125\n",
      "epoch 4 batch 184 loss: 2.316153049468994\n",
      "epoch 4 batch 185 loss: 2.124560832977295\n",
      "epoch 4 batch 186 loss: 2.235919237136841\n",
      "epoch 4 batch 187 loss: 2.3099749088287354\n",
      "epoch 4 batch 188 loss: 2.345168113708496\n",
      "epoch 4 batch 189 loss: 2.4969708919525146\n",
      "epoch 4 batch 190 loss: 2.390352725982666\n",
      "epoch 4 batch 191 loss: 2.3059921264648438\n",
      "epoch 4 batch 192 loss: 2.361677646636963\n",
      "epoch 4 batch 193 loss: 2.390866756439209\n",
      "epoch 4 batch 194 loss: 2.3854198455810547\n",
      "epoch 4 batch 195 loss: 2.5834171772003174\n",
      "epoch 4 batch 196 loss: 2.3876912593841553\n",
      "epoch 4 batch 197 loss: 2.3432517051696777\n",
      "epoch 4 batch 198 loss: 2.524951457977295\n",
      "epoch 4 batch 199 loss: 2.4750685691833496\n",
      "epoch 4 batch 200 loss: 2.2699382305145264\n",
      "epoch 4 batch 201 loss: 2.1790008544921875\n",
      "epoch 4 batch 202 loss: 2.5124502182006836\n",
      "epoch 4 batch 203 loss: 2.456066131591797\n",
      "epoch 4 batch 204 loss: 2.219968795776367\n",
      "epoch 4 batch 205 loss: 2.3603854179382324\n",
      "epoch 4 batch 206 loss: 2.4388980865478516\n",
      "epoch 4 batch 207 loss: 2.5761075019836426\n",
      "epoch 4 batch 208 loss: 2.1566553115844727\n",
      "epoch 4 batch 209 loss: 2.3090977668762207\n",
      "epoch 4 batch 210 loss: 2.176238775253296\n",
      "epoch 4 batch 211 loss: 2.422727108001709\n",
      "epoch 4 batch 212 loss: 2.187391757965088\n",
      "epoch 4 batch 213 loss: 2.434236764907837\n",
      "epoch 4 batch 214 loss: 2.338548183441162\n",
      "epoch 4 batch 215 loss: 2.2065465450286865\n",
      "epoch 4 batch 216 loss: 2.413909912109375\n",
      "epoch 4 batch 217 loss: 2.4670944213867188\n",
      "epoch 4 batch 218 loss: 2.3411011695861816\n",
      "epoch 4 batch 219 loss: 2.5015053749084473\n",
      "epoch 4 batch 220 loss: 2.3949880599975586\n",
      "epoch 4 batch 221 loss: 2.468543529510498\n",
      "epoch 4 batch 222 loss: 2.2012808322906494\n",
      "epoch 4 batch 223 loss: 2.1496198177337646\n",
      "epoch 4 batch 224 loss: 2.5272514820098877\n",
      "epoch 4 batch 225 loss: 2.4061059951782227\n",
      "epoch 4 batch 226 loss: 2.3163695335388184\n",
      "epoch 4 batch 227 loss: 2.4201555252075195\n",
      "epoch 4 batch 228 loss: 2.5666277408599854\n",
      "epoch 4 batch 229 loss: 2.3493971824645996\n",
      "epoch 4 batch 230 loss: 2.5601413249969482\n",
      "epoch 4 batch 231 loss: 2.6228418350219727\n",
      "epoch 4 batch 232 loss: 2.43198823928833\n",
      "epoch 4 batch 233 loss: 2.6153976917266846\n",
      "epoch 4 batch 234 loss: 2.469338893890381\n",
      "epoch 4 batch 235 loss: 2.5485246181488037\n",
      "epoch 4 batch 236 loss: 2.572394371032715\n",
      "epoch 4 batch 237 loss: 2.2582345008850098\n",
      "epoch 4 batch 238 loss: 2.3764472007751465\n",
      "epoch 4 batch 239 loss: 2.299497127532959\n",
      "epoch 4 batch 240 loss: 2.252119779586792\n",
      "epoch 4 batch 241 loss: 2.3421120643615723\n",
      "epoch 4 batch 242 loss: 2.1926748752593994\n",
      "epoch 4 batch 243 loss: 2.763888120651245\n",
      "epoch 4 batch 244 loss: 2.3334386348724365\n",
      "epoch 4 batch 245 loss: 2.343843460083008\n",
      "epoch 4 batch 246 loss: 2.3940086364746094\n",
      "epoch 4 batch 247 loss: 2.431119918823242\n",
      "epoch 4 batch 248 loss: 2.320690631866455\n",
      "epoch 4 batch 249 loss: 2.080195426940918\n",
      "epoch 4 batch 250 loss: 2.541464328765869\n",
      "epoch 4 batch 251 loss: 2.4773755073547363\n",
      "epoch 4 batch 252 loss: 2.5784900188446045\n",
      "epoch 4 batch 253 loss: 2.4465079307556152\n",
      "epoch 4 batch 254 loss: 2.322722911834717\n",
      "epoch 4 batch 255 loss: 2.0612235069274902\n",
      "epoch 4 batch 256 loss: 2.4394664764404297\n",
      "epoch 4 batch 257 loss: 2.8850111961364746\n",
      "epoch 4 batch 258 loss: 2.4342029094696045\n",
      "epoch 4 batch 259 loss: 2.4456193447113037\n",
      "epoch 4 batch 260 loss: 2.353908061981201\n",
      "epoch 4 batch 261 loss: 2.5470991134643555\n",
      "epoch 4 batch 262 loss: 2.176887273788452\n",
      "epoch 4 batch 263 loss: 2.4410901069641113\n",
      "epoch 4 batch 264 loss: 2.279913902282715\n",
      "epoch 4 batch 265 loss: 2.4613730907440186\n",
      "epoch 4 batch 266 loss: 2.2426886558532715\n",
      "epoch 4 batch 267 loss: 2.4511003494262695\n",
      "epoch 4 batch 268 loss: 2.3297691345214844\n",
      "epoch 4 batch 269 loss: 2.2454919815063477\n",
      "epoch 4 batch 270 loss: 2.542379379272461\n",
      "epoch 4 batch 271 loss: 2.7408018112182617\n",
      "epoch 4 batch 272 loss: 2.3274478912353516\n",
      "epoch 4 batch 273 loss: 2.1350433826446533\n",
      "epoch 4 batch 274 loss: 2.3060364723205566\n",
      "epoch 4 batch 275 loss: 2.2492294311523438\n",
      "epoch 4 batch 276 loss: 2.36023211479187\n",
      "epoch 4 batch 277 loss: 2.2681031227111816\n",
      "epoch 4 batch 278 loss: 2.242361068725586\n",
      "epoch 4 batch 279 loss: 2.4641671180725098\n",
      "epoch 4 batch 280 loss: 2.3619046211242676\n",
      "epoch 4 batch 281 loss: 2.393646478652954\n",
      "epoch 4 batch 282 loss: 2.596778392791748\n",
      "epoch 4 batch 283 loss: 2.400662422180176\n",
      "epoch 4 batch 284 loss: 2.3388304710388184\n",
      "epoch 4 batch 285 loss: 2.285898208618164\n",
      "epoch 4 batch 286 loss: 2.368818759918213\n",
      "epoch 4 batch 287 loss: 2.170750141143799\n",
      "epoch 4 batch 288 loss: 2.37888240814209\n",
      "epoch 4 batch 289 loss: 2.5356929302215576\n",
      "epoch 4 batch 290 loss: 2.245945930480957\n",
      "epoch 4 batch 291 loss: 2.575206756591797\n",
      "epoch 4 batch 292 loss: 2.5377390384674072\n",
      "epoch 4 batch 293 loss: 2.2761499881744385\n",
      "epoch 4 batch 294 loss: 2.1984705924987793\n",
      "epoch 4 batch 295 loss: 2.6839609146118164\n",
      "epoch 4 batch 296 loss: 2.0664141178131104\n",
      "epoch 4 batch 297 loss: 2.394556999206543\n",
      "epoch 4 batch 298 loss: 2.5163025856018066\n",
      "epoch 4 batch 299 loss: 2.2290914058685303\n",
      "epoch 4 batch 300 loss: 2.2903144359588623\n",
      "epoch 4 batch 301 loss: 2.430910348892212\n",
      "epoch 4 batch 302 loss: 2.5382988452911377\n",
      "epoch 4 batch 303 loss: 2.3342151641845703\n",
      "epoch 4 batch 304 loss: 2.261836528778076\n",
      "epoch 4 batch 305 loss: 2.2792563438415527\n",
      "epoch 4 batch 306 loss: 2.2784743309020996\n",
      "epoch 4 batch 307 loss: 2.5833022594451904\n",
      "epoch 4 batch 308 loss: 2.703695297241211\n",
      "epoch 4 batch 309 loss: 2.271979570388794\n",
      "epoch 4 batch 310 loss: 2.359103202819824\n",
      "epoch 4 batch 311 loss: 2.3765392303466797\n",
      "epoch 4 batch 312 loss: 2.5049498081207275\n",
      "epoch 4 batch 313 loss: 2.233539342880249\n",
      "epoch 4 batch 314 loss: 2.254641056060791\n",
      "epoch 4 batch 315 loss: 2.165466547012329\n",
      "epoch 4 batch 316 loss: 2.373121738433838\n",
      "epoch 4 batch 317 loss: 2.214046001434326\n",
      "epoch 4 batch 318 loss: 2.4217689037323\n",
      "epoch 4 batch 319 loss: 2.302922248840332\n",
      "epoch 4 batch 320 loss: 2.1656503677368164\n",
      "epoch 4 batch 321 loss: 2.247598171234131\n",
      "epoch 4 batch 322 loss: 2.311450481414795\n",
      "epoch 4 batch 323 loss: 2.213440418243408\n",
      "epoch 4 batch 324 loss: 2.4112942218780518\n",
      "epoch 4 batch 325 loss: 2.441028118133545\n",
      "epoch 4 batch 326 loss: 2.3855183124542236\n",
      "epoch 4 batch 327 loss: 2.602423667907715\n",
      "epoch 4 batch 328 loss: 2.2857635021209717\n",
      "epoch 4 batch 329 loss: 2.3207545280456543\n",
      "epoch 4 batch 330 loss: 2.352163553237915\n",
      "epoch 4 batch 331 loss: 2.210775375366211\n",
      "epoch 4 batch 332 loss: 2.4031624794006348\n",
      "epoch 4 batch 333 loss: 2.3812999725341797\n",
      "epoch 4 batch 334 loss: 2.3230533599853516\n",
      "epoch 4 batch 335 loss: 2.356987237930298\n",
      "epoch 4 batch 336 loss: 2.286233901977539\n",
      "epoch 4 batch 337 loss: 2.458469867706299\n",
      "epoch 4 batch 338 loss: 2.5006070137023926\n",
      "epoch 4 batch 339 loss: 2.494847297668457\n",
      "epoch 4 batch 340 loss: 2.3667588233947754\n",
      "epoch 4 batch 341 loss: 2.326700210571289\n",
      "epoch 4 batch 342 loss: 2.3572306632995605\n",
      "epoch 4 batch 343 loss: 2.3856189250946045\n",
      "epoch 4 batch 344 loss: 2.2129335403442383\n",
      "epoch 4 batch 345 loss: 2.5437440872192383\n",
      "epoch 4 batch 346 loss: 2.407090663909912\n",
      "epoch 4 batch 347 loss: 2.273303747177124\n",
      "epoch 4 batch 348 loss: 2.21226167678833\n",
      "epoch 4 batch 349 loss: 2.2984817028045654\n",
      "epoch 4 batch 350 loss: 2.2191829681396484\n",
      "epoch 4 batch 351 loss: 2.270594358444214\n",
      "epoch 4 batch 352 loss: 2.407839775085449\n",
      "epoch 4 batch 353 loss: 2.295319080352783\n",
      "epoch 4 batch 354 loss: 2.054807662963867\n",
      "epoch 4 batch 355 loss: 2.299588441848755\n",
      "epoch 4 batch 356 loss: 2.5264246463775635\n",
      "epoch 4 batch 357 loss: 2.0967953205108643\n",
      "epoch 4 batch 358 loss: 2.3758654594421387\n",
      "epoch 4 batch 359 loss: 2.2454891204833984\n",
      "epoch 4 batch 360 loss: 2.7699429988861084\n",
      "epoch 4 batch 361 loss: 2.439481496810913\n",
      "epoch 4 batch 362 loss: 2.3847808837890625\n",
      "epoch 4 batch 363 loss: 2.267028331756592\n",
      "epoch 4 batch 364 loss: 2.2936015129089355\n",
      "epoch 4 batch 365 loss: 2.440836191177368\n",
      "epoch 4 batch 366 loss: 2.221081256866455\n",
      "epoch 4 batch 367 loss: 2.5425424575805664\n",
      "epoch 4 batch 368 loss: 2.255825996398926\n",
      "epoch 4 batch 369 loss: 2.2943496704101562\n",
      "epoch 4 batch 370 loss: 2.441666603088379\n",
      "epoch 4 batch 371 loss: 2.3144631385803223\n",
      "epoch 4 batch 372 loss: 2.204723596572876\n",
      "epoch 4 batch 373 loss: 2.273428440093994\n",
      "epoch 4 batch 374 loss: 2.2037980556488037\n",
      "epoch 4 batch 375 loss: 2.408696413040161\n",
      "epoch 4 batch 376 loss: 2.3193540573120117\n",
      "epoch 4 batch 377 loss: 2.559164524078369\n",
      "epoch 4 batch 378 loss: 2.187504768371582\n",
      "epoch 4 batch 379 loss: 2.2004992961883545\n",
      "epoch 4 batch 380 loss: 2.4354043006896973\n",
      "epoch 4 batch 381 loss: 2.316009521484375\n",
      "epoch 4 batch 382 loss: 2.2329657077789307\n",
      "epoch 4 batch 383 loss: 2.3301548957824707\n",
      "epoch 4 batch 384 loss: 2.3459079265594482\n",
      "epoch 4 batch 385 loss: 2.2837300300598145\n",
      "epoch 4 batch 386 loss: 2.0783820152282715\n",
      "epoch 4 batch 387 loss: 2.2287869453430176\n",
      "epoch 4 batch 388 loss: 2.451711416244507\n",
      "epoch 4 batch 389 loss: 2.700824499130249\n",
      "epoch 4 batch 390 loss: 2.3663289546966553\n",
      "epoch 4 batch 391 loss: 2.2968153953552246\n",
      "epoch 4 batch 392 loss: 2.425863742828369\n",
      "epoch 4 batch 393 loss: 2.29440975189209\n",
      "epoch 4 batch 394 loss: 2.285677909851074\n",
      "epoch 4 batch 395 loss: 2.648092746734619\n",
      "epoch 4 batch 396 loss: 2.602787494659424\n",
      "epoch 4 batch 397 loss: 2.4167001247406006\n",
      "epoch 4 batch 398 loss: 2.3334624767303467\n",
      "epoch 4 batch 399 loss: 2.4750044345855713\n",
      "epoch 4 batch 400 loss: 2.5078330039978027\n",
      "epoch 4 batch 401 loss: 2.367152452468872\n",
      "epoch 4 batch 402 loss: 2.3634026050567627\n",
      "epoch 4 batch 403 loss: 2.458639621734619\n",
      "epoch 4 batch 404 loss: 2.4019615650177\n",
      "epoch 4 batch 405 loss: 2.318535089492798\n",
      "epoch 4 batch 406 loss: 2.2027876377105713\n",
      "epoch 4 batch 407 loss: 2.131135940551758\n",
      "epoch 4 batch 408 loss: 2.2577037811279297\n",
      "epoch 4 batch 409 loss: 2.4440581798553467\n",
      "epoch 4 batch 410 loss: 2.379580497741699\n",
      "epoch 4 batch 411 loss: 2.4003310203552246\n",
      "epoch 4 batch 412 loss: 2.238462448120117\n",
      "epoch 4 batch 413 loss: 2.1611571311950684\n",
      "epoch 4 batch 414 loss: 2.4403486251831055\n",
      "epoch 4 batch 415 loss: 2.196352481842041\n",
      "epoch 4 batch 416 loss: 2.316497325897217\n",
      "epoch 4 batch 417 loss: 2.3581438064575195\n",
      "epoch 4 batch 418 loss: 2.2366437911987305\n",
      "epoch 4 batch 419 loss: 2.4023349285125732\n",
      "epoch 4 batch 420 loss: 2.407712459564209\n",
      "epoch 4 batch 421 loss: 2.1779215335845947\n",
      "epoch 4 batch 422 loss: 2.1886062622070312\n",
      "epoch 4 batch 423 loss: 2.3595359325408936\n",
      "epoch 4 batch 424 loss: 2.2634425163269043\n",
      "epoch 4 batch 425 loss: 2.4524881839752197\n",
      "epoch 4 batch 426 loss: 2.289079189300537\n",
      "epoch 4 batch 427 loss: 2.601377487182617\n",
      "epoch 4 batch 428 loss: 2.516155242919922\n",
      "epoch 4 batch 429 loss: 2.4012560844421387\n",
      "epoch 4 batch 430 loss: 2.287571430206299\n",
      "epoch 4 batch 431 loss: 2.59472918510437\n",
      "epoch 4 batch 432 loss: 2.442103147506714\n",
      "epoch 4 batch 433 loss: 2.502145290374756\n",
      "epoch 4 batch 434 loss: 2.179805278778076\n",
      "epoch 4 batch 435 loss: 2.3943936824798584\n",
      "epoch 4 batch 436 loss: 2.3346800804138184\n",
      "epoch 4 batch 437 loss: 2.0975048542022705\n",
      "epoch 4 batch 438 loss: 2.4866280555725098\n",
      "epoch 4 batch 439 loss: 2.235970973968506\n",
      "epoch 4 batch 440 loss: 2.244576930999756\n",
      "epoch 4 batch 441 loss: 2.367722749710083\n",
      "epoch 4 batch 442 loss: 2.2886228561401367\n",
      "epoch 4 batch 443 loss: 2.01992130279541\n",
      "epoch 4 batch 444 loss: 2.51179838180542\n",
      "epoch 4 batch 445 loss: 2.4048876762390137\n",
      "epoch 4 batch 446 loss: 2.180994987487793\n",
      "epoch 4 batch 447 loss: 2.2233290672302246\n",
      "epoch 4 batch 448 loss: 2.283198356628418\n",
      "epoch 4 batch 449 loss: 2.525620460510254\n",
      "epoch 4 batch 450 loss: 2.308924436569214\n",
      "epoch 4 batch 451 loss: 2.3413901329040527\n",
      "epoch 4 batch 452 loss: 2.486691951751709\n",
      "epoch 4 batch 453 loss: 2.5347230434417725\n",
      "epoch 4 batch 454 loss: 2.380894422531128\n",
      "epoch 4 batch 455 loss: 2.242825746536255\n",
      "epoch 4 batch 456 loss: 2.390516996383667\n",
      "epoch 4 batch 457 loss: 2.3645739555358887\n",
      "epoch 4 batch 458 loss: 2.4039855003356934\n",
      "epoch 4 batch 459 loss: 2.1487629413604736\n",
      "epoch 4 batch 460 loss: 2.1872687339782715\n",
      "epoch 4 batch 461 loss: 2.4284348487854004\n",
      "epoch 4 batch 462 loss: 2.307772159576416\n",
      "epoch 4 batch 463 loss: 2.5397050380706787\n",
      "epoch 4 batch 464 loss: 2.4322664737701416\n",
      "epoch 4 batch 465 loss: 2.6029839515686035\n",
      "epoch 4 batch 466 loss: 2.2561564445495605\n",
      "epoch 4 batch 467 loss: 2.360593557357788\n",
      "epoch 4 batch 468 loss: 2.4470977783203125\n",
      "epoch 4 batch 469 loss: 2.3102917671203613\n",
      "epoch 4 batch 470 loss: 2.608961582183838\n",
      "epoch 4 batch 471 loss: 2.3226912021636963\n",
      "epoch 4 batch 472 loss: 2.400048017501831\n",
      "epoch 4 batch 473 loss: 2.667452812194824\n",
      "epoch 4 batch 474 loss: 2.431241273880005\n",
      "epoch 4 batch 475 loss: 2.3006691932678223\n",
      "epoch 4 batch 476 loss: 2.3783349990844727\n",
      "epoch 4 batch 477 loss: 2.5210442543029785\n",
      "epoch 4 batch 478 loss: 2.5063843727111816\n",
      "epoch 4 batch 479 loss: 2.310965061187744\n",
      "epoch 4 batch 480 loss: 2.247786521911621\n",
      "epoch 4 batch 481 loss: 2.4692153930664062\n",
      "epoch 4 batch 482 loss: 2.2805657386779785\n",
      "epoch 4 batch 483 loss: 2.2977676391601562\n",
      "epoch 4 batch 484 loss: 2.3367180824279785\n",
      "epoch 4 batch 485 loss: 2.271718978881836\n",
      "epoch 4 batch 486 loss: 2.1936304569244385\n",
      "epoch 4 batch 487 loss: 2.2714076042175293\n",
      "epoch 4 batch 488 loss: 2.1852951049804688\n",
      "epoch 4 batch 489 loss: 2.279116153717041\n",
      "epoch 4 batch 490 loss: 2.253676176071167\n",
      "epoch 4 batch 491 loss: 2.165189504623413\n",
      "epoch 4 batch 492 loss: 2.1321849822998047\n",
      "epoch 4 batch 493 loss: 2.337695360183716\n",
      "epoch 4 batch 494 loss: 2.343975782394409\n",
      "epoch 4 batch 495 loss: 2.296825647354126\n",
      "epoch 4 batch 496 loss: 2.5301246643066406\n",
      "epoch 4 batch 497 loss: 2.234440326690674\n",
      "epoch 4 batch 498 loss: 2.302886486053467\n",
      "epoch 4 batch 499 loss: 2.462843179702759\n",
      "epoch 4 batch 500 loss: 2.213416337966919\n",
      "epoch 4 batch 501 loss: 2.3724989891052246\n",
      "epoch 4 batch 502 loss: 2.4149179458618164\n",
      "epoch 4 batch 503 loss: 2.4428391456604004\n",
      "epoch 4 batch 504 loss: 2.487720251083374\n",
      "epoch 4 batch 505 loss: 2.285862684249878\n",
      "epoch 4 batch 506 loss: 2.429091453552246\n",
      "epoch 4 batch 507 loss: 2.279097080230713\n",
      "epoch 4 batch 508 loss: 2.4167518615722656\n",
      "epoch 4 batch 509 loss: 2.2846920490264893\n",
      "epoch 4 batch 510 loss: 2.680366277694702\n",
      "epoch 4 batch 511 loss: 2.4735710620880127\n",
      "epoch 4 batch 512 loss: 2.3603217601776123\n",
      "epoch 4 batch 513 loss: 2.2682063579559326\n",
      "epoch 4 batch 514 loss: 2.2109625339508057\n",
      "epoch 4 batch 515 loss: 2.1774699687957764\n",
      "epoch 4 batch 516 loss: 2.4784679412841797\n",
      "epoch 4 batch 517 loss: 2.3986754417419434\n",
      "epoch 4 batch 518 loss: 2.4010567665100098\n",
      "epoch 4 batch 519 loss: 2.5470402240753174\n",
      "epoch 4 batch 520 loss: 2.321455955505371\n",
      "epoch 4 batch 521 loss: 2.301384210586548\n",
      "epoch 4 batch 522 loss: 2.4390053749084473\n",
      "epoch 4 batch 523 loss: 2.2664542198181152\n",
      "epoch 4 batch 524 loss: 2.403097152709961\n",
      "epoch 4 batch 525 loss: 2.65937876701355\n",
      "epoch 4 batch 526 loss: 2.537745475769043\n",
      "epoch 4 batch 527 loss: 2.5533905029296875\n",
      "epoch 4 batch 528 loss: 2.275667190551758\n",
      "epoch 4 batch 529 loss: 2.263800621032715\n",
      "epoch 4 batch 530 loss: 2.3150062561035156\n",
      "epoch 4 batch 531 loss: 2.4034814834594727\n",
      "epoch 4 batch 532 loss: 2.314718723297119\n",
      "epoch 4 batch 533 loss: 2.438098192214966\n",
      "epoch 4 batch 534 loss: 2.541914939880371\n",
      "epoch 4 batch 535 loss: 2.442100763320923\n",
      "epoch 4 batch 536 loss: 2.3899312019348145\n",
      "epoch 4 batch 537 loss: 2.2308311462402344\n",
      "epoch 4 batch 538 loss: 2.236236572265625\n",
      "epoch 4 batch 539 loss: 2.680828094482422\n",
      "epoch 4 batch 540 loss: 2.300632953643799\n",
      "epoch 4 batch 541 loss: 2.5370914936065674\n",
      "epoch 4 batch 542 loss: 2.4483752250671387\n",
      "epoch 4 batch 543 loss: 2.1632308959960938\n",
      "epoch 4 batch 544 loss: 2.603668689727783\n",
      "epoch 4 batch 545 loss: 2.4083619117736816\n",
      "epoch 4 batch 546 loss: 2.5087602138519287\n",
      "epoch 4 batch 547 loss: 2.370129346847534\n",
      "epoch 4 batch 548 loss: 2.3768584728240967\n",
      "epoch 4 batch 549 loss: 2.166821002960205\n",
      "epoch 4 batch 550 loss: 2.3982391357421875\n",
      "epoch 4 batch 551 loss: 2.7276663780212402\n",
      "epoch 4 batch 552 loss: 2.3340861797332764\n",
      "epoch 4 batch 553 loss: 2.2933125495910645\n",
      "epoch 4 batch 554 loss: 2.1337196826934814\n",
      "epoch 4 batch 555 loss: 2.367706060409546\n",
      "epoch 4 batch 556 loss: 2.3813822269439697\n",
      "epoch 4 batch 557 loss: 2.5295863151550293\n",
      "epoch 4 batch 558 loss: 2.6461730003356934\n",
      "epoch 4 batch 559 loss: 2.1537013053894043\n",
      "epoch 4 batch 560 loss: 2.3720860481262207\n",
      "epoch 4 batch 561 loss: 2.221595048904419\n",
      "epoch 4 batch 562 loss: 2.251919984817505\n",
      "epoch 4 batch 563 loss: 2.4160263538360596\n",
      "epoch 4 batch 564 loss: 2.6674485206604004\n",
      "epoch 4 batch 565 loss: 2.171727180480957\n",
      "epoch 4 batch 566 loss: 2.3851637840270996\n",
      "epoch 4 batch 567 loss: 2.3429439067840576\n",
      "epoch 4 batch 568 loss: 2.4403178691864014\n",
      "epoch 4 batch 569 loss: 2.501286029815674\n",
      "epoch 4 batch 570 loss: 2.3060948848724365\n",
      "epoch 4 batch 571 loss: 2.2082412242889404\n",
      "epoch 4 batch 572 loss: 2.133180618286133\n",
      "epoch 4 batch 573 loss: 2.3972480297088623\n",
      "epoch 4 batch 574 loss: 2.29404354095459\n",
      "epoch 4 batch 575 loss: 2.3957176208496094\n",
      "epoch 4 batch 576 loss: 2.242776870727539\n",
      "epoch 4 batch 577 loss: 2.3106203079223633\n",
      "epoch 4 batch 578 loss: 2.3145463466644287\n",
      "epoch 4 batch 579 loss: 2.4200806617736816\n",
      "epoch 4 batch 580 loss: 2.2919583320617676\n",
      "epoch 4 batch 581 loss: 2.261699676513672\n",
      "epoch 4 batch 582 loss: 2.2791941165924072\n",
      "epoch 4 batch 583 loss: 2.522089719772339\n",
      "epoch 4 batch 584 loss: 2.3749375343322754\n",
      "epoch 4 batch 585 loss: 2.3385250568389893\n",
      "epoch 4 batch 586 loss: 2.1089813709259033\n",
      "epoch 4 batch 587 loss: 2.0503482818603516\n",
      "epoch 4 batch 588 loss: 2.374629497528076\n",
      "epoch 4 batch 589 loss: 2.294113874435425\n",
      "epoch 4 batch 590 loss: 2.2681221961975098\n",
      "epoch 4 batch 591 loss: 2.426635265350342\n",
      "epoch 4 batch 592 loss: 2.582880973815918\n",
      "epoch 4 batch 593 loss: 2.3258185386657715\n",
      "epoch 4 batch 594 loss: 2.491940498352051\n",
      "epoch 4 batch 595 loss: 2.276254653930664\n",
      "epoch 4 batch 596 loss: 2.3049702644348145\n",
      "epoch 4 batch 597 loss: 2.5309672355651855\n",
      "epoch 4 batch 598 loss: 2.325831413269043\n",
      "epoch 4 batch 599 loss: 2.500990390777588\n",
      "epoch 4 batch 600 loss: 2.265963554382324\n",
      "epoch 4 batch 601 loss: 2.285999298095703\n",
      "epoch 4 batch 602 loss: 2.319709539413452\n",
      "epoch 4 batch 603 loss: 2.1655569076538086\n",
      "epoch 4 batch 604 loss: 2.2157037258148193\n",
      "epoch 4 batch 605 loss: 2.348135471343994\n",
      "epoch 4 batch 606 loss: 2.1686697006225586\n",
      "epoch 4 batch 607 loss: 2.5189990997314453\n",
      "epoch 4 batch 608 loss: 2.2307214736938477\n",
      "epoch 4 batch 609 loss: 2.3491108417510986\n",
      "epoch 4 batch 610 loss: 2.472799301147461\n",
      "epoch 4 batch 611 loss: 2.122267246246338\n",
      "epoch 4 batch 612 loss: 2.250744342803955\n",
      "epoch 4 batch 613 loss: 2.4375298023223877\n",
      "epoch 4 batch 614 loss: 2.2068049907684326\n",
      "epoch 4 batch 615 loss: 2.4731101989746094\n",
      "epoch 4 batch 616 loss: 2.354076385498047\n",
      "epoch 4 batch 617 loss: 2.2601771354675293\n",
      "epoch 4 batch 618 loss: 2.332388401031494\n",
      "epoch 4 batch 619 loss: 2.2136998176574707\n",
      "epoch 4 batch 620 loss: 2.365730047225952\n",
      "epoch 4 batch 621 loss: 2.481780767440796\n",
      "epoch 4 batch 622 loss: 2.464390516281128\n",
      "epoch 4 batch 623 loss: 2.245934009552002\n",
      "epoch 4 batch 624 loss: 2.345707893371582\n",
      "epoch 4 batch 625 loss: 2.1477997303009033\n",
      "epoch 4 batch 626 loss: 2.2454323768615723\n",
      "epoch 4 batch 627 loss: 2.1094088554382324\n",
      "epoch 4 batch 628 loss: 2.3383193016052246\n",
      "epoch 4 batch 629 loss: 2.269927978515625\n",
      "epoch 4 batch 630 loss: 2.3605380058288574\n",
      "epoch 4 batch 631 loss: 2.5617153644561768\n",
      "epoch 4 batch 632 loss: 2.1869699954986572\n",
      "epoch 4 batch 633 loss: 2.2787728309631348\n",
      "epoch 4 batch 634 loss: 2.4505672454833984\n",
      "epoch 4 batch 635 loss: 2.3293182849884033\n",
      "epoch 4 batch 636 loss: 2.2952919006347656\n",
      "epoch 4 batch 637 loss: 2.347733974456787\n",
      "epoch 4 batch 638 loss: 2.1481571197509766\n",
      "epoch 4 batch 639 loss: 2.27852725982666\n",
      "epoch 4 batch 640 loss: 2.821281671524048\n",
      "epoch 4 batch 641 loss: 2.3607640266418457\n",
      "epoch 4 batch 642 loss: 2.3779664039611816\n",
      "epoch 4 batch 643 loss: 2.7652368545532227\n",
      "epoch 4 batch 644 loss: 2.248159408569336\n",
      "epoch 4 batch 645 loss: 2.216660499572754\n",
      "epoch 4 batch 646 loss: 2.3413825035095215\n",
      "epoch 4 batch 647 loss: 2.45021390914917\n",
      "epoch 4 batch 648 loss: 2.2670183181762695\n",
      "epoch 4 batch 649 loss: 2.317903518676758\n",
      "epoch 4 batch 650 loss: 2.4164071083068848\n",
      "epoch 4 batch 651 loss: 2.274899959564209\n",
      "epoch 4 batch 652 loss: 2.2380309104919434\n",
      "epoch 4 batch 653 loss: 2.3970046043395996\n",
      "epoch 4 batch 654 loss: 2.211913585662842\n",
      "epoch 4 batch 655 loss: 2.4865217208862305\n",
      "epoch 4 batch 656 loss: 2.0928444862365723\n",
      "epoch 4 batch 657 loss: 2.0358738899230957\n",
      "epoch 4 batch 658 loss: 2.4150214195251465\n",
      "epoch 4 batch 659 loss: 2.267613410949707\n",
      "epoch 4 batch 660 loss: 2.1400418281555176\n",
      "epoch 4 batch 661 loss: 2.427713394165039\n",
      "epoch 4 batch 662 loss: 2.3347649574279785\n",
      "epoch 4 batch 663 loss: 2.3202645778656006\n",
      "epoch 4 batch 664 loss: 2.1819236278533936\n",
      "epoch 4 batch 665 loss: 2.2380003929138184\n",
      "epoch 4 batch 666 loss: 2.240102767944336\n",
      "epoch 4 batch 667 loss: 2.381288766860962\n",
      "epoch 4 batch 668 loss: 2.3206534385681152\n",
      "epoch 4 batch 669 loss: 2.2098560333251953\n",
      "epoch 4 batch 670 loss: 2.3428969383239746\n",
      "epoch 4 batch 671 loss: 2.499657154083252\n",
      "epoch 4 batch 672 loss: 1.9874361753463745\n",
      "epoch 4 batch 673 loss: 2.294076442718506\n",
      "epoch 4 batch 674 loss: 2.1141557693481445\n",
      "epoch 4 batch 675 loss: 2.276270866394043\n",
      "epoch 4 batch 676 loss: 2.2168052196502686\n",
      "epoch 4 batch 677 loss: 2.2345948219299316\n",
      "epoch 4 batch 678 loss: 2.2684335708618164\n",
      "epoch 4 batch 679 loss: 2.3003122806549072\n",
      "epoch 4 batch 680 loss: 2.132171630859375\n",
      "epoch 4 batch 681 loss: 2.195669174194336\n",
      "epoch 4 batch 682 loss: 2.4239494800567627\n",
      "epoch 4 batch 683 loss: 2.401726722717285\n",
      "epoch 4 batch 684 loss: 2.6192502975463867\n",
      "epoch 4 batch 685 loss: 2.384334087371826\n",
      "epoch 4 batch 686 loss: 2.2276175022125244\n",
      "epoch 4 batch 687 loss: 2.3750007152557373\n",
      "epoch 4 batch 688 loss: 2.5194344520568848\n",
      "epoch 4 batch 689 loss: 2.0822434425354004\n",
      "epoch 4 batch 690 loss: 2.6202425956726074\n",
      "epoch 4 batch 691 loss: 2.5235366821289062\n",
      "epoch 4 batch 692 loss: 2.301741123199463\n",
      "epoch 4 batch 693 loss: 2.3470544815063477\n",
      "epoch 4 batch 694 loss: 2.286623954772949\n",
      "epoch 4 batch 695 loss: 2.3136348724365234\n",
      "epoch 4 batch 696 loss: 2.3939054012298584\n",
      "epoch 4 batch 697 loss: 2.705854654312134\n",
      "epoch 4 batch 698 loss: 2.618851661682129\n",
      "epoch 4 batch 699 loss: 2.466102361679077\n",
      "epoch 4 batch 700 loss: 2.453801393508911\n",
      "epoch 4 batch 701 loss: 2.3594565391540527\n",
      "epoch 4 batch 702 loss: 2.5017008781433105\n",
      "epoch 4 batch 703 loss: 2.5453057289123535\n",
      "epoch 4 batch 704 loss: 2.2967898845672607\n",
      "epoch 4 batch 705 loss: 2.2862560749053955\n",
      "epoch 4 batch 706 loss: 2.4655840396881104\n",
      "epoch 4 batch 707 loss: 2.3759782314300537\n",
      "epoch 4 batch 708 loss: 2.3767428398132324\n",
      "epoch 4 batch 709 loss: 2.3682055473327637\n",
      "epoch 4 batch 710 loss: 2.3366637229919434\n",
      "epoch 4 batch 711 loss: 2.2902584075927734\n",
      "epoch 4 batch 712 loss: 2.4150195121765137\n",
      "epoch 4 batch 713 loss: 2.5688910484313965\n",
      "epoch 4 batch 714 loss: 2.4699156284332275\n",
      "epoch 4 batch 715 loss: 2.1343507766723633\n",
      "epoch 4 batch 716 loss: 2.537075996398926\n",
      "epoch 4 batch 717 loss: 2.5443921089172363\n",
      "epoch 4 batch 718 loss: 2.3021931648254395\n",
      "epoch 4 batch 719 loss: 2.441206932067871\n",
      "epoch 4 batch 720 loss: 2.3147032260894775\n",
      "epoch 4 batch 721 loss: 2.3203625679016113\n",
      "epoch 4 batch 722 loss: 2.3387908935546875\n",
      "epoch 4 batch 723 loss: 2.4943721294403076\n",
      "epoch 4 batch 724 loss: 2.4658546447753906\n",
      "epoch 4 batch 725 loss: 2.5590028762817383\n",
      "epoch 4 batch 726 loss: 2.240933895111084\n",
      "epoch 4 batch 727 loss: 2.2306501865386963\n",
      "epoch 4 batch 728 loss: 2.4750280380249023\n",
      "epoch 4 batch 729 loss: 2.384885787963867\n",
      "epoch 4 batch 730 loss: 2.5251829624176025\n",
      "epoch 4 batch 731 loss: 2.4146718978881836\n",
      "epoch 4 batch 732 loss: 2.297618865966797\n",
      "epoch 4 batch 733 loss: 2.186770439147949\n",
      "epoch 4 batch 734 loss: 2.5075526237487793\n",
      "epoch 4 batch 735 loss: 2.0657832622528076\n",
      "epoch 4 batch 736 loss: 2.532165765762329\n",
      "epoch 4 batch 737 loss: 2.49312686920166\n",
      "epoch 4 batch 738 loss: 2.4427807331085205\n",
      "epoch 4 batch 739 loss: 2.321499824523926\n",
      "epoch 4 batch 740 loss: 2.6514406204223633\n",
      "epoch 4 batch 741 loss: 2.4520039558410645\n",
      "epoch 4 batch 742 loss: 2.353311061859131\n",
      "epoch 4 batch 743 loss: 2.258936643600464\n",
      "epoch 4 batch 744 loss: 2.27972412109375\n",
      "epoch 4 batch 745 loss: 2.685281753540039\n",
      "epoch 4 batch 746 loss: 2.2110135555267334\n",
      "epoch 4 batch 747 loss: 2.5917980670928955\n",
      "epoch 4 batch 748 loss: 2.3306713104248047\n",
      "epoch 4 batch 749 loss: 2.1617238521575928\n",
      "epoch 4 batch 750 loss: 2.237715721130371\n",
      "epoch 4 batch 751 loss: 2.3082377910614014\n",
      "epoch 4 batch 752 loss: 2.5199766159057617\n",
      "epoch 4 batch 753 loss: 2.213883638381958\n",
      "epoch 4 batch 754 loss: 2.333979368209839\n",
      "epoch 4 batch 755 loss: 2.4225821495056152\n",
      "epoch 4 batch 756 loss: 2.3071537017822266\n",
      "epoch 4 batch 757 loss: 2.290835380554199\n",
      "epoch 4 batch 758 loss: 2.1391234397888184\n",
      "epoch 4 batch 759 loss: 2.247873544692993\n",
      "epoch 4 batch 760 loss: 2.274693012237549\n",
      "epoch 4 batch 761 loss: 2.737640380859375\n",
      "epoch 4 batch 762 loss: 2.30838680267334\n",
      "epoch 4 batch 763 loss: 2.6094844341278076\n",
      "epoch 4 batch 764 loss: 2.410177707672119\n",
      "epoch 4 batch 765 loss: 2.510833740234375\n",
      "epoch 4 batch 766 loss: 2.520224094390869\n",
      "epoch 4 batch 767 loss: 2.2807583808898926\n",
      "epoch 4 batch 768 loss: 2.566509485244751\n",
      "epoch 4 batch 769 loss: 2.3836264610290527\n",
      "epoch 4 batch 770 loss: 2.1861329078674316\n",
      "epoch 4 batch 771 loss: 2.49611759185791\n",
      "epoch 4 batch 772 loss: 2.4203410148620605\n",
      "epoch 4 batch 773 loss: 2.6851301193237305\n",
      "epoch 4 batch 774 loss: 2.485600471496582\n",
      "epoch 4 batch 775 loss: 2.2776007652282715\n",
      "epoch 4 batch 776 loss: 2.3300700187683105\n",
      "epoch 4 batch 777 loss: 2.048915147781372\n",
      "epoch 4 batch 778 loss: 2.31541109085083\n",
      "epoch 4 batch 779 loss: 2.2698986530303955\n",
      "epoch 4 batch 780 loss: 2.3144962787628174\n",
      "epoch 4 batch 781 loss: 2.1118030548095703\n",
      "epoch 4 batch 782 loss: 2.703756332397461\n",
      "epoch 4 batch 783 loss: 2.3876185417175293\n",
      "epoch 4 batch 784 loss: 2.412367343902588\n",
      "epoch 4 batch 785 loss: 2.361408233642578\n",
      "epoch 4 batch 786 loss: 2.2831995487213135\n",
      "epoch 4 batch 787 loss: 2.3263230323791504\n",
      "epoch 4 batch 788 loss: 2.240135908126831\n",
      "epoch 4 batch 789 loss: 2.451542377471924\n",
      "epoch 4 batch 790 loss: 2.4848127365112305\n",
      "epoch 4 batch 791 loss: 2.1389641761779785\n",
      "epoch 4 batch 792 loss: 2.423031806945801\n",
      "epoch 4 batch 793 loss: 2.254059076309204\n",
      "epoch 4 batch 794 loss: 2.280445098876953\n",
      "epoch 4 batch 795 loss: 2.0905349254608154\n",
      "epoch 4 batch 796 loss: 2.2815933227539062\n",
      "epoch 4 batch 797 loss: 2.4029018878936768\n",
      "epoch 4 batch 798 loss: 2.3842086791992188\n",
      "epoch 4 batch 799 loss: 2.138490676879883\n",
      "epoch 4 batch 800 loss: 2.603522300720215\n",
      "epoch 4 batch 801 loss: 2.560765266418457\n",
      "epoch 4 batch 802 loss: 2.4438509941101074\n",
      "epoch 4 batch 803 loss: 2.270967721939087\n",
      "epoch 4 batch 804 loss: 2.3250231742858887\n",
      "epoch 4 batch 805 loss: 2.4194862842559814\n",
      "epoch 4 batch 806 loss: 2.2015984058380127\n",
      "epoch 4 batch 807 loss: 2.498673677444458\n",
      "epoch 4 batch 808 loss: 2.4048938751220703\n",
      "epoch 4 batch 809 loss: 2.431628704071045\n",
      "epoch 4 batch 810 loss: 2.484219551086426\n",
      "epoch 4 batch 811 loss: 2.282118320465088\n",
      "epoch 4 batch 812 loss: 2.159677028656006\n",
      "epoch 4 batch 813 loss: 2.3675122261047363\n",
      "epoch 4 batch 814 loss: 2.4395804405212402\n",
      "epoch 4 batch 815 loss: 2.2963509559631348\n",
      "epoch 4 batch 816 loss: 2.2346999645233154\n",
      "epoch 4 batch 817 loss: 2.2395639419555664\n",
      "epoch 4 batch 818 loss: 2.2982964515686035\n",
      "epoch 4 batch 819 loss: 2.3891913890838623\n",
      "epoch 4 batch 820 loss: 2.1798598766326904\n",
      "epoch 4 batch 821 loss: 2.3103504180908203\n",
      "epoch 4 batch 822 loss: 2.292036533355713\n",
      "epoch 4 batch 823 loss: 2.513195514678955\n",
      "epoch 4 batch 824 loss: 2.395444393157959\n",
      "epoch 4 batch 825 loss: 2.3596441745758057\n",
      "epoch 4 batch 826 loss: 2.3773152828216553\n",
      "epoch 4 batch 827 loss: 2.434812068939209\n",
      "epoch 4 batch 828 loss: 2.1472129821777344\n",
      "epoch 4 batch 829 loss: 2.344099998474121\n",
      "epoch 4 batch 830 loss: 2.4636471271514893\n",
      "epoch 4 batch 831 loss: 2.2367265224456787\n",
      "epoch 4 batch 832 loss: 2.729580879211426\n",
      "epoch 4 batch 833 loss: 2.602003335952759\n",
      "epoch 4 batch 834 loss: 2.3451642990112305\n",
      "epoch 4 batch 835 loss: 2.3090200424194336\n",
      "epoch 4 batch 836 loss: 2.132518768310547\n",
      "epoch 4 batch 837 loss: 2.0667216777801514\n",
      "epoch 4 batch 838 loss: 2.3437187671661377\n",
      "epoch 4 batch 839 loss: 2.299121379852295\n",
      "epoch 4 batch 840 loss: 2.408482789993286\n",
      "epoch 4 batch 841 loss: 2.542571783065796\n",
      "epoch 4 batch 842 loss: 2.412374258041382\n",
      "epoch 4 batch 843 loss: 2.6431710720062256\n",
      "epoch 4 batch 844 loss: 2.0027971267700195\n",
      "epoch 4 batch 845 loss: 2.2940173149108887\n",
      "epoch 4 batch 846 loss: 2.4533257484436035\n",
      "epoch 4 batch 847 loss: 2.2010722160339355\n",
      "epoch 4 batch 848 loss: 2.179349184036255\n",
      "epoch 4 batch 849 loss: 2.2799072265625\n",
      "epoch 4 batch 850 loss: 2.1217403411865234\n",
      "epoch 4 batch 851 loss: 2.1224043369293213\n",
      "epoch 4 batch 852 loss: 2.1099069118499756\n",
      "epoch 4 batch 853 loss: 2.1568355560302734\n",
      "epoch 4 batch 854 loss: 2.1449031829833984\n",
      "epoch 4 batch 855 loss: 2.509779453277588\n",
      "epoch 4 batch 856 loss: 2.415001392364502\n",
      "epoch 4 batch 857 loss: 2.6508989334106445\n",
      "epoch 4 batch 858 loss: 2.407583236694336\n",
      "epoch 4 batch 859 loss: 2.4203858375549316\n",
      "epoch 4 batch 860 loss: 2.051314353942871\n",
      "epoch 4 batch 861 loss: 2.3671889305114746\n",
      "epoch 4 batch 862 loss: 2.6695797443389893\n",
      "epoch 4 batch 863 loss: 2.2238621711730957\n",
      "epoch 4 batch 864 loss: 2.372898578643799\n",
      "epoch 4 batch 865 loss: 2.461343765258789\n",
      "epoch 4 batch 866 loss: 2.4407811164855957\n",
      "epoch 4 batch 867 loss: 2.255507469177246\n",
      "epoch 4 batch 868 loss: 2.432800769805908\n",
      "epoch 4 batch 869 loss: 2.257150650024414\n",
      "epoch 4 batch 870 loss: 2.279299020767212\n",
      "epoch 4 batch 871 loss: 2.9806296825408936\n",
      "epoch 4 batch 872 loss: 2.4382100105285645\n",
      "epoch 4 batch 873 loss: 2.019899606704712\n",
      "epoch 4 batch 874 loss: 2.1446077823638916\n",
      "epoch 4 batch 875 loss: 2.2051353454589844\n",
      "epoch 4 batch 876 loss: 2.4545204639434814\n",
      "epoch 4 batch 877 loss: 2.500795364379883\n",
      "epoch 4 batch 878 loss: 2.284679412841797\n",
      "epoch 4 batch 879 loss: 2.3534488677978516\n",
      "epoch 4 batch 880 loss: 2.549025774002075\n",
      "epoch 4 batch 881 loss: 2.3246867656707764\n",
      "epoch 4 batch 882 loss: 2.346254348754883\n",
      "epoch 4 batch 883 loss: 2.382892370223999\n",
      "epoch 4 batch 884 loss: 2.7297279834747314\n",
      "epoch 4 batch 885 loss: 2.4150238037109375\n",
      "epoch 4 batch 886 loss: 2.3383212089538574\n",
      "epoch 4 batch 887 loss: 2.33485746383667\n",
      "epoch 4 batch 888 loss: 2.482395648956299\n",
      "epoch 4 batch 889 loss: 2.3306424617767334\n",
      "epoch 4 batch 890 loss: 2.2315971851348877\n",
      "epoch 4 batch 891 loss: 2.3089022636413574\n",
      "epoch 4 batch 892 loss: 2.414295196533203\n",
      "epoch 4 batch 893 loss: 2.1883175373077393\n",
      "epoch 4 batch 894 loss: 2.350390672683716\n",
      "epoch 4 batch 895 loss: 2.1769518852233887\n",
      "epoch 4 batch 896 loss: 2.242316484451294\n",
      "epoch 4 batch 897 loss: 2.299060344696045\n",
      "epoch 4 batch 898 loss: 2.5526204109191895\n",
      "epoch 4 batch 899 loss: 2.264028549194336\n",
      "epoch 4 batch 900 loss: 2.1915526390075684\n",
      "epoch 4 batch 901 loss: 2.684767246246338\n",
      "epoch 4 batch 902 loss: 2.3626697063446045\n",
      "epoch 4 batch 903 loss: 2.2722463607788086\n",
      "epoch 4 batch 904 loss: 2.295773983001709\n",
      "epoch 4 batch 905 loss: 2.390428066253662\n",
      "epoch 4 batch 906 loss: 2.5667943954467773\n",
      "epoch 4 batch 907 loss: 2.3944501876831055\n",
      "epoch 4 batch 908 loss: 2.5562093257904053\n",
      "epoch 4 batch 909 loss: 2.3148016929626465\n",
      "epoch 4 batch 910 loss: 2.510378837585449\n",
      "epoch 4 batch 911 loss: 2.5068187713623047\n",
      "epoch 4 batch 912 loss: 2.276212692260742\n",
      "epoch 4 batch 913 loss: 2.3518595695495605\n",
      "epoch 4 batch 914 loss: 2.4830048084259033\n",
      "epoch 4 batch 915 loss: 2.1400036811828613\n",
      "epoch 4 batch 916 loss: 2.5930140018463135\n",
      "epoch 4 batch 917 loss: 2.5863261222839355\n",
      "epoch 4 batch 918 loss: 2.3110580444335938\n",
      "epoch 4 batch 919 loss: 2.3573145866394043\n",
      "epoch 4 batch 920 loss: 2.648899793624878\n",
      "epoch 4 batch 921 loss: 2.236996650695801\n",
      "epoch 4 batch 922 loss: 2.6691083908081055\n",
      "epoch 4 batch 923 loss: 2.1942977905273438\n",
      "epoch 4 batch 924 loss: 2.4180667400360107\n",
      "epoch 4 batch 925 loss: 2.231372833251953\n",
      "epoch 4 batch 926 loss: 2.2266221046447754\n",
      "epoch 4 batch 927 loss: 2.3269922733306885\n",
      "epoch 4 batch 928 loss: 2.437328338623047\n",
      "epoch 4 batch 929 loss: 2.255051612854004\n",
      "epoch 4 batch 930 loss: 2.546929121017456\n",
      "epoch 4 batch 931 loss: 2.3155534267425537\n",
      "epoch 4 batch 932 loss: 2.390631675720215\n",
      "epoch 4 batch 933 loss: 2.5222177505493164\n",
      "epoch 4 batch 934 loss: 2.439054250717163\n",
      "epoch 4 batch 935 loss: 2.0591063499450684\n",
      "epoch 4 batch 936 loss: 2.367807388305664\n",
      "epoch 4 batch 937 loss: 2.6569600105285645\n",
      "epoch 4 batch 938 loss: 2.54547381401062\n",
      "epoch 4 batch 939 loss: 2.280282974243164\n",
      "epoch 4 batch 940 loss: 2.3570709228515625\n",
      "epoch 4 batch 941 loss: 2.2444570064544678\n",
      "epoch 4 batch 942 loss: 2.1858534812927246\n",
      "epoch 4 batch 943 loss: 2.0460011959075928\n",
      "epoch 4 batch 944 loss: 2.324834108352661\n",
      "epoch 4 batch 945 loss: 2.4511728286743164\n",
      "epoch 4 batch 946 loss: 2.6781067848205566\n",
      "epoch 4 batch 947 loss: 2.2563395500183105\n",
      "epoch 4 batch 948 loss: 2.030304193496704\n",
      "epoch 4 batch 949 loss: 2.463895797729492\n",
      "epoch 4 batch 950 loss: 2.268810749053955\n",
      "epoch 4 batch 951 loss: 2.82260799407959\n",
      "epoch 4 batch 952 loss: 2.605567455291748\n",
      "epoch 4 batch 953 loss: 2.3090896606445312\n",
      "epoch 4 batch 954 loss: 2.292084217071533\n",
      "epoch 4 batch 955 loss: 2.2258665561676025\n",
      "epoch 4 batch 956 loss: 2.3730411529541016\n",
      "epoch 4 batch 957 loss: 2.1535072326660156\n",
      "epoch 4 batch 958 loss: 2.3470354080200195\n",
      "epoch 4 batch 959 loss: 2.201723575592041\n",
      "epoch 4 batch 960 loss: 2.4460206031799316\n",
      "epoch 4 batch 961 loss: 2.201901912689209\n",
      "epoch 4 batch 962 loss: 2.353315830230713\n",
      "epoch 4 batch 963 loss: 2.173158884048462\n",
      "epoch 4 batch 964 loss: 2.275125503540039\n",
      "epoch 4 batch 965 loss: 2.1802892684936523\n",
      "epoch 4 batch 966 loss: 2.3056766986846924\n",
      "epoch 4 batch 967 loss: 2.3963160514831543\n",
      "epoch 4 batch 968 loss: 2.0840845108032227\n",
      "epoch 4 batch 969 loss: 2.5043222904205322\n",
      "epoch 4 batch 970 loss: 2.320106267929077\n",
      "epoch 4 batch 971 loss: 2.20588755607605\n",
      "epoch 4 batch 972 loss: 2.360875129699707\n",
      "epoch 4 batch 973 loss: 2.334296226501465\n",
      "epoch 4 batch 974 loss: 2.4592604637145996\n",
      "epoch 4 batch 975 loss: 2.3655848503112793\n",
      "epoch 4 batch 976 loss: 2.4332101345062256\n",
      "epoch 4 batch 977 loss: 2.2232718467712402\n",
      "epoch 4 batch 978 loss: 2.393627166748047\n",
      "epoch 4 batch 979 loss: 2.215855598449707\n",
      "epoch 4 batch 980 loss: 2.2962851524353027\n",
      "epoch 4 batch 981 loss: 2.3224058151245117\n",
      "epoch 4 batch 982 loss: 2.373579978942871\n",
      "epoch 4 batch 983 loss: 2.40218448638916\n",
      "epoch 4 batch 984 loss: 2.3011600971221924\n",
      "epoch 4 batch 985 loss: 2.201368808746338\n",
      "epoch 4 batch 986 loss: 2.4446887969970703\n",
      "epoch 4 batch 987 loss: 2.2143616676330566\n",
      "epoch 4 batch 988 loss: 2.1018519401550293\n",
      "epoch 4 batch 989 loss: 2.063499927520752\n",
      "epoch 4 batch 990 loss: 2.473597526550293\n",
      "epoch 4 batch 991 loss: 2.311560869216919\n",
      "epoch 4 batch 992 loss: 2.2167506217956543\n",
      "epoch 4 batch 993 loss: 2.2180652618408203\n",
      "epoch 4 batch 994 loss: 2.156627893447876\n",
      "epoch 4 batch 995 loss: 2.278378486633301\n",
      "epoch 4 batch 996 loss: 2.4254140853881836\n",
      "epoch 4 batch 997 loss: 2.2006328105926514\n",
      "epoch 4 batch 998 loss: 2.5017881393432617\n",
      "epoch 4 batch 999 loss: 2.4622554779052734\n",
      "epoch 4 batch 1000 loss: 2.115330219268799\n",
      "epoch 4 batch 1001 loss: 2.212502956390381\n",
      "epoch 4 batch 1002 loss: 2.264375925064087\n",
      "epoch 4 batch 1003 loss: 2.253880023956299\n",
      "epoch 4 batch 1004 loss: 2.405043363571167\n",
      "epoch 4 batch 1005 loss: 2.549267292022705\n",
      "epoch 4 batch 1006 loss: 2.556727647781372\n",
      "epoch 4 batch 1007 loss: 2.3826119899749756\n",
      "epoch 4 batch 1008 loss: 2.2564868927001953\n",
      "epoch 4 batch 1009 loss: 2.044020175933838\n",
      "epoch 4 batch 1010 loss: 2.3821990489959717\n",
      "epoch 4 batch 1011 loss: 2.3563036918640137\n",
      "epoch 4 batch 1012 loss: 2.388906955718994\n",
      "epoch 4 batch 1013 loss: 2.2146873474121094\n",
      "epoch 4 batch 1014 loss: 2.707810878753662\n",
      "epoch 4 batch 1015 loss: 2.263490676879883\n",
      "epoch 4 batch 1016 loss: 2.2672080993652344\n",
      "epoch 4 batch 1017 loss: 2.2086851596832275\n",
      "epoch 4 batch 1018 loss: 2.271815538406372\n",
      "epoch 4 batch 1019 loss: 2.3292195796966553\n",
      "epoch 4 batch 1020 loss: 2.30562162399292\n",
      "epoch 4 batch 1021 loss: 2.3031091690063477\n",
      "epoch 4 batch 1022 loss: 2.4249444007873535\n",
      "epoch 4 batch 1023 loss: 2.1142473220825195\n",
      "epoch 4 batch 1024 loss: 2.0564846992492676\n",
      "epoch 4 batch 1025 loss: 2.1256656646728516\n",
      "epoch 4 batch 1026 loss: 2.330505847930908\n",
      "epoch 4 batch 1027 loss: 2.1856229305267334\n",
      "epoch 4 batch 1028 loss: 2.2883028984069824\n",
      "epoch 4 batch 1029 loss: 2.127425193786621\n",
      "epoch 4 batch 1030 loss: 2.271601915359497\n",
      "epoch 4 batch 1031 loss: 2.552565574645996\n",
      "epoch 4 batch 1032 loss: 2.0604217052459717\n",
      "epoch 4 batch 1033 loss: 2.2822396755218506\n",
      "epoch 4 batch 1034 loss: 2.563391923904419\n",
      "epoch 4 batch 1035 loss: 2.361264705657959\n",
      "epoch 4 batch 1036 loss: 2.548797607421875\n",
      "epoch 4 batch 1037 loss: 2.0178730487823486\n",
      "epoch 4 batch 1038 loss: 2.458989381790161\n",
      "epoch 4 batch 1039 loss: 2.1575229167938232\n",
      "epoch 4 batch 1040 loss: 2.4151785373687744\n",
      "epoch 4 batch 1041 loss: 2.509915828704834\n",
      "epoch 4 batch 1042 loss: 2.0498507022857666\n",
      "epoch 4 batch 1043 loss: 2.2035579681396484\n",
      "epoch 4 batch 1044 loss: 2.389608860015869\n",
      "epoch 4 batch 1045 loss: 2.279961585998535\n",
      "epoch 4 batch 1046 loss: 2.082237958908081\n",
      "epoch 4 batch 1047 loss: 2.1770148277282715\n",
      "epoch 4 batch 1048 loss: 2.269516944885254\n",
      "epoch 4 batch 1049 loss: 2.3724589347839355\n",
      "epoch 4 batch 1050 loss: 2.0618886947631836\n",
      "epoch 4 batch 1051 loss: 2.256113290786743\n",
      "epoch 4 batch 1052 loss: 2.2210440635681152\n",
      "epoch 4 batch 1053 loss: 2.2258992195129395\n",
      "epoch 4 batch 1054 loss: 2.276340961456299\n",
      "epoch 4 batch 1055 loss: 2.1843535900115967\n",
      "epoch 4 batch 1056 loss: 2.259814977645874\n",
      "epoch 4 batch 1057 loss: 2.2475061416625977\n",
      "epoch 4 batch 1058 loss: 2.5840466022491455\n",
      "epoch 4 batch 1059 loss: 2.5329740047454834\n",
      "epoch 4 batch 1060 loss: 2.1747283935546875\n",
      "epoch 4 batch 1061 loss: 2.525637626647949\n",
      "epoch 4 batch 1062 loss: 2.15761661529541\n",
      "epoch 4 batch 1063 loss: 2.3999600410461426\n",
      "epoch 4 batch 1064 loss: 2.283186674118042\n",
      "epoch 4 batch 1065 loss: 2.187319755554199\n",
      "epoch 4 batch 1066 loss: 2.3046560287475586\n",
      "epoch 4 batch 1067 loss: 2.4381508827209473\n",
      "epoch 4 batch 1068 loss: 2.5716629028320312\n",
      "epoch 4 batch 1069 loss: 2.297657012939453\n",
      "epoch 4 batch 1070 loss: 2.223057746887207\n",
      "epoch 4 batch 1071 loss: 2.120479106903076\n",
      "epoch 4 batch 1072 loss: 2.201146125793457\n",
      "epoch 4 batch 1073 loss: 2.3546953201293945\n",
      "epoch 4 batch 1074 loss: 2.1147546768188477\n",
      "epoch 4 batch 1075 loss: 2.1683638095855713\n",
      "epoch 4 batch 1076 loss: 2.3288521766662598\n",
      "epoch 4 batch 1077 loss: 2.2209930419921875\n",
      "epoch 4 batch 1078 loss: 2.469581365585327\n",
      "epoch 4 batch 1079 loss: 2.407682418823242\n",
      "epoch 4 batch 1080 loss: 2.5591251850128174\n",
      "epoch 4 batch 1081 loss: 2.366459608078003\n",
      "epoch 4 batch 1082 loss: 2.510929584503174\n",
      "epoch 4 batch 1083 loss: 2.3861074447631836\n",
      "epoch 4 batch 1084 loss: 2.37375545501709\n",
      "epoch 4 batch 1085 loss: 2.3101487159729004\n",
      "epoch 4 batch 1086 loss: 2.156810760498047\n",
      "epoch 4 batch 1087 loss: 2.3224692344665527\n",
      "epoch 4 batch 1088 loss: 2.549013137817383\n",
      "epoch 4 batch 1089 loss: 2.379924774169922\n",
      "epoch 4 batch 1090 loss: 2.6099493503570557\n",
      "epoch 4 batch 1091 loss: 2.5489864349365234\n",
      "epoch 4 batch 1092 loss: 2.2603893280029297\n",
      "epoch 4 batch 1093 loss: 1.9613795280456543\n",
      "epoch 4 batch 1094 loss: 2.4515881538391113\n",
      "epoch 4 batch 1095 loss: 2.30441951751709\n",
      "epoch 4 batch 1096 loss: 2.2059218883514404\n",
      "epoch 4 batch 1097 loss: 2.520169973373413\n",
      "epoch 4 batch 1098 loss: 2.2479441165924072\n",
      "epoch 4 batch 1099 loss: 2.5109901428222656\n",
      "epoch 4 batch 1100 loss: 2.400855541229248\n",
      "epoch 4 batch 1101 loss: 2.491729974746704\n",
      "epoch 4 batch 1102 loss: 2.0689024925231934\n",
      "epoch 4 batch 1103 loss: 2.4281625747680664\n",
      "epoch 4 batch 1104 loss: 2.2139153480529785\n",
      "epoch 4 batch 1105 loss: 2.163999080657959\n",
      "epoch 4 batch 1106 loss: 2.4098527431488037\n",
      "epoch 4 batch 1107 loss: 2.3787569999694824\n",
      "epoch 4 batch 1108 loss: 2.44636607170105\n",
      "epoch 4 batch 1109 loss: 2.2721238136291504\n",
      "epoch 4 batch 1110 loss: 2.199995994567871\n",
      "epoch 4 batch 1111 loss: 2.411379337310791\n",
      "epoch 4 batch 1112 loss: 2.4479689598083496\n",
      "epoch 4 batch 1113 loss: 2.5451841354370117\n",
      "epoch 4 batch 1114 loss: 2.2728006839752197\n",
      "epoch 4 batch 1115 loss: 2.5615415573120117\n",
      "epoch 4 batch 1116 loss: 2.264963388442993\n",
      "epoch 4 batch 1117 loss: 2.2449283599853516\n",
      "epoch 4 batch 1118 loss: 2.1520252227783203\n",
      "epoch 4 batch 1119 loss: 2.432706594467163\n",
      "epoch 4 batch 1120 loss: 2.2697157859802246\n",
      "epoch 4 batch 1121 loss: 2.451106071472168\n",
      "epoch 4 batch 1122 loss: 2.479247570037842\n",
      "epoch 4 batch 1123 loss: 2.344144105911255\n",
      "epoch 4 batch 1124 loss: 2.468186140060425\n",
      "epoch 4 batch 1125 loss: 2.1672255992889404\n",
      "epoch 4 batch 1126 loss: 2.1542305946350098\n",
      "epoch 4 batch 1127 loss: 2.448624610900879\n",
      "epoch 4 batch 1128 loss: 2.4116663932800293\n",
      "epoch 4 batch 1129 loss: 2.7619471549987793\n",
      "epoch 4 batch 1130 loss: 2.5487313270568848\n",
      "epoch 4 batch 1131 loss: 2.2313718795776367\n",
      "epoch 4 batch 1132 loss: 2.3137645721435547\n",
      "epoch 4 batch 1133 loss: 2.2186498641967773\n",
      "epoch 4 batch 1134 loss: 1.9956583976745605\n",
      "epoch 4 batch 1135 loss: 2.460850715637207\n",
      "epoch 4 batch 1136 loss: 2.354015827178955\n",
      "epoch 4 batch 1137 loss: 2.1102757453918457\n",
      "epoch 4 batch 1138 loss: 2.357213020324707\n",
      "epoch 4 batch 1139 loss: 2.3909852504730225\n",
      "epoch 4 batch 1140 loss: 2.117036819458008\n",
      "epoch 4 batch 1141 loss: 2.5584700107574463\n",
      "epoch 4 batch 1142 loss: 2.1147212982177734\n",
      "epoch 4 batch 1143 loss: 2.360610008239746\n",
      "epoch 4 batch 1144 loss: 2.3394343852996826\n",
      "epoch 4 batch 1145 loss: 2.507392406463623\n",
      "epoch 4 batch 1146 loss: 2.1932969093322754\n",
      "epoch 4 batch 1147 loss: 2.245941162109375\n",
      "epoch 4 batch 1148 loss: 2.438140392303467\n",
      "epoch 4 batch 1149 loss: 2.3194732666015625\n",
      "epoch 4 batch 1150 loss: 2.5228970050811768\n",
      "epoch 4 batch 1151 loss: 2.501437187194824\n",
      "epoch 4 batch 1152 loss: 2.267765760421753\n",
      "epoch 4 batch 1153 loss: 2.585056781768799\n",
      "epoch 4 batch 1154 loss: 2.4900569915771484\n",
      "epoch 4 batch 1155 loss: 2.3692336082458496\n",
      "epoch 4 batch 1156 loss: 2.375849723815918\n",
      "epoch 4 batch 1157 loss: 2.0599021911621094\n",
      "epoch 4 batch 1158 loss: 2.2492432594299316\n",
      "epoch 4 batch 1159 loss: 2.2649998664855957\n",
      "epoch 4 batch 1160 loss: 2.3661670684814453\n",
      "epoch 4 batch 1161 loss: 2.4478580951690674\n",
      "epoch 4 batch 1162 loss: 2.507530689239502\n",
      "epoch 4 batch 1163 loss: 2.4729063510894775\n",
      "epoch 4 batch 1164 loss: 2.24855375289917\n",
      "epoch 4 batch 1165 loss: 2.4006314277648926\n",
      "epoch 4 batch 1166 loss: 2.1841952800750732\n",
      "epoch 4 batch 1167 loss: 2.552172899246216\n",
      "epoch 4 batch 1168 loss: 2.2688522338867188\n",
      "epoch 4 batch 1169 loss: 2.382154703140259\n",
      "epoch 4 batch 1170 loss: 2.320464611053467\n",
      "epoch 4 batch 1171 loss: 2.4236669540405273\n",
      "epoch 4 batch 1172 loss: 2.471214771270752\n",
      "epoch 4 batch 1173 loss: 2.3687386512756348\n",
      "epoch 4 batch 1174 loss: 2.3549880981445312\n",
      "epoch 4 batch 1175 loss: 2.240300178527832\n",
      "epoch 4 batch 1176 loss: 2.3031227588653564\n",
      "epoch 4 batch 1177 loss: 2.4702558517456055\n",
      "epoch 4 batch 1178 loss: 2.336559772491455\n",
      "epoch 4 batch 1179 loss: 2.347233295440674\n",
      "epoch 4 batch 1180 loss: 2.338451385498047\n",
      "epoch 4 batch 1181 loss: 2.578803539276123\n",
      "epoch 4 batch 1182 loss: 2.61215877532959\n",
      "epoch 4 batch 1183 loss: 2.1912927627563477\n",
      "epoch 4 batch 1184 loss: 2.1693577766418457\n",
      "epoch 4 batch 1185 loss: 2.671290397644043\n",
      "epoch 4 batch 1186 loss: 2.396535873413086\n",
      "epoch 4 batch 1187 loss: 2.463557243347168\n",
      "epoch 4 batch 1188 loss: 2.2848992347717285\n",
      "epoch 4 batch 1189 loss: 2.6655478477478027\n",
      "epoch 4 batch 1190 loss: 2.213038921356201\n",
      "epoch 4 batch 1191 loss: 2.2158052921295166\n",
      "epoch 4 batch 1192 loss: 2.224912166595459\n",
      "epoch 4 batch 1193 loss: 2.300865650177002\n",
      "epoch 4 batch 1194 loss: 2.2747251987457275\n",
      "epoch 4 batch 1195 loss: 2.2595531940460205\n",
      "epoch 4 batch 1196 loss: 2.1155619621276855\n",
      "epoch 4 batch 1197 loss: 2.3212485313415527\n",
      "epoch 4 batch 1198 loss: 2.35067081451416\n",
      "epoch 4 batch 1199 loss: 2.336390495300293\n",
      "epoch 4 batch 1200 loss: 2.304213523864746\n",
      "epoch 4 batch 1201 loss: 2.269807815551758\n",
      "epoch 4 batch 1202 loss: 2.480947256088257\n",
      "epoch 4 batch 1203 loss: 2.235241413116455\n",
      "epoch 4 batch 1204 loss: 2.504525661468506\n",
      "epoch 4 batch 1205 loss: 2.2456021308898926\n",
      "epoch 4 batch 1206 loss: 2.602591037750244\n",
      "epoch 4 batch 1207 loss: 2.2864999771118164\n",
      "epoch 4 batch 1208 loss: 2.3766491413116455\n",
      "epoch 4 batch 1209 loss: 2.211564779281616\n",
      "epoch 4 batch 1210 loss: 2.367821216583252\n",
      "epoch 4 batch 1211 loss: 2.5030124187469482\n",
      "epoch 4 batch 1212 loss: 2.1889195442199707\n",
      "epoch 4 batch 1213 loss: 2.3291399478912354\n",
      "epoch 4 batch 1214 loss: 2.592775583267212\n",
      "epoch 4 batch 1215 loss: 2.4569807052612305\n",
      "epoch 4 batch 1216 loss: 2.2656102180480957\n",
      "epoch 4 batch 1217 loss: 2.267338275909424\n",
      "epoch 4 batch 1218 loss: 2.2518818378448486\n",
      "epoch 4 batch 1219 loss: 2.1585965156555176\n",
      "epoch 4 batch 1220 loss: 2.3209571838378906\n",
      "epoch 4 batch 1221 loss: 2.4066765308380127\n",
      "epoch 4 batch 1222 loss: 2.1746349334716797\n",
      "epoch 4 batch 1223 loss: 2.3637337684631348\n",
      "epoch 4 batch 1224 loss: 2.407013416290283\n",
      "epoch 4 batch 1225 loss: 2.1300628185272217\n",
      "epoch 4 batch 1226 loss: 2.252516746520996\n",
      "epoch 4 batch 1227 loss: 2.5431008338928223\n",
      "epoch 4 batch 1228 loss: 2.217207431793213\n",
      "epoch 4 batch 1229 loss: 2.22249174118042\n",
      "epoch 4 batch 1230 loss: 2.2689895629882812\n",
      "epoch 4 batch 1231 loss: 2.660628080368042\n",
      "epoch 4 batch 1232 loss: 2.4044432640075684\n",
      "epoch 4 batch 1233 loss: 2.2772531509399414\n",
      "epoch 4 batch 1234 loss: 2.334139347076416\n",
      "epoch 4 batch 1235 loss: 2.1600382328033447\n",
      "epoch 4 batch 1236 loss: 2.168004035949707\n",
      "epoch 4 batch 1237 loss: 2.5627613067626953\n",
      "epoch 4 batch 1238 loss: 2.1994595527648926\n",
      "epoch 4 batch 1239 loss: 2.4806323051452637\n",
      "epoch 4 batch 1240 loss: 2.2674059867858887\n",
      "epoch 4 batch 1241 loss: 2.4274892807006836\n",
      "epoch 4 batch 1242 loss: 2.414804220199585\n",
      "epoch 4 batch 1243 loss: 2.432438611984253\n",
      "epoch 4 batch 1244 loss: 2.406041383743286\n",
      "epoch 4 batch 1245 loss: 2.4119606018066406\n",
      "epoch 4 batch 1246 loss: 2.257455825805664\n",
      "epoch 4 batch 1247 loss: 2.2655229568481445\n",
      "epoch 4 batch 1248 loss: 2.4668054580688477\n",
      "epoch 4 batch 1249 loss: 2.3534111976623535\n",
      "epoch 4 batch 1250 loss: 2.3590126037597656\n",
      "epoch 4 batch 1251 loss: 2.264730930328369\n",
      "epoch 4 batch 1252 loss: 2.3927626609802246\n",
      "epoch 4 batch 1253 loss: 2.155273199081421\n",
      "epoch 4 batch 1254 loss: 2.3629095554351807\n",
      "epoch 4 batch 1255 loss: 2.4791831970214844\n",
      "epoch 4 batch 1256 loss: 2.4282422065734863\n",
      "epoch 4 batch 1257 loss: 2.391187906265259\n",
      "epoch 4 batch 1258 loss: 2.249793529510498\n",
      "epoch 4 batch 1259 loss: 2.269101619720459\n",
      "epoch 4 batch 1260 loss: 2.26202392578125\n",
      "epoch 4 batch 1261 loss: 2.683703899383545\n",
      "epoch 4 batch 1262 loss: 2.094970464706421\n",
      "epoch 4 batch 1263 loss: 2.254307746887207\n",
      "epoch 4 batch 1264 loss: 2.596217155456543\n",
      "epoch 4 batch 1265 loss: 2.612318754196167\n",
      "epoch 4 batch 1266 loss: 2.2600457668304443\n",
      "epoch 4 batch 1267 loss: 2.7081198692321777\n",
      "epoch 4 batch 1268 loss: 2.411729097366333\n",
      "epoch 4 batch 1269 loss: 2.4509384632110596\n",
      "epoch 4 batch 1270 loss: 2.3744943141937256\n",
      "epoch 4 batch 1271 loss: 2.4674227237701416\n",
      "epoch 4 batch 1272 loss: 2.277090072631836\n",
      "epoch 4 batch 1273 loss: 2.2670187950134277\n",
      "epoch 4 batch 1274 loss: 2.293497323989868\n",
      "epoch 4 batch 1275 loss: 2.195160388946533\n",
      "epoch 4 batch 1276 loss: 2.4511001110076904\n",
      "epoch 4 batch 1277 loss: 2.5491228103637695\n",
      "epoch 4 batch 1278 loss: 2.2245497703552246\n",
      "epoch 4 batch 1279 loss: 2.1986494064331055\n",
      "epoch 4 batch 1280 loss: 2.425985336303711\n",
      "epoch 4 batch 1281 loss: 2.4666385650634766\n",
      "epoch 4 batch 1282 loss: 2.35498046875\n",
      "epoch 4 batch 1283 loss: 2.343759059906006\n",
      "epoch 4 batch 1284 loss: 2.365901470184326\n",
      "epoch 4 batch 1285 loss: 2.3241403102874756\n",
      "epoch 4 batch 1286 loss: 2.1961121559143066\n",
      "epoch 4 batch 1287 loss: 2.1015830039978027\n",
      "epoch 4 batch 1288 loss: 2.4084157943725586\n",
      "epoch 4 batch 1289 loss: 2.2006454467773438\n",
      "epoch 4 batch 1290 loss: 2.352402925491333\n",
      "epoch 4 batch 1291 loss: 2.2742834091186523\n",
      "epoch 4 batch 1292 loss: 2.453420639038086\n",
      "epoch 4 batch 1293 loss: 2.1679253578186035\n",
      "epoch 4 batch 1294 loss: 2.5700011253356934\n",
      "epoch 4 batch 1295 loss: 2.31489634513855\n",
      "epoch 4 batch 1296 loss: 2.4508650302886963\n",
      "epoch 4 batch 1297 loss: 2.225220203399658\n",
      "epoch 4 batch 1298 loss: 2.3308522701263428\n",
      "epoch 4 batch 1299 loss: 2.1881425380706787\n",
      "epoch 4 batch 1300 loss: 2.1910176277160645\n",
      "epoch 4 batch 1301 loss: 2.3593087196350098\n",
      "epoch 4 batch 1302 loss: 2.557497978210449\n",
      "epoch 4 batch 1303 loss: 2.2986786365509033\n",
      "epoch 4 batch 1304 loss: 2.4311904907226562\n",
      "epoch 4 batch 1305 loss: 2.2697858810424805\n",
      "epoch 4 batch 1306 loss: 2.1578192710876465\n",
      "epoch 4 batch 1307 loss: 2.326211929321289\n",
      "epoch 4 batch 1308 loss: 2.2693333625793457\n",
      "epoch 4 batch 1309 loss: 2.442416191101074\n",
      "epoch 4 batch 1310 loss: 2.1396000385284424\n",
      "epoch 4 batch 1311 loss: 2.3696138858795166\n",
      "epoch 4 batch 1312 loss: 2.484147071838379\n",
      "epoch 4 batch 1313 loss: 2.5618672370910645\n",
      "epoch 4 batch 1314 loss: 2.1748504638671875\n",
      "epoch 4 batch 1315 loss: 2.255124568939209\n",
      "epoch 4 batch 1316 loss: 2.216167688369751\n",
      "epoch 4 batch 1317 loss: 2.285792350769043\n",
      "epoch 4 batch 1318 loss: 2.630958080291748\n",
      "epoch 4 batch 1319 loss: 2.0480175018310547\n",
      "epoch 4 batch 1320 loss: 2.4850947856903076\n",
      "epoch 4 batch 1321 loss: 2.173872947692871\n",
      "epoch 4 batch 1322 loss: 2.4347732067108154\n",
      "epoch 4 batch 1323 loss: 2.5024099349975586\n",
      "epoch 4 batch 1324 loss: 2.1633780002593994\n",
      "epoch 4 batch 1325 loss: 2.3431787490844727\n",
      "epoch 4 batch 1326 loss: 2.379857063293457\n",
      "epoch 4 batch 1327 loss: 2.2618284225463867\n",
      "epoch 4 batch 1328 loss: 2.458807945251465\n",
      "epoch 4 batch 1329 loss: 2.3594319820404053\n",
      "epoch 4 batch 1330 loss: 2.2980971336364746\n",
      "epoch 4 batch 1331 loss: 2.329786777496338\n",
      "epoch 4 batch 1332 loss: 2.133709669113159\n",
      "epoch 4 batch 1333 loss: 2.2842183113098145\n",
      "epoch 4 batch 1334 loss: 2.210707902908325\n",
      "epoch 4 batch 1335 loss: 2.188081979751587\n",
      "epoch 4 batch 1336 loss: 2.205411911010742\n",
      "epoch 4 batch 1337 loss: 2.075105667114258\n",
      "epoch 4 batch 1338 loss: 2.4593214988708496\n",
      "epoch 4 batch 1339 loss: 2.304555892944336\n",
      "epoch 4 batch 1340 loss: 2.2965712547302246\n",
      "epoch 4 batch 1341 loss: 2.2548370361328125\n",
      "epoch 4 batch 1342 loss: 2.307119846343994\n",
      "epoch 4 batch 1343 loss: 2.394395351409912\n",
      "epoch 4 batch 1344 loss: 2.222834825515747\n",
      "epoch 4 batch 1345 loss: 2.393263101577759\n",
      "epoch 4 batch 1346 loss: 2.345630168914795\n",
      "epoch 4 batch 1347 loss: 2.2765262126922607\n",
      "epoch 4 batch 1348 loss: 2.2074027061462402\n",
      "epoch 4 batch 1349 loss: 2.388370990753174\n",
      "epoch 4 batch 1350 loss: 2.4794187545776367\n",
      "epoch 4 batch 1351 loss: 2.131924629211426\n",
      "epoch 4 batch 1352 loss: 2.266751766204834\n",
      "epoch 4 batch 1353 loss: 2.212695598602295\n",
      "epoch 4 batch 1354 loss: 2.2884087562561035\n",
      "epoch 4 batch 1355 loss: 2.7557950019836426\n",
      "epoch 4 batch 1356 loss: 2.5009188652038574\n",
      "epoch 4 batch 1357 loss: 2.370091438293457\n",
      "epoch 4 batch 1358 loss: 2.3679304122924805\n",
      "epoch 4 batch 1359 loss: 2.0619125366210938\n",
      "epoch 4 batch 1360 loss: 2.426077365875244\n",
      "epoch 4 batch 1361 loss: 2.092195987701416\n",
      "epoch 4 batch 1362 loss: 2.1448330879211426\n",
      "epoch 4 batch 1363 loss: 2.204833507537842\n",
      "epoch 4 batch 1364 loss: 2.221506357192993\n",
      "epoch 4 batch 1365 loss: 2.4436893463134766\n",
      "epoch 4 batch 1366 loss: 2.179898500442505\n",
      "epoch 4 batch 1367 loss: 2.3010921478271484\n",
      "epoch 4 batch 1368 loss: 2.1667706966400146\n",
      "epoch 4 batch 1369 loss: 2.4861719608306885\n",
      "epoch 4 batch 1370 loss: 2.213819980621338\n",
      "epoch 4 batch 1371 loss: 2.182837724685669\n",
      "epoch 4 batch 1372 loss: 2.493415355682373\n",
      "epoch 4 batch 1373 loss: 2.1419622898101807\n",
      "epoch 4 batch 1374 loss: 2.1506235599517822\n",
      "epoch 4 batch 1375 loss: 2.3024396896362305\n",
      "epoch 4 batch 1376 loss: 2.263803482055664\n",
      "epoch 4 batch 1377 loss: 2.259039878845215\n",
      "epoch 4 batch 1378 loss: 2.296724796295166\n",
      "epoch 4 batch 1379 loss: 2.371575355529785\n",
      "epoch 4 batch 1380 loss: 2.392918109893799\n",
      "epoch 4 batch 1381 loss: 2.5252270698547363\n",
      "epoch 4 batch 1382 loss: 2.4162707328796387\n",
      "epoch 4 batch 1383 loss: 2.2302331924438477\n",
      "epoch 4 batch 1384 loss: 2.646155595779419\n",
      "epoch 4 batch 1385 loss: 2.2709405422210693\n",
      "epoch 4 batch 1386 loss: 2.226012945175171\n",
      "epoch 4 batch 1387 loss: 2.3252997398376465\n",
      "epoch 4 batch 1388 loss: 2.4985804557800293\n",
      "epoch 4 batch 1389 loss: 2.203319549560547\n",
      "epoch 4 batch 1390 loss: 2.3291478157043457\n",
      "epoch 4 batch 1391 loss: 2.375692367553711\n",
      "epoch 4 batch 1392 loss: 2.3412208557128906\n",
      "epoch 4 batch 1393 loss: 2.246046543121338\n",
      "epoch 4 batch 1394 loss: 2.4986255168914795\n",
      "epoch 4 batch 1395 loss: 2.521024703979492\n",
      "epoch 4 batch 1396 loss: 2.71852970123291\n",
      "epoch 4 batch 1397 loss: 2.422791004180908\n",
      "epoch 4 batch 1398 loss: 2.265212059020996\n",
      "epoch 4 batch 1399 loss: 2.3620405197143555\n",
      "epoch 4 batch 1400 loss: 2.573350667953491\n",
      "epoch 4 batch 1401 loss: 2.255378246307373\n",
      "epoch 4 batch 1402 loss: 2.4081668853759766\n",
      "epoch 4 batch 1403 loss: 2.2725610733032227\n",
      "epoch 4 batch 1404 loss: 2.218224048614502\n",
      "epoch 4 batch 1405 loss: 2.421781539916992\n",
      "epoch 4 batch 1406 loss: 2.1818037033081055\n",
      "epoch 4 batch 1407 loss: 2.20919132232666\n",
      "epoch 4 batch 1408 loss: 2.3725414276123047\n",
      "epoch 4 batch 1409 loss: 2.3262083530426025\n",
      "epoch 4 batch 1410 loss: 2.1151962280273438\n",
      "epoch 4 batch 1411 loss: 2.1363492012023926\n",
      "epoch 4 batch 1412 loss: 2.3979716300964355\n",
      "epoch 4 batch 1413 loss: 2.391303300857544\n",
      "epoch 4 batch 1414 loss: 2.462987184524536\n",
      "epoch 4 batch 1415 loss: 2.434209108352661\n",
      "epoch 4 batch 1416 loss: 2.157559394836426\n",
      "epoch 4 batch 1417 loss: 2.330061435699463\n",
      "epoch 4 batch 1418 loss: 2.2170603275299072\n",
      "epoch 4 batch 1419 loss: 2.2440643310546875\n",
      "epoch 4 batch 1420 loss: 2.2381069660186768\n",
      "epoch 4 batch 1421 loss: 2.1902832984924316\n",
      "epoch 4 batch 1422 loss: 2.280543804168701\n",
      "epoch 4 batch 1423 loss: 2.2464599609375\n",
      "epoch 4 batch 1424 loss: 2.1621358394622803\n",
      "epoch 4 batch 1425 loss: 2.340057373046875\n",
      "epoch 4 batch 1426 loss: 2.523434638977051\n",
      "epoch 4 batch 1427 loss: 2.1987104415893555\n",
      "epoch 4 batch 1428 loss: 2.730046272277832\n",
      "epoch 4 batch 1429 loss: 2.1651015281677246\n",
      "epoch 4 batch 1430 loss: 2.4945144653320312\n",
      "epoch 4 batch 1431 loss: 2.288220167160034\n",
      "epoch 4 batch 1432 loss: 2.4639382362365723\n",
      "epoch 4 batch 1433 loss: 2.3487136363983154\n",
      "epoch 4 batch 1434 loss: 2.291224956512451\n",
      "epoch 4 batch 1435 loss: 2.3492376804351807\n",
      "epoch 4 batch 1436 loss: 2.1125965118408203\n",
      "epoch 4 batch 1437 loss: 2.3895578384399414\n",
      "epoch 4 batch 1438 loss: 2.282473087310791\n",
      "epoch 4 batch 1439 loss: 2.2876126766204834\n",
      "epoch 4 batch 1440 loss: 2.561161756515503\n",
      "epoch 4 batch 1441 loss: 2.3172659873962402\n",
      "epoch 4 batch 1442 loss: 2.3923683166503906\n",
      "epoch 4 batch 1443 loss: 2.437350273132324\n",
      "epoch 4 batch 1444 loss: 2.274918794631958\n",
      "epoch 4 batch 1445 loss: 2.2603540420532227\n",
      "epoch 4 batch 1446 loss: 2.4490575790405273\n",
      "epoch 4 batch 1447 loss: 2.4005215167999268\n",
      "epoch 4 batch 1448 loss: 2.266294479370117\n",
      "epoch 4 batch 1449 loss: 2.471369981765747\n",
      "epoch 4 batch 1450 loss: 2.2000694274902344\n",
      "epoch 4 batch 1451 loss: 2.27974271774292\n",
      "epoch 4 batch 1452 loss: 2.199118137359619\n",
      "epoch 4 batch 1453 loss: 2.1960740089416504\n",
      "epoch 4 batch 1454 loss: 2.308927059173584\n",
      "epoch 4 batch 1455 loss: 2.2212319374084473\n",
      "epoch 4 batch 1456 loss: 2.217564344406128\n",
      "epoch 4 batch 1457 loss: 2.581962823867798\n",
      "epoch 4 batch 1458 loss: 2.3881282806396484\n",
      "epoch 4 batch 1459 loss: 2.330483913421631\n",
      "epoch 4 batch 1460 loss: 2.2685904502868652\n",
      "epoch 4 batch 1461 loss: 2.440934896469116\n",
      "epoch 4 batch 1462 loss: 2.105135440826416\n",
      "epoch 4 batch 1463 loss: 2.3165159225463867\n",
      "epoch 4 batch 1464 loss: 2.348729133605957\n",
      "epoch 4 batch 1465 loss: 2.5910050868988037\n",
      "epoch 4 batch 1466 loss: 2.1354870796203613\n",
      "epoch 4 batch 1467 loss: 2.6464507579803467\n",
      "epoch 4 batch 1468 loss: 2.2576043605804443\n",
      "epoch 4 batch 1469 loss: 2.2134556770324707\n",
      "epoch 4 batch 1470 loss: 2.279914379119873\n",
      "epoch 4 batch 1471 loss: 2.291935443878174\n",
      "epoch 4 batch 1472 loss: 2.613373279571533\n",
      "epoch 4 batch 1473 loss: 2.114358901977539\n",
      "epoch 4 batch 1474 loss: 2.290121078491211\n",
      "epoch 4 batch 1475 loss: 2.266111373901367\n",
      "epoch 4 batch 1476 loss: 2.501218795776367\n",
      "epoch 4 batch 1477 loss: 2.3547658920288086\n",
      "epoch 4 batch 1478 loss: 2.238525152206421\n",
      "epoch 4 batch 1479 loss: 2.46066951751709\n",
      "epoch 4 batch 1480 loss: 2.744870662689209\n",
      "epoch 4 batch 1481 loss: 2.0911834239959717\n",
      "epoch 4 batch 1482 loss: 2.178971767425537\n",
      "epoch 4 batch 1483 loss: 2.1198644638061523\n",
      "epoch 4 batch 1484 loss: 2.341498374938965\n",
      "epoch 4 batch 1485 loss: 2.438462734222412\n",
      "epoch 4 batch 1486 loss: 2.3601956367492676\n",
      "epoch 4 batch 1487 loss: 2.263960599899292\n",
      "epoch 4 batch 1488 loss: 2.2793936729431152\n",
      "epoch 4 batch 1489 loss: 2.334165573120117\n",
      "epoch 4 batch 1490 loss: 2.122084856033325\n",
      "epoch 4 batch 1491 loss: 2.2778737545013428\n",
      "epoch 4 batch 1492 loss: 2.6517534255981445\n",
      "epoch 4 batch 1493 loss: 2.4022531509399414\n",
      "epoch 4 batch 1494 loss: 2.1538448333740234\n",
      "epoch 4 batch 1495 loss: 2.100594997406006\n",
      "epoch 4 batch 1496 loss: 2.3656516075134277\n",
      "epoch 4 batch 1497 loss: 2.3947458267211914\n",
      "epoch 4 batch 1498 loss: 2.0723159313201904\n",
      "epoch 4 batch 1499 loss: 2.506145715713501\n",
      "epoch 4 batch 1500 loss: 2.2877187728881836\n",
      "epoch 4 batch 1501 loss: 2.2484045028686523\n",
      "epoch 4 batch 1502 loss: 2.1696977615356445\n",
      "epoch 4 batch 1503 loss: 2.424736738204956\n",
      "epoch 4 batch 1504 loss: 2.3285117149353027\n",
      "epoch 4 batch 1505 loss: 2.2014665603637695\n",
      "epoch 4 batch 1506 loss: 2.3749568462371826\n",
      "epoch 4 batch 1507 loss: 2.373940944671631\n",
      "epoch 4 batch 1508 loss: 2.147937297821045\n",
      "epoch 4 batch 1509 loss: 2.3894639015197754\n",
      "epoch 4 batch 1510 loss: 2.1637511253356934\n",
      "epoch 4 batch 1511 loss: 2.449456214904785\n",
      "epoch 4 batch 1512 loss: 2.210638999938965\n",
      "epoch 4 batch 1513 loss: 2.2988576889038086\n",
      "epoch 4 batch 1514 loss: 2.4605443477630615\n",
      "epoch 4 batch 1515 loss: 2.258657217025757\n",
      "epoch 4 batch 1516 loss: 2.2067461013793945\n",
      "epoch 4 batch 1517 loss: 2.526801586151123\n",
      "epoch 4 batch 1518 loss: 2.5595004558563232\n",
      "epoch 4 batch 1519 loss: 2.4258921146392822\n",
      "epoch 4 batch 1520 loss: 2.4161996841430664\n",
      "epoch 4 batch 1521 loss: 2.5027077198028564\n",
      "epoch 4 batch 1522 loss: 2.482090473175049\n",
      "epoch 4 batch 1523 loss: 2.2201919555664062\n",
      "epoch 4 batch 1524 loss: 2.1915440559387207\n",
      "epoch 4 batch 1525 loss: 2.1680259704589844\n",
      "epoch 4 batch 1526 loss: 2.5730342864990234\n",
      "epoch 4 batch 1527 loss: 2.15194034576416\n",
      "epoch 4 batch 1528 loss: 2.1872057914733887\n",
      "epoch 4 batch 1529 loss: 2.295149803161621\n",
      "epoch 4 batch 1530 loss: 2.360954761505127\n",
      "epoch 4 batch 1531 loss: 2.5441722869873047\n",
      "epoch 4 batch 1532 loss: 2.541632890701294\n",
      "epoch 4 batch 1533 loss: 2.1975862979888916\n",
      "epoch 4 batch 1534 loss: 2.169203042984009\n",
      "epoch 4 batch 1535 loss: 2.3656229972839355\n",
      "epoch 4 batch 1536 loss: 2.339841842651367\n",
      "epoch 4 batch 1537 loss: 2.277067184448242\n",
      "epoch 4 batch 1538 loss: 2.1667041778564453\n",
      "epoch 4 batch 1539 loss: 2.110898733139038\n",
      "epoch 4 batch 1540 loss: 2.248587131500244\n",
      "epoch 4 batch 1541 loss: 2.1406126022338867\n",
      "epoch 4 batch 1542 loss: 2.1917972564697266\n",
      "epoch 4 batch 1543 loss: 2.3529763221740723\n",
      "epoch 4 batch 1544 loss: 2.3561012744903564\n",
      "epoch 4 batch 1545 loss: 2.1891791820526123\n",
      "epoch 4 batch 1546 loss: 2.3309717178344727\n",
      "epoch 4 batch 1547 loss: 2.270982265472412\n",
      "epoch 4 batch 1548 loss: 2.51127290725708\n",
      "epoch 4 batch 1549 loss: 2.465313196182251\n",
      "epoch 4 batch 1550 loss: 2.1160082817077637\n",
      "epoch 4 batch 1551 loss: 2.5153231620788574\n",
      "epoch 4 batch 1552 loss: 2.147063732147217\n",
      "epoch 4 batch 1553 loss: 2.2402639389038086\n",
      "epoch 4 batch 1554 loss: 2.3869495391845703\n",
      "epoch 4 batch 1555 loss: 2.363218307495117\n",
      "epoch 4 batch 1556 loss: 2.308216094970703\n",
      "epoch 4 batch 1557 loss: 2.292813301086426\n",
      "epoch 4 batch 1558 loss: 2.116884231567383\n",
      "epoch 4 batch 1559 loss: 2.1445839405059814\n",
      "epoch 4 batch 1560 loss: 2.3152217864990234\n",
      "epoch 4 batch 1561 loss: 2.7145164012908936\n",
      "epoch 4 batch 1562 loss: 2.58583402633667\n",
      "epoch 4 batch 1563 loss: 2.159942626953125\n",
      "epoch 4 batch 1564 loss: 2.0966053009033203\n",
      "epoch 4 batch 1565 loss: 2.2199137210845947\n",
      "epoch 4 batch 1566 loss: 2.064774990081787\n",
      "epoch 4 batch 1567 loss: 2.345888614654541\n",
      "epoch 4 batch 1568 loss: 2.625730514526367\n",
      "epoch 4 batch 1569 loss: 2.360828399658203\n",
      "epoch 4 batch 1570 loss: 2.7293338775634766\n",
      "epoch 4 batch 1571 loss: 2.551815986633301\n",
      "epoch 4 batch 1572 loss: 2.2518527507781982\n",
      "epoch 4 batch 1573 loss: 2.2877743244171143\n",
      "epoch 4 batch 1574 loss: 2.0972189903259277\n",
      "epoch 4 batch 1575 loss: 2.392090082168579\n",
      "epoch 4 batch 1576 loss: 2.277151346206665\n",
      "epoch 4 batch 1577 loss: 2.524430513381958\n",
      "epoch 4 batch 1578 loss: 2.3500490188598633\n",
      "epoch 4 batch 1579 loss: 2.5589375495910645\n",
      "epoch 4 batch 1580 loss: 2.4197287559509277\n",
      "epoch 4 batch 1581 loss: 2.2846031188964844\n",
      "epoch 4 batch 1582 loss: 2.692391872406006\n",
      "epoch 4 batch 1583 loss: 2.4631450176239014\n",
      "epoch 4 batch 1584 loss: 2.6694741249084473\n",
      "epoch 4 batch 1585 loss: 2.3634519577026367\n",
      "epoch 4 batch 1586 loss: 2.174382209777832\n",
      "epoch 4 batch 1587 loss: 2.421842098236084\n",
      "epoch 4 batch 1588 loss: 2.070051908493042\n",
      "epoch 4 batch 1589 loss: 2.1507720947265625\n",
      "epoch 4 batch 1590 loss: 2.44534969329834\n",
      "epoch 4 batch 1591 loss: 2.409959077835083\n",
      "epoch 4 batch 1592 loss: 2.6596903800964355\n",
      "epoch 4 batch 1593 loss: 2.2641329765319824\n",
      "epoch 4 batch 1594 loss: 2.528717041015625\n",
      "epoch 4 batch 1595 loss: 2.4489006996154785\n",
      "epoch 4 batch 1596 loss: 2.340625286102295\n",
      "epoch 4 batch 1597 loss: 2.135902166366577\n",
      "epoch 4 batch 1598 loss: 2.6429810523986816\n",
      "epoch 4 batch 1599 loss: 2.2994370460510254\n",
      "epoch 4 batch 1600 loss: 2.3939437866210938\n",
      "epoch 4 batch 1601 loss: 2.2883195877075195\n",
      "epoch 4 batch 1602 loss: 2.193392276763916\n",
      "epoch 4 batch 1603 loss: 2.282973527908325\n",
      "epoch 4 batch 1604 loss: 2.01857328414917\n",
      "epoch 4 batch 1605 loss: 2.0918235778808594\n",
      "epoch 4 batch 1606 loss: 2.1398675441741943\n",
      "epoch 4 batch 1607 loss: 2.3554539680480957\n",
      "epoch 4 batch 1608 loss: 2.4364776611328125\n",
      "epoch 4 batch 1609 loss: 2.2955050468444824\n",
      "epoch 4 batch 1610 loss: 2.391298294067383\n",
      "epoch 4 batch 1611 loss: 2.340907096862793\n",
      "epoch 4 batch 1612 loss: 2.182204246520996\n",
      "epoch 4 batch 1613 loss: 2.3121654987335205\n",
      "epoch 4 batch 1614 loss: 2.5044190883636475\n",
      "epoch 4 batch 1615 loss: 2.468052387237549\n",
      "epoch 4 batch 1616 loss: 2.3613271713256836\n",
      "epoch 4 batch 1617 loss: 2.396040678024292\n",
      "epoch 4 batch 1618 loss: 2.6073379516601562\n",
      "epoch 4 batch 1619 loss: 2.2616050243377686\n",
      "epoch 4 batch 1620 loss: 2.3349146842956543\n",
      "epoch 4 batch 1621 loss: 2.2899179458618164\n",
      "epoch 4 batch 1622 loss: 2.3350718021392822\n",
      "epoch 4 batch 1623 loss: 2.3670637607574463\n",
      "epoch 4 batch 1624 loss: 2.6275863647460938\n",
      "epoch 4 batch 1625 loss: 2.2814853191375732\n",
      "epoch 4 batch 1626 loss: 2.631649971008301\n",
      "epoch 4 batch 1627 loss: 2.506136178970337\n",
      "epoch 4 batch 1628 loss: 2.3520755767822266\n",
      "epoch 4 batch 1629 loss: 2.656205177307129\n",
      "epoch 4 batch 1630 loss: 2.324540615081787\n",
      "epoch 4 batch 1631 loss: 2.2033069133758545\n",
      "epoch 4 batch 1632 loss: 2.5709035396575928\n",
      "epoch 4 batch 1633 loss: 2.214881420135498\n",
      "epoch 4 batch 1634 loss: 2.307623863220215\n",
      "epoch 4 batch 1635 loss: 2.555418014526367\n",
      "epoch 4 batch 1636 loss: 2.2105233669281006\n",
      "epoch 4 batch 1637 loss: 2.2924935817718506\n",
      "epoch 4 batch 1638 loss: 2.5429253578186035\n",
      "epoch 4 batch 1639 loss: 2.3169050216674805\n",
      "epoch 4 batch 1640 loss: 2.4402034282684326\n",
      "epoch 4 batch 1641 loss: 2.3262674808502197\n",
      "epoch 4 batch 1642 loss: 2.1905226707458496\n",
      "epoch 4 batch 1643 loss: 2.288553476333618\n",
      "epoch 4 batch 1644 loss: 2.368467330932617\n",
      "epoch 4 batch 1645 loss: 2.2221853733062744\n",
      "epoch 4 batch 1646 loss: 2.3210935592651367\n",
      "epoch 4 batch 1647 loss: 2.3175208568573\n",
      "epoch 4 batch 1648 loss: 2.066770076751709\n",
      "epoch 4 batch 1649 loss: 2.215639591217041\n",
      "epoch 4 batch 1650 loss: 2.270766019821167\n",
      "epoch 4 batch 1651 loss: 2.21514630317688\n",
      "epoch 4 batch 1652 loss: 2.254757881164551\n",
      "epoch 4 batch 1653 loss: 2.178955316543579\n",
      "epoch 4 batch 1654 loss: 2.4281816482543945\n",
      "epoch 4 batch 1655 loss: 2.4365928173065186\n",
      "epoch 4 batch 1656 loss: 2.201824426651001\n",
      "epoch 4 batch 1657 loss: 2.671818256378174\n",
      "epoch 4 batch 1658 loss: 2.3577609062194824\n",
      "epoch 4 batch 1659 loss: 2.260756731033325\n",
      "epoch 4 batch 1660 loss: 2.06221342086792\n",
      "epoch 4 batch 1661 loss: 2.4902307987213135\n",
      "epoch 4 batch 1662 loss: 2.462561845779419\n",
      "epoch 4 batch 1663 loss: 2.462151050567627\n",
      "epoch 4 batch 1664 loss: 2.5083882808685303\n",
      "epoch 4 batch 1665 loss: 2.366936445236206\n",
      "epoch 4 batch 1666 loss: 2.4147660732269287\n",
      "epoch 4 batch 1667 loss: 2.0224294662475586\n",
      "epoch 4 batch 1668 loss: 2.3093745708465576\n",
      "epoch 4 batch 1669 loss: 2.6101861000061035\n",
      "epoch 4 batch 1670 loss: 2.483147621154785\n",
      "epoch 4 batch 1671 loss: 2.165491819381714\n",
      "epoch 4 batch 1672 loss: 2.3549106121063232\n",
      "epoch 4 batch 1673 loss: 2.062978506088257\n",
      "epoch 4 batch 1674 loss: 2.2176437377929688\n",
      "epoch 4 batch 1675 loss: 2.3394737243652344\n",
      "epoch 4 batch 1676 loss: 2.467461109161377\n",
      "epoch 4 batch 1677 loss: 2.202024459838867\n",
      "epoch 4 batch 1678 loss: 2.386406421661377\n",
      "epoch 4 batch 1679 loss: 2.3156325817108154\n",
      "epoch 4 batch 1680 loss: 2.6857194900512695\n",
      "epoch 4 batch 1681 loss: 2.4832911491394043\n",
      "epoch 4 batch 1682 loss: 2.0513176918029785\n",
      "epoch 4 batch 1683 loss: 2.3311619758605957\n",
      "epoch 4 batch 1684 loss: 2.2170822620391846\n",
      "epoch 4 batch 1685 loss: 2.4588587284088135\n",
      "epoch 4 batch 1686 loss: 2.4037423133850098\n",
      "epoch 4 batch 1687 loss: 2.5701611042022705\n",
      "epoch 4 batch 1688 loss: 2.3145649433135986\n",
      "epoch 4 batch 1689 loss: 2.441068172454834\n",
      "epoch 4 batch 1690 loss: 2.480268955230713\n",
      "epoch 4 batch 1691 loss: 2.0263214111328125\n",
      "epoch 4 batch 1692 loss: 2.373647928237915\n",
      "epoch 4 batch 1693 loss: 2.5114336013793945\n",
      "epoch 4 batch 1694 loss: 2.7302908897399902\n",
      "epoch 4 batch 1695 loss: 2.4402575492858887\n",
      "epoch 4 batch 1696 loss: 2.2132325172424316\n",
      "epoch 4 batch 1697 loss: 2.243241786956787\n",
      "epoch 4 batch 1698 loss: 2.3937485218048096\n",
      "epoch 4 batch 1699 loss: 2.208165168762207\n",
      "epoch 4 batch 1700 loss: 2.3745908737182617\n",
      "epoch 4 batch 1701 loss: 2.199578285217285\n",
      "epoch 4 batch 1702 loss: 2.219623565673828\n",
      "epoch 4 batch 1703 loss: 2.309630870819092\n",
      "epoch 4 batch 1704 loss: 2.4270682334899902\n",
      "epoch 4 batch 1705 loss: 2.1534037590026855\n",
      "epoch 4 batch 1706 loss: 2.255246162414551\n",
      "epoch 4 batch 1707 loss: 2.286590337753296\n",
      "epoch 4 batch 1708 loss: 2.445749282836914\n",
      "epoch 4 batch 1709 loss: 2.257737398147583\n",
      "epoch 4 batch 1710 loss: 2.201464891433716\n",
      "epoch 4 batch 1711 loss: 2.071489095687866\n",
      "epoch 4 batch 1712 loss: 2.3600645065307617\n",
      "epoch 4 batch 1713 loss: 2.345292091369629\n",
      "epoch 4 batch 1714 loss: 2.1909804344177246\n",
      "epoch 4 batch 1715 loss: 2.2790074348449707\n",
      "epoch 4 batch 1716 loss: 2.358694553375244\n",
      "epoch 4 batch 1717 loss: 2.5723042488098145\n",
      "epoch 4 batch 1718 loss: 2.2331254482269287\n",
      "epoch 4 batch 1719 loss: 2.053063154220581\n",
      "epoch 4 batch 1720 loss: 2.2954816818237305\n",
      "epoch 4 batch 1721 loss: 2.321192979812622\n",
      "epoch 4 batch 1722 loss: 2.220442771911621\n",
      "epoch 4 batch 1723 loss: 2.661101818084717\n",
      "epoch 4 batch 1724 loss: 2.1940853595733643\n",
      "epoch 4 batch 1725 loss: 2.316110610961914\n",
      "epoch 4 batch 1726 loss: 2.3036999702453613\n",
      "epoch 4 batch 1727 loss: 2.3012161254882812\n",
      "epoch 4 batch 1728 loss: 2.5742931365966797\n",
      "epoch 4 batch 1729 loss: 2.318150043487549\n",
      "epoch 4 batch 1730 loss: 2.503774404525757\n",
      "epoch 4 batch 1731 loss: 2.2480087280273438\n",
      "epoch 4 batch 1732 loss: 2.1578259468078613\n",
      "epoch 4 batch 1733 loss: 2.4525749683380127\n",
      "epoch 4 batch 1734 loss: 2.2755558490753174\n",
      "epoch 4 batch 1735 loss: 2.258253335952759\n",
      "epoch 4 batch 1736 loss: 2.2368128299713135\n",
      "epoch 4 batch 1737 loss: 2.2013072967529297\n",
      "epoch 4 batch 1738 loss: 2.2626824378967285\n",
      "epoch 4 batch 1739 loss: 2.2524547576904297\n",
      "epoch 4 batch 1740 loss: 2.152358293533325\n",
      "epoch 4 batch 1741 loss: 2.4842052459716797\n",
      "epoch 4 batch 1742 loss: 2.296663999557495\n",
      "epoch 4 batch 1743 loss: 2.412360906600952\n",
      "epoch 4 batch 1744 loss: 2.3950629234313965\n",
      "epoch 4 batch 1745 loss: 2.2887492179870605\n",
      "epoch 4 batch 1746 loss: 2.366889476776123\n",
      "epoch 4 batch 1747 loss: 2.3447201251983643\n",
      "epoch 4 batch 1748 loss: 2.1545474529266357\n",
      "epoch 4 batch 1749 loss: 2.3404088020324707\n",
      "epoch 4 batch 1750 loss: 2.433885097503662\n",
      "epoch 4 batch 1751 loss: 2.438450813293457\n",
      "epoch 4 batch 1752 loss: 2.233166217803955\n",
      "epoch 4 batch 1753 loss: 2.5572595596313477\n",
      "epoch 4 batch 1754 loss: 2.490751266479492\n",
      "epoch 4 batch 1755 loss: 2.2929611206054688\n",
      "epoch 4 batch 1756 loss: 2.3293938636779785\n",
      "epoch 4 batch 1757 loss: 2.7204484939575195\n",
      "epoch 4 batch 1758 loss: 2.459333896636963\n",
      "epoch 4 batch 1759 loss: 2.5481066703796387\n",
      "epoch 4 batch 1760 loss: 2.216355085372925\n",
      "epoch 4 batch 1761 loss: 2.491276264190674\n",
      "epoch 4 batch 1762 loss: 2.34733510017395\n",
      "epoch 4 batch 1763 loss: 2.233708381652832\n",
      "epoch 4 batch 1764 loss: 2.2603721618652344\n",
      "epoch 4 batch 1765 loss: 2.4398539066314697\n",
      "epoch 4 batch 1766 loss: 2.3404488563537598\n",
      "epoch 4 batch 1767 loss: 2.3929176330566406\n",
      "epoch 4 batch 1768 loss: 2.6574363708496094\n",
      "epoch 4 batch 1769 loss: 2.461468458175659\n",
      "epoch 4 batch 1770 loss: 2.2205495834350586\n",
      "epoch 4 batch 1771 loss: 2.3611176013946533\n",
      "epoch 4 batch 1772 loss: 2.7349116802215576\n",
      "epoch 4 batch 1773 loss: 2.3017947673797607\n",
      "epoch 4 batch 1774 loss: 2.5423741340637207\n",
      "epoch 4 batch 1775 loss: 2.2105274200439453\n",
      "epoch 4 batch 1776 loss: 2.585350751876831\n",
      "epoch 4 batch 1777 loss: 2.3714914321899414\n",
      "epoch 4 batch 1778 loss: 2.0974364280700684\n",
      "epoch 4 batch 1779 loss: 2.3091366291046143\n",
      "epoch 4 batch 1780 loss: 2.315289258956909\n",
      "epoch 4 batch 1781 loss: 2.287277936935425\n",
      "epoch 4 batch 1782 loss: 2.273592710494995\n",
      "epoch 4 batch 1783 loss: 2.4137775897979736\n",
      "epoch 4 batch 1784 loss: 2.4196674823760986\n",
      "epoch 4 batch 1785 loss: 2.3342976570129395\n",
      "epoch 4 batch 1786 loss: 2.2239608764648438\n",
      "epoch 4 batch 1787 loss: 2.454799175262451\n",
      "epoch 4 batch 1788 loss: 2.307248592376709\n",
      "epoch 4 batch 1789 loss: 2.2816359996795654\n",
      "epoch 4 batch 1790 loss: 2.1074116230010986\n",
      "epoch 4 batch 1791 loss: 2.1834731101989746\n",
      "epoch 4 batch 1792 loss: 2.3858141899108887\n",
      "epoch 4 batch 1793 loss: 2.4045114517211914\n",
      "epoch 4 batch 1794 loss: 2.7226243019104004\n",
      "epoch 4 batch 1795 loss: 2.5214953422546387\n",
      "epoch 4 batch 1796 loss: 2.3172802925109863\n",
      "epoch 4 batch 1797 loss: 2.338618755340576\n",
      "epoch 4 batch 1798 loss: 2.2983884811401367\n",
      "epoch 4 batch 1799 loss: 2.4466712474823\n",
      "epoch 4 batch 1800 loss: 2.8481481075286865\n",
      "epoch 4 batch 1801 loss: 2.193979501724243\n",
      "epoch 4 batch 1802 loss: 2.4457039833068848\n",
      "epoch 4 batch 1803 loss: 2.2483034133911133\n",
      "epoch 4 batch 1804 loss: 2.2385082244873047\n",
      "epoch 4 batch 1805 loss: 2.148850679397583\n",
      "epoch 4 batch 1806 loss: 2.2880029678344727\n",
      "epoch 4 batch 1807 loss: 2.31467604637146\n",
      "epoch 4 batch 1808 loss: 2.5611181259155273\n",
      "epoch 4 batch 1809 loss: 2.3652029037475586\n",
      "epoch 4 batch 1810 loss: 2.7766551971435547\n",
      "epoch 4 batch 1811 loss: 2.323505163192749\n",
      "epoch 4 batch 1812 loss: 2.08368182182312\n",
      "epoch 4 batch 1813 loss: 2.3185651302337646\n",
      "epoch 4 batch 1814 loss: 2.3611106872558594\n",
      "epoch 4 batch 1815 loss: 2.6113452911376953\n",
      "epoch 4 batch 1816 loss: 2.6861929893493652\n",
      "epoch 4 batch 1817 loss: 2.457507610321045\n",
      "epoch 4 batch 1818 loss: 2.3457696437835693\n",
      "epoch 4 batch 1819 loss: 2.41328763961792\n",
      "epoch 4 batch 1820 loss: 2.2622218132019043\n",
      "epoch 4 batch 1821 loss: 2.501622438430786\n",
      "epoch 4 batch 1822 loss: 2.20450496673584\n",
      "epoch 4 batch 1823 loss: 2.249389171600342\n",
      "epoch 4 batch 1824 loss: 2.317113161087036\n",
      "epoch 4 batch 1825 loss: 2.1722302436828613\n",
      "epoch 4 batch 1826 loss: 2.264744520187378\n",
      "epoch 4 batch 1827 loss: 2.5720291137695312\n",
      "epoch 4 batch 1828 loss: 2.2692718505859375\n",
      "epoch 4 batch 1829 loss: 2.2955968379974365\n",
      "epoch 4 batch 1830 loss: 2.3746726512908936\n",
      "epoch 4 batch 1831 loss: 2.081554651260376\n",
      "epoch 4 batch 1832 loss: 2.1798105239868164\n",
      "epoch 4 batch 1833 loss: 2.234959602355957\n",
      "epoch 4 batch 1834 loss: 2.3145551681518555\n",
      "epoch 4 batch 1835 loss: 2.2696595191955566\n",
      "epoch 4 batch 1836 loss: 2.226050853729248\n",
      "epoch 4 batch 1837 loss: 2.4113197326660156\n",
      "epoch 4 batch 1838 loss: 2.435788154602051\n",
      "epoch 4 batch 1839 loss: 2.365659475326538\n",
      "epoch 4 batch 1840 loss: 2.364267349243164\n",
      "epoch 4 batch 1841 loss: 2.1462180614471436\n",
      "epoch 4 batch 1842 loss: 2.4025135040283203\n",
      "epoch 4 batch 1843 loss: 2.2851428985595703\n",
      "epoch 4 batch 1844 loss: 2.3340563774108887\n",
      "epoch 4 batch 1845 loss: 2.334724187850952\n",
      "epoch 4 batch 1846 loss: 2.4834227561950684\n",
      "epoch 4 batch 1847 loss: 2.3004555702209473\n",
      "epoch 4 batch 1848 loss: 2.348015546798706\n",
      "epoch 4 batch 1849 loss: 2.3918020725250244\n",
      "epoch 4 batch 1850 loss: 2.6749696731567383\n",
      "epoch 4 batch 1851 loss: 2.5819196701049805\n",
      "epoch 4 batch 1852 loss: 2.4317455291748047\n",
      "epoch 4 batch 1853 loss: 2.4955391883850098\n",
      "epoch 4 batch 1854 loss: 2.1374363899230957\n",
      "epoch 4 batch 1855 loss: 2.2591066360473633\n",
      "epoch 4 batch 1856 loss: 2.1957263946533203\n",
      "epoch 4 batch 1857 loss: 2.605295181274414\n",
      "epoch 4 batch 1858 loss: 1.9842212200164795\n",
      "epoch 4 batch 1859 loss: 2.49774432182312\n",
      "epoch 4 batch 1860 loss: 2.4963269233703613\n",
      "epoch 4 batch 1861 loss: 2.3504085540771484\n",
      "epoch 4 batch 1862 loss: 2.257024049758911\n",
      "epoch 4 batch 1863 loss: 2.1394357681274414\n",
      "epoch 4 batch 1864 loss: 2.2687692642211914\n",
      "epoch 4 batch 1865 loss: 2.494853973388672\n",
      "epoch 4 batch 1866 loss: 2.616147041320801\n",
      "epoch 4 batch 1867 loss: 2.5266551971435547\n",
      "epoch 4 batch 1868 loss: 2.304673194885254\n",
      "epoch 4 batch 1869 loss: 2.1916680335998535\n",
      "epoch 4 batch 1870 loss: 2.291926860809326\n",
      "epoch 4 batch 1871 loss: 2.3772401809692383\n",
      "epoch 4 batch 1872 loss: 2.7068276405334473\n",
      "epoch 4 batch 1873 loss: 2.155160903930664\n",
      "epoch 4 batch 1874 loss: 2.420815944671631\n",
      "epoch 4 batch 1875 loss: 3.002166748046875\n",
      "epoch 4 batch 1876 loss: 2.3598504066467285\n",
      "epoch 4 batch 1877 loss: 2.385883331298828\n",
      "epoch 4 batch 1878 loss: 2.2593393325805664\n",
      "epoch 4 batch 1879 loss: 2.3361496925354004\n",
      "epoch 4 batch 1880 loss: 2.476935625076294\n",
      "epoch 4 batch 1881 loss: 2.3573038578033447\n",
      "epoch 4 batch 1882 loss: 2.2083334922790527\n",
      "epoch 4 batch 1883 loss: 2.3073978424072266\n",
      "epoch 4 batch 1884 loss: 2.2772202491760254\n",
      "epoch 4 batch 1885 loss: 2.2857565879821777\n",
      "epoch 4 batch 1886 loss: 2.334184169769287\n",
      "epoch 4 batch 1887 loss: 2.278473138809204\n",
      "epoch 4 batch 1888 loss: 2.3516969680786133\n",
      "epoch 4 batch 1889 loss: 2.3260657787323\n",
      "epoch 4 batch 1890 loss: 2.6566033363342285\n",
      "epoch 4 batch 1891 loss: 2.5365333557128906\n",
      "epoch 4 batch 1892 loss: 2.194896697998047\n",
      "epoch 4 batch 1893 loss: 2.4957685470581055\n",
      "epoch 4 batch 1894 loss: 2.2286009788513184\n",
      "epoch 4 batch 1895 loss: 2.181211471557617\n",
      "epoch 4 batch 1896 loss: 2.2862701416015625\n",
      "epoch 4 batch 1897 loss: 2.2597601413726807\n",
      "epoch 4 batch 1898 loss: 2.2223358154296875\n",
      "epoch 4 batch 1899 loss: 2.2791662216186523\n",
      "epoch 4 batch 1900 loss: 2.522878646850586\n",
      "epoch 4 batch 1901 loss: 2.6062731742858887\n",
      "epoch 4 batch 1902 loss: 2.0711145401000977\n",
      "epoch 4 batch 1903 loss: 2.2877395153045654\n",
      "epoch 4 batch 1904 loss: 2.4046616554260254\n",
      "epoch 4 batch 1905 loss: 2.2542381286621094\n",
      "epoch 4 batch 1906 loss: 2.5194385051727295\n",
      "epoch 4 batch 1907 loss: 2.1696581840515137\n",
      "epoch 4 batch 1908 loss: 2.144057273864746\n",
      "epoch 4 batch 1909 loss: 2.1524033546447754\n",
      "epoch 4 batch 1910 loss: 2.471189022064209\n",
      "epoch 4 batch 1911 loss: 2.1255993843078613\n",
      "epoch 4 batch 1912 loss: 2.2022714614868164\n",
      "epoch 4 batch 1913 loss: 2.269097089767456\n",
      "epoch 4 batch 1914 loss: 2.169795036315918\n",
      "epoch 4 batch 1915 loss: 2.3389759063720703\n",
      "epoch 4 batch 1916 loss: 2.4549238681793213\n",
      "epoch 4 batch 1917 loss: 2.5596587657928467\n",
      "epoch 4 batch 1918 loss: 2.478749990463257\n",
      "epoch 4 batch 1919 loss: 2.2207319736480713\n",
      "epoch 4 batch 1920 loss: 2.2013890743255615\n",
      "epoch 4 batch 1921 loss: 2.3320484161376953\n",
      "epoch 4 batch 1922 loss: 2.3400659561157227\n",
      "epoch 4 batch 1923 loss: 2.234154224395752\n",
      "epoch 4 batch 1924 loss: 2.612644672393799\n",
      "epoch 4 batch 1925 loss: 2.5287833213806152\n",
      "epoch 4 batch 1926 loss: 2.138015031814575\n",
      "epoch 4 batch 1927 loss: 2.398970127105713\n",
      "epoch 4 batch 1928 loss: 2.2870419025421143\n",
      "epoch 4 batch 1929 loss: 2.485171318054199\n",
      "epoch 4 batch 1930 loss: 2.6464133262634277\n",
      "epoch 4 batch 1931 loss: 2.2250924110412598\n",
      "epoch 4 batch 1932 loss: 2.1940956115722656\n",
      "epoch 4 batch 1933 loss: 2.2955946922302246\n",
      "epoch 4 batch 1934 loss: 2.5036420822143555\n",
      "epoch 4 batch 1935 loss: 2.6661171913146973\n",
      "epoch 4 batch 1936 loss: 2.303234100341797\n",
      "epoch 4 batch 1937 loss: 2.4549779891967773\n",
      "epoch 4 batch 1938 loss: 2.342165231704712\n",
      "epoch 4 batch 1939 loss: 2.350386619567871\n",
      "epoch 4 batch 1940 loss: 2.2676548957824707\n",
      "epoch 4 batch 1941 loss: 2.4536333084106445\n",
      "epoch 4 batch 1942 loss: 2.5423712730407715\n",
      "epoch 4 batch 1943 loss: 2.223799705505371\n",
      "epoch 4 batch 1944 loss: 2.211127519607544\n",
      "epoch 4 batch 1945 loss: 2.202904224395752\n",
      "epoch 4 batch 1946 loss: 2.3160011768341064\n",
      "epoch 4 batch 1947 loss: 2.3436174392700195\n",
      "epoch 4 batch 1948 loss: 2.450665235519409\n",
      "epoch 4 batch 1949 loss: 2.3537240028381348\n",
      "epoch 4 batch 1950 loss: 2.3552918434143066\n",
      "epoch 4 batch 1951 loss: 2.3043811321258545\n",
      "epoch 4 batch 1952 loss: 2.3204703330993652\n",
      "epoch 4 batch 1953 loss: 2.3146305084228516\n",
      "epoch 4 batch 1954 loss: 2.24959135055542\n",
      "epoch 4 batch 1955 loss: 2.505898952484131\n",
      "epoch 4 batch 1956 loss: 2.4033114910125732\n",
      "epoch 4 batch 1957 loss: 2.1757888793945312\n",
      "epoch 4 batch 1958 loss: 2.316316604614258\n",
      "epoch 4 batch 1959 loss: 2.369030475616455\n",
      "epoch 4 batch 1960 loss: 2.4540939331054688\n",
      "epoch 4 batch 1961 loss: 2.568305730819702\n",
      "epoch 4 batch 1962 loss: 2.3503832817077637\n",
      "epoch 4 batch 1963 loss: 2.3503122329711914\n",
      "epoch 4 batch 1964 loss: 2.462860107421875\n",
      "epoch 4 batch 1965 loss: 2.228264570236206\n",
      "epoch 4 batch 1966 loss: 2.060884952545166\n",
      "epoch 4 batch 1967 loss: 2.217050552368164\n",
      "epoch 4 batch 1968 loss: 2.245513677597046\n",
      "epoch 4 batch 1969 loss: 2.325191020965576\n",
      "epoch 4 batch 1970 loss: 2.5161261558532715\n",
      "epoch 4 batch 1971 loss: 2.1548571586608887\n",
      "epoch 4 batch 1972 loss: 1.9710073471069336\n",
      "epoch 4 batch 1973 loss: 2.2563352584838867\n",
      "epoch 4 batch 1974 loss: 2.310971736907959\n",
      "epoch 4 batch 1975 loss: 2.2316231727600098\n",
      "epoch 4 batch 1976 loss: 2.3663034439086914\n",
      "epoch 4 batch 1977 loss: 2.6065163612365723\n",
      "epoch 4 batch 1978 loss: 2.4724011421203613\n",
      "epoch 4 batch 1979 loss: 2.20886492729187\n",
      "epoch 4 batch 1980 loss: 2.2463490962982178\n",
      "epoch 4 batch 1981 loss: 2.26336407661438\n",
      "epoch 4 batch 1982 loss: 2.320241928100586\n",
      "epoch 4 batch 1983 loss: 2.205509901046753\n",
      "epoch 4 batch 1984 loss: 2.483809232711792\n",
      "epoch 4 batch 1985 loss: 2.44461727142334\n",
      "epoch 4 batch 1986 loss: 2.3571243286132812\n",
      "epoch 4 batch 1987 loss: 2.456829071044922\n",
      "epoch 4 batch 1988 loss: 2.485708236694336\n",
      "epoch 4 batch 1989 loss: 2.5366697311401367\n",
      "epoch 4 batch 1990 loss: 2.444821357727051\n",
      "epoch 4 batch 1991 loss: 2.244671583175659\n",
      "epoch 4 batch 1992 loss: 2.7093074321746826\n",
      "epoch 4 batch 1993 loss: 2.5935726165771484\n",
      "epoch 4 batch 1994 loss: 2.225433349609375\n",
      "epoch 4 batch 1995 loss: 2.3068928718566895\n",
      "epoch 4 batch 1996 loss: 2.129554271697998\n",
      "epoch 4 batch 1997 loss: 2.1400301456451416\n",
      "epoch 4 batch 1998 loss: 2.400786876678467\n",
      "epoch 4 batch 1999 loss: 2.566105365753174\n",
      "epoch 4 batch 2000 loss: 2.1589667797088623\n",
      "epoch 4 batch 2001 loss: 2.4193115234375\n",
      "epoch 4 batch 2002 loss: 2.4712910652160645\n",
      "epoch 4 batch 2003 loss: 2.3824992179870605\n",
      "epoch 4 batch 2004 loss: 2.4613194465637207\n",
      "epoch 4 batch 2005 loss: 2.365473747253418\n",
      "epoch 4 batch 2006 loss: 2.1903600692749023\n",
      "epoch 4 batch 2007 loss: 2.24837589263916\n",
      "epoch 4 batch 2008 loss: 2.1623339653015137\n",
      "epoch 4 batch 2009 loss: 2.150780200958252\n",
      "epoch 4 batch 2010 loss: 2.243069648742676\n",
      "epoch 4 batch 2011 loss: 2.4620962142944336\n",
      "epoch 4 batch 2012 loss: 2.261298179626465\n",
      "epoch 4 batch 2013 loss: 2.3950908184051514\n",
      "epoch 4 batch 2014 loss: 2.3896994590759277\n",
      "epoch 4 batch 2015 loss: 2.788865327835083\n",
      "epoch 4 batch 2016 loss: 2.269951820373535\n",
      "epoch 4 batch 2017 loss: 2.2089695930480957\n",
      "epoch 4 batch 2018 loss: 2.4795546531677246\n",
      "epoch 4 batch 2019 loss: 2.4907095432281494\n",
      "epoch 4 batch 2020 loss: 2.591972827911377\n",
      "epoch 4 batch 2021 loss: 2.4962258338928223\n",
      "epoch 4 batch 2022 loss: 2.3435726165771484\n",
      "epoch 4 batch 2023 loss: 2.1533119678497314\n",
      "epoch 4 batch 2024 loss: 2.2780332565307617\n",
      "epoch 4 batch 2025 loss: 2.333589553833008\n",
      "epoch 4 batch 2026 loss: 2.3790462017059326\n",
      "epoch 4 batch 2027 loss: 2.206908941268921\n",
      "epoch 4 batch 2028 loss: 2.2454183101654053\n",
      "epoch 4 batch 2029 loss: 2.2606258392333984\n",
      "epoch 4 batch 2030 loss: 2.306985378265381\n",
      "epoch 4 batch 2031 loss: 2.3156089782714844\n",
      "epoch 4 batch 2032 loss: 2.207023859024048\n",
      "epoch 4 batch 2033 loss: 2.4792728424072266\n",
      "epoch 4 batch 2034 loss: 2.4692137241363525\n",
      "epoch 4 batch 2035 loss: 2.0957441329956055\n",
      "epoch 4 batch 2036 loss: 2.2534964084625244\n",
      "epoch 4 batch 2037 loss: 2.258941173553467\n",
      "epoch 4 batch 2038 loss: 2.405405044555664\n",
      "epoch 4 batch 2039 loss: 2.4873085021972656\n",
      "epoch 4 batch 2040 loss: 2.3116259574890137\n",
      "epoch 4 batch 2041 loss: 2.1837222576141357\n",
      "epoch 4 batch 2042 loss: 2.6096699237823486\n",
      "epoch 4 batch 2043 loss: 2.275099992752075\n",
      "epoch 4 batch 2044 loss: 2.040602684020996\n",
      "epoch 4 batch 2045 loss: 2.271127223968506\n",
      "epoch 4 batch 2046 loss: 2.443276882171631\n",
      "epoch 4 batch 2047 loss: 2.208536386489868\n",
      "epoch 4 batch 2048 loss: 2.1032779216766357\n",
      "epoch 4 batch 2049 loss: 2.541865110397339\n",
      "epoch 4 batch 2050 loss: 2.37190842628479\n",
      "epoch 4 batch 2051 loss: 2.4444727897644043\n",
      "epoch 4 batch 2052 loss: 2.0975356101989746\n",
      "epoch 4 batch 2053 loss: 2.1498990058898926\n",
      "epoch 4 batch 2054 loss: 2.2341599464416504\n",
      "epoch 4 batch 2055 loss: 2.0903615951538086\n",
      "epoch 4 batch 2056 loss: 2.256155014038086\n",
      "epoch 4 batch 2057 loss: 2.530078887939453\n",
      "epoch 4 batch 2058 loss: 2.3724968433380127\n",
      "epoch 4 batch 2059 loss: 2.63491153717041\n",
      "epoch 4 batch 2060 loss: 2.4971423149108887\n",
      "epoch 4 batch 2061 loss: 2.3312458992004395\n",
      "epoch 4 batch 2062 loss: 2.192488670349121\n",
      "epoch 4 batch 2063 loss: 2.447634220123291\n",
      "epoch 4 batch 2064 loss: 2.455751895904541\n",
      "epoch 4 batch 2065 loss: 2.302074432373047\n",
      "epoch 4 batch 2066 loss: 2.640923500061035\n",
      "epoch 4 batch 2067 loss: 2.343194007873535\n",
      "epoch 4 batch 2068 loss: 2.4075300693511963\n",
      "epoch 4 batch 2069 loss: 2.239828109741211\n",
      "epoch 4 batch 2070 loss: 2.4550929069519043\n",
      "epoch 4 batch 2071 loss: 2.593290090560913\n",
      "epoch 4 batch 2072 loss: 2.324219226837158\n",
      "epoch 4 batch 2073 loss: 2.356560468673706\n",
      "epoch 4 batch 2074 loss: 2.3712260723114014\n",
      "epoch 4 batch 2075 loss: 2.276773691177368\n",
      "epoch 4 batch 2076 loss: 2.3040578365325928\n",
      "epoch 4 batch 2077 loss: 2.459461212158203\n",
      "epoch 4 batch 2078 loss: 2.3695406913757324\n",
      "epoch 4 batch 2079 loss: 2.0889577865600586\n",
      "epoch 4 batch 2080 loss: 2.385622978210449\n",
      "epoch 4 batch 2081 loss: 2.509481906890869\n",
      "epoch 4 batch 2082 loss: 2.5986671447753906\n",
      "epoch 4 batch 2083 loss: 2.1571202278137207\n",
      "epoch 4 batch 2084 loss: 2.3490824699401855\n",
      "epoch 4 batch 2085 loss: 2.442234754562378\n",
      "epoch 4 batch 2086 loss: 2.4394683837890625\n",
      "epoch 4 batch 2087 loss: 2.4435670375823975\n",
      "epoch 4 batch 2088 loss: 2.2558846473693848\n",
      "epoch 4 batch 2089 loss: 2.57987904548645\n",
      "epoch 4 batch 2090 loss: 2.3970751762390137\n",
      "epoch 4 batch 2091 loss: 2.3858861923217773\n",
      "epoch 4 batch 2092 loss: 2.799302577972412\n",
      "epoch 4 batch 2093 loss: 2.264157295227051\n",
      "epoch 4 batch 2094 loss: 2.3979201316833496\n",
      "epoch 4 batch 2095 loss: 2.298616886138916\n",
      "epoch 4 batch 2096 loss: 2.3110342025756836\n",
      "epoch 4 batch 2097 loss: 2.2329297065734863\n",
      "epoch 4 batch 2098 loss: 2.162909507751465\n",
      "epoch 4 batch 2099 loss: 2.43430233001709\n",
      "epoch 4 batch 2100 loss: 2.4332563877105713\n",
      "epoch 4 batch 2101 loss: 2.1376380920410156\n",
      "epoch 4 batch 2102 loss: 2.4627013206481934\n",
      "epoch 4 batch 2103 loss: 2.2212533950805664\n",
      "epoch 4 batch 2104 loss: 2.2663605213165283\n",
      "epoch 4 batch 2105 loss: 2.6923751831054688\n",
      "epoch 4 batch 2106 loss: 2.26680588722229\n",
      "epoch 4 batch 2107 loss: 2.4799609184265137\n",
      "epoch 4 batch 2108 loss: 2.1566483974456787\n",
      "epoch 4 batch 2109 loss: 2.201871871948242\n",
      "epoch 4 batch 2110 loss: 2.0514426231384277\n",
      "epoch 4 batch 2111 loss: 2.5286526679992676\n",
      "epoch 4 batch 2112 loss: 2.4932327270507812\n",
      "epoch 4 batch 2113 loss: 2.5415587425231934\n",
      "epoch 4 batch 2114 loss: 2.2971391677856445\n",
      "epoch 4 batch 2115 loss: 2.1788418292999268\n",
      "epoch 4 batch 2116 loss: 2.4428834915161133\n",
      "epoch 4 batch 2117 loss: 2.31526517868042\n",
      "epoch 4 batch 2118 loss: 2.5077357292175293\n",
      "epoch 4 batch 2119 loss: 2.4063849449157715\n",
      "epoch 4 batch 2120 loss: 2.480705499649048\n",
      "epoch 4 batch 2121 loss: 2.260425567626953\n",
      "epoch 4 batch 2122 loss: 2.423112392425537\n",
      "epoch 4 batch 2123 loss: 2.318502426147461\n",
      "epoch 4 batch 2124 loss: 2.1931369304656982\n",
      "epoch 4 batch 2125 loss: 2.2831549644470215\n",
      "epoch 4 batch 2126 loss: 2.2295982837677\n",
      "epoch 4 batch 2127 loss: 2.3422670364379883\n",
      "epoch 4 batch 2128 loss: 2.4851222038269043\n",
      "epoch 4 batch 2129 loss: 2.296830892562866\n",
      "epoch 4 batch 2130 loss: 2.486790180206299\n",
      "epoch 4 batch 2131 loss: 2.328446626663208\n",
      "epoch 4 batch 2132 loss: 2.026360511779785\n",
      "epoch 4 batch 2133 loss: 2.2746639251708984\n",
      "epoch 4 batch 2134 loss: 2.279460906982422\n",
      "epoch 4 batch 2135 loss: 2.2327938079833984\n",
      "epoch 4 batch 2136 loss: 2.5084829330444336\n",
      "epoch 4 batch 2137 loss: 2.3011393547058105\n",
      "epoch 4 batch 2138 loss: 2.499183416366577\n",
      "epoch 4 batch 2139 loss: 2.1708803176879883\n",
      "epoch 4 batch 2140 loss: 2.408937692642212\n",
      "epoch 4 batch 2141 loss: 2.2027361392974854\n",
      "epoch 4 batch 2142 loss: 2.2777600288391113\n",
      "epoch 4 batch 2143 loss: 2.1652936935424805\n",
      "epoch 4 batch 2144 loss: 2.1469953060150146\n",
      "epoch 4 batch 2145 loss: 2.1200966835021973\n",
      "epoch 4 batch 2146 loss: 2.522409439086914\n",
      "epoch 4 batch 2147 loss: 2.4236271381378174\n",
      "epoch 4 batch 2148 loss: 2.01918363571167\n",
      "epoch 4 batch 2149 loss: 2.1840524673461914\n",
      "epoch 4 batch 2150 loss: 2.1878817081451416\n",
      "epoch 4 batch 2151 loss: 2.1928391456604004\n",
      "epoch 4 batch 2152 loss: 2.286442279815674\n",
      "epoch 4 batch 2153 loss: 2.5245094299316406\n",
      "epoch 4 batch 2154 loss: 2.395246744155884\n",
      "epoch 4 batch 2155 loss: 2.4041218757629395\n",
      "epoch 4 batch 2156 loss: 2.379122257232666\n",
      "epoch 4 batch 2157 loss: 2.1027607917785645\n",
      "epoch 4 batch 2158 loss: 2.477656841278076\n",
      "epoch 4 batch 2159 loss: 2.1497740745544434\n",
      "epoch 4 batch 2160 loss: 2.33101487159729\n",
      "epoch 4 batch 2161 loss: 2.3310256004333496\n",
      "epoch 4 batch 2162 loss: 2.1991665363311768\n",
      "epoch 4 batch 2163 loss: 2.421766757965088\n",
      "epoch 4 batch 2164 loss: 2.536314010620117\n",
      "epoch 4 batch 2165 loss: 2.2615060806274414\n",
      "epoch 4 batch 2166 loss: 2.3426566123962402\n",
      "epoch 4 batch 2167 loss: 2.472994565963745\n",
      "epoch 4 batch 2168 loss: 2.120556116104126\n",
      "epoch 4 batch 2169 loss: 2.3486242294311523\n",
      "epoch 4 batch 2170 loss: 2.1443498134613037\n",
      "epoch 4 batch 2171 loss: 2.378584623336792\n",
      "epoch 4 batch 2172 loss: 2.45432186126709\n",
      "epoch 4 batch 2173 loss: 2.3690764904022217\n",
      "epoch 4 batch 2174 loss: 2.472276210784912\n",
      "epoch 4 batch 2175 loss: 2.1668760776519775\n",
      "epoch 4 batch 2176 loss: 2.309493064880371\n",
      "epoch 4 batch 2177 loss: 2.590672492980957\n",
      "epoch 4 batch 2178 loss: 2.359856128692627\n",
      "epoch 4 batch 2179 loss: 2.1603362560272217\n",
      "epoch 4 batch 2180 loss: 2.156130790710449\n",
      "epoch 4 batch 2181 loss: 2.279526710510254\n",
      "epoch 4 batch 2182 loss: 2.336350440979004\n",
      "epoch 4 batch 2183 loss: 2.3960771560668945\n",
      "epoch 4 batch 2184 loss: 2.3123509883880615\n",
      "epoch 4 batch 2185 loss: 2.050109386444092\n",
      "epoch 4 batch 2186 loss: 2.3363289833068848\n",
      "epoch 4 batch 2187 loss: 2.422667980194092\n",
      "epoch 4 batch 2188 loss: 2.3031044006347656\n",
      "epoch 4 batch 2189 loss: 2.309439182281494\n",
      "epoch 4 batch 2190 loss: 2.160106897354126\n",
      "epoch 4 batch 2191 loss: 2.3211610317230225\n",
      "epoch 4 batch 2192 loss: 2.487370014190674\n",
      "epoch 4 batch 2193 loss: 2.2376961708068848\n",
      "epoch 4 batch 2194 loss: 2.223629951477051\n",
      "epoch 4 batch 2195 loss: 2.452928066253662\n",
      "epoch 4 batch 2196 loss: 2.194260358810425\n",
      "epoch 4 batch 2197 loss: 2.621150016784668\n",
      "epoch 4 batch 2198 loss: 2.386415481567383\n",
      "epoch 4 batch 2199 loss: 2.4863455295562744\n",
      "epoch 4 batch 2200 loss: 2.3733513355255127\n",
      "epoch 4 batch 2201 loss: 2.264055013656616\n",
      "epoch 4 batch 2202 loss: 2.318347930908203\n",
      "epoch 4 batch 2203 loss: 2.4943511486053467\n",
      "epoch 4 batch 2204 loss: 2.243065118789673\n",
      "epoch 4 batch 2205 loss: 2.135878562927246\n",
      "epoch 4 batch 2206 loss: 2.455465078353882\n",
      "epoch 4 batch 2207 loss: 2.3045907020568848\n",
      "epoch 4 batch 2208 loss: 2.4194493293762207\n",
      "epoch 4 batch 2209 loss: 2.4881350994110107\n",
      "epoch 4 batch 2210 loss: 2.3096437454223633\n",
      "epoch 4 batch 2211 loss: 2.0719926357269287\n",
      "epoch 4 batch 2212 loss: 2.219794988632202\n",
      "epoch 4 batch 2213 loss: 2.5525827407836914\n",
      "epoch 4 batch 2214 loss: 2.542743682861328\n",
      "epoch 4 batch 2215 loss: 2.35168194770813\n",
      "epoch 4 batch 2216 loss: 2.2599234580993652\n",
      "epoch 4 batch 2217 loss: 2.30354642868042\n",
      "epoch 4 batch 2218 loss: 2.3482799530029297\n",
      "epoch 4 batch 2219 loss: 2.236927032470703\n",
      "epoch 4 batch 2220 loss: 2.545626401901245\n",
      "epoch 4 batch 2221 loss: 2.0571818351745605\n",
      "epoch 4 batch 2222 loss: 2.067232131958008\n",
      "epoch 4 batch 2223 loss: 2.1743814945220947\n",
      "epoch 4 batch 2224 loss: 2.0889883041381836\n",
      "epoch 4 batch 2225 loss: 2.5559449195861816\n",
      "epoch 4 batch 2226 loss: 2.3872971534729004\n",
      "epoch 4 batch 2227 loss: 2.420177459716797\n",
      "epoch 4 batch 2228 loss: 2.4123528003692627\n",
      "epoch 4 batch 2229 loss: 2.2697153091430664\n",
      "epoch 4 batch 2230 loss: 2.3760342597961426\n",
      "epoch 4 batch 2231 loss: 2.251771926879883\n",
      "epoch 4 batch 2232 loss: 2.516842842102051\n",
      "epoch 4 batch 2233 loss: 2.2422423362731934\n",
      "epoch 4 batch 2234 loss: 2.4461545944213867\n",
      "epoch 4 batch 2235 loss: 2.375455379486084\n",
      "epoch 4 batch 2236 loss: 2.3274784088134766\n",
      "epoch 4 batch 2237 loss: 2.3212509155273438\n",
      "epoch 4 batch 2238 loss: 2.3595924377441406\n",
      "epoch 4 batch 2239 loss: 2.27291202545166\n",
      "epoch 4 batch 2240 loss: 2.434953212738037\n",
      "epoch 4 batch 2241 loss: 2.462725877761841\n",
      "epoch 4 batch 2242 loss: 2.356189250946045\n",
      "epoch 4 batch 2243 loss: 2.285210132598877\n",
      "epoch 4 batch 2244 loss: 2.190250873565674\n",
      "epoch 4 batch 2245 loss: 2.2274010181427\n",
      "epoch 4 batch 2246 loss: 2.5456573963165283\n",
      "epoch 4 batch 2247 loss: 2.173037528991699\n",
      "epoch 4 batch 2248 loss: 2.3065600395202637\n",
      "epoch 4 batch 2249 loss: 2.459430694580078\n",
      "epoch 4 batch 2250 loss: 2.1796112060546875\n",
      "epoch 4 batch 2251 loss: 2.282285690307617\n",
      "epoch 4 batch 2252 loss: 2.678652763366699\n",
      "epoch 4 batch 2253 loss: 2.4275217056274414\n",
      "epoch 4 batch 2254 loss: 2.3794994354248047\n",
      "epoch 4 batch 2255 loss: 2.169844388961792\n",
      "epoch 4 batch 2256 loss: 2.316255807876587\n",
      "epoch 4 batch 2257 loss: 2.4071977138519287\n",
      "epoch 4 batch 2258 loss: 2.6034412384033203\n",
      "epoch 4 batch 2259 loss: 2.3949074745178223\n",
      "epoch 4 batch 2260 loss: 2.3453493118286133\n",
      "epoch 4 batch 2261 loss: 2.3527169227600098\n",
      "epoch 4 batch 2262 loss: 2.6114206314086914\n",
      "epoch 4 batch 2263 loss: 2.541602849960327\n",
      "epoch 4 batch 2264 loss: 2.3016371726989746\n",
      "epoch 4 batch 2265 loss: 2.605957508087158\n",
      "epoch 4 batch 2266 loss: 2.176145076751709\n",
      "epoch 4 batch 2267 loss: 2.2847700119018555\n",
      "epoch 4 batch 2268 loss: 2.3511292934417725\n",
      "epoch 4 batch 2269 loss: 2.1876611709594727\n",
      "epoch 4 batch 2270 loss: 2.3296120166778564\n",
      "epoch 4 batch 2271 loss: 2.1120810508728027\n",
      "epoch 4 batch 2272 loss: 2.3096282482147217\n",
      "epoch 4 batch 2273 loss: 2.401528835296631\n",
      "epoch 4 batch 2274 loss: 2.2021231651306152\n",
      "epoch 4 batch 2275 loss: 2.432711601257324\n",
      "epoch 4 batch 2276 loss: 2.200925350189209\n",
      "epoch 4 batch 2277 loss: 2.262180805206299\n",
      "epoch 4 batch 2278 loss: 2.486804485321045\n",
      "epoch 4 batch 2279 loss: 2.4347214698791504\n",
      "epoch 4 batch 2280 loss: 2.4895451068878174\n",
      "epoch 4 batch 2281 loss: 2.1198501586914062\n",
      "epoch 4 batch 2282 loss: 2.130671501159668\n",
      "epoch 4 batch 2283 loss: 2.0067121982574463\n",
      "epoch 4 batch 2284 loss: 2.4852099418640137\n",
      "epoch 4 batch 2285 loss: 2.200841188430786\n",
      "epoch 4 batch 2286 loss: 2.3251547813415527\n",
      "epoch 4 batch 2287 loss: 2.223493814468384\n",
      "epoch 4 batch 2288 loss: 2.2440438270568848\n",
      "epoch 4 batch 2289 loss: 2.416951894760132\n",
      "epoch 4 batch 2290 loss: 2.3635621070861816\n",
      "epoch 4 batch 2291 loss: 2.4359612464904785\n",
      "epoch 4 batch 2292 loss: 2.2551002502441406\n",
      "epoch 4 batch 2293 loss: 2.348659038543701\n",
      "epoch 4 batch 2294 loss: 2.322709798812866\n",
      "epoch 4 batch 2295 loss: 2.370894193649292\n",
      "epoch 4 batch 2296 loss: 2.229078769683838\n",
      "epoch 4 batch 2297 loss: 2.340395927429199\n",
      "epoch 4 batch 2298 loss: 2.4804534912109375\n",
      "epoch 4 batch 2299 loss: 2.2017760276794434\n",
      "epoch 4 batch 2300 loss: 2.0834908485412598\n",
      "epoch 4 batch 2301 loss: 2.457059860229492\n",
      "epoch 4 batch 2302 loss: 2.346789836883545\n",
      "epoch 4 batch 2303 loss: 2.125805139541626\n",
      "epoch 4 batch 2304 loss: 2.281311511993408\n",
      "epoch 4 batch 2305 loss: 2.333306312561035\n",
      "epoch 4 batch 2306 loss: 2.390748977661133\n",
      "epoch 4 batch 2307 loss: 2.228677749633789\n",
      "epoch 4 batch 2308 loss: 2.133615493774414\n",
      "epoch 4 batch 2309 loss: 2.41751766204834\n",
      "epoch 4 batch 2310 loss: 2.264940023422241\n",
      "epoch 4 batch 2311 loss: 2.4408624172210693\n",
      "epoch 4 batch 2312 loss: 2.3887219429016113\n",
      "epoch 4 batch 2313 loss: 2.4369900226593018\n",
      "epoch 4 batch 2314 loss: 2.2418107986450195\n",
      "epoch 4 batch 2315 loss: 2.618647336959839\n",
      "epoch 4 batch 2316 loss: 2.6051230430603027\n",
      "epoch 4 batch 2317 loss: 2.387800693511963\n",
      "epoch 4 batch 2318 loss: 2.3523402214050293\n",
      "epoch 4 batch 2319 loss: 2.4101486206054688\n",
      "epoch 4 batch 2320 loss: 2.2074480056762695\n",
      "epoch 4 batch 2321 loss: 2.165795087814331\n",
      "epoch 4 batch 2322 loss: 2.0991971492767334\n",
      "epoch 4 batch 2323 loss: 2.3424668312072754\n",
      "epoch 4 batch 2324 loss: 2.280885696411133\n",
      "epoch 4 batch 2325 loss: 2.018918752670288\n",
      "epoch 4 batch 2326 loss: 2.471024990081787\n",
      "epoch 4 batch 2327 loss: 2.176760196685791\n",
      "epoch 4 batch 2328 loss: 2.1697497367858887\n",
      "epoch 4 batch 2329 loss: 2.5713412761688232\n",
      "epoch 4 batch 2330 loss: 2.3090758323669434\n",
      "epoch 4 batch 2331 loss: 2.0446276664733887\n",
      "epoch 4 batch 2332 loss: 2.381802558898926\n",
      "epoch 4 batch 2333 loss: 2.3253073692321777\n",
      "epoch 4 batch 2334 loss: 2.419517993927002\n",
      "epoch 4 batch 2335 loss: 2.4132628440856934\n",
      "epoch 4 batch 2336 loss: 2.3523659706115723\n",
      "epoch 4 batch 2337 loss: 2.3703577518463135\n",
      "epoch 4 batch 2338 loss: 2.5198464393615723\n",
      "epoch 4 batch 2339 loss: 2.1141791343688965\n",
      "epoch 4 batch 2340 loss: 2.152263641357422\n",
      "epoch 4 batch 2341 loss: 2.130272388458252\n",
      "epoch 4 batch 2342 loss: 2.4552993774414062\n",
      "epoch 4 batch 2343 loss: 2.411848545074463\n",
      "epoch 4 batch 2344 loss: 2.416818618774414\n",
      "epoch 4 batch 2345 loss: 2.4064393043518066\n",
      "epoch 4 batch 2346 loss: 2.6205649375915527\n",
      "epoch 4 batch 2347 loss: 2.4988789558410645\n",
      "epoch 4 batch 2348 loss: 2.5730419158935547\n",
      "epoch 4 batch 2349 loss: 2.394937038421631\n",
      "epoch 4 batch 2350 loss: 2.331862449645996\n",
      "epoch 4 batch 2351 loss: 2.1093595027923584\n",
      "epoch 4 batch 2352 loss: 2.171544313430786\n",
      "epoch 4 batch 2353 loss: 2.1003923416137695\n",
      "epoch 4 batch 2354 loss: 2.133462905883789\n",
      "epoch 4 batch 2355 loss: 2.5125629901885986\n",
      "epoch 4 batch 2356 loss: 2.4179086685180664\n",
      "epoch 4 batch 2357 loss: 2.7416141033172607\n",
      "epoch 4 batch 2358 loss: 2.45428204536438\n",
      "epoch 4 batch 2359 loss: 2.3421759605407715\n",
      "epoch 4 batch 2360 loss: 2.292698383331299\n",
      "epoch 4 batch 2361 loss: 2.544229507446289\n",
      "epoch 4 batch 2362 loss: 2.09539794921875\n",
      "epoch 4 batch 2363 loss: 2.3524317741394043\n",
      "epoch 4 batch 2364 loss: 2.336991310119629\n",
      "epoch 4 batch 2365 loss: 2.473235607147217\n",
      "epoch 4 batch 2366 loss: 2.588779926300049\n",
      "epoch 4 batch 2367 loss: 2.3814358711242676\n",
      "epoch 4 batch 2368 loss: 2.378767490386963\n",
      "epoch 4 batch 2369 loss: 2.312760353088379\n",
      "epoch 4 batch 2370 loss: 2.2497005462646484\n",
      "epoch 4 batch 2371 loss: 2.8092100620269775\n",
      "epoch 4 batch 2372 loss: 2.4844298362731934\n",
      "epoch 4 batch 2373 loss: 2.427208423614502\n",
      "epoch 4 batch 2374 loss: 2.463894844055176\n",
      "epoch 4 batch 2375 loss: 2.4286088943481445\n",
      "epoch 4 batch 2376 loss: 2.227334499359131\n",
      "epoch 4 batch 2377 loss: 2.4178125858306885\n",
      "epoch 4 batch 2378 loss: 2.2342212200164795\n",
      "epoch 4 batch 2379 loss: 2.2548279762268066\n",
      "epoch 4 batch 2380 loss: 2.089268684387207\n",
      "epoch 4 batch 2381 loss: 2.2114903926849365\n",
      "epoch 4 batch 2382 loss: 2.3539514541625977\n",
      "epoch 4 batch 2383 loss: 2.367889404296875\n",
      "epoch 4 batch 2384 loss: 2.2564096450805664\n",
      "epoch 4 batch 2385 loss: 2.556814670562744\n",
      "epoch 4 batch 2386 loss: 2.39963960647583\n",
      "epoch 4 batch 2387 loss: 2.34833025932312\n",
      "epoch 4 batch 2388 loss: 2.4008679389953613\n",
      "epoch 4 batch 2389 loss: 2.3821096420288086\n",
      "epoch 4 batch 2390 loss: 2.2552297115325928\n",
      "epoch 4 batch 2391 loss: 2.196943521499634\n",
      "epoch 4 batch 2392 loss: 2.110926389694214\n",
      "epoch 4 batch 2393 loss: 2.331819772720337\n",
      "epoch 4 batch 2394 loss: 2.159273147583008\n",
      "epoch 4 batch 2395 loss: 2.6798255443573\n",
      "epoch 4 batch 2396 loss: 2.3181064128875732\n",
      "epoch 4 batch 2397 loss: 2.0996456146240234\n",
      "epoch 4 batch 2398 loss: 2.3097681999206543\n",
      "epoch 4 batch 2399 loss: 2.7006523609161377\n",
      "epoch 4 batch 2400 loss: 2.5473358631134033\n",
      "epoch 4 batch 2401 loss: 2.6173343658447266\n",
      "epoch 4 batch 2402 loss: 2.406765937805176\n",
      "epoch 4 batch 2403 loss: 2.3315985202789307\n",
      "epoch 4 batch 2404 loss: 2.1918225288391113\n",
      "epoch 4 batch 2405 loss: 2.378354072570801\n",
      "epoch 4 batch 2406 loss: 2.2889342308044434\n",
      "epoch 4 batch 2407 loss: 2.251652717590332\n",
      "epoch 4 batch 2408 loss: 2.063032865524292\n",
      "epoch 4 batch 2409 loss: 2.235783100128174\n",
      "epoch 4 batch 2410 loss: 2.5026750564575195\n",
      "epoch 4 batch 2411 loss: 2.32063364982605\n",
      "epoch 4 batch 2412 loss: 2.322293281555176\n",
      "epoch 4 batch 2413 loss: 2.46235728263855\n",
      "epoch 4 batch 2414 loss: 2.2333319187164307\n",
      "epoch 4 batch 2415 loss: 2.4368364810943604\n",
      "epoch 4 batch 2416 loss: 2.1349947452545166\n",
      "epoch 4 batch 2417 loss: 2.540085792541504\n",
      "epoch 4 batch 2418 loss: 2.5027053356170654\n",
      "epoch 4 batch 2419 loss: 2.2258472442626953\n",
      "epoch 4 batch 2420 loss: 2.29158878326416\n",
      "epoch 4 batch 2421 loss: 2.3573856353759766\n",
      "epoch 4 batch 2422 loss: 2.5037291049957275\n",
      "epoch 4 batch 2423 loss: 2.178788661956787\n",
      "epoch 4 batch 2424 loss: 2.3504719734191895\n",
      "epoch 4 batch 2425 loss: 2.1785778999328613\n",
      "epoch 4 batch 2426 loss: 2.157700538635254\n",
      "epoch 4 batch 2427 loss: 2.4876668453216553\n",
      "epoch 4 batch 2428 loss: 2.3568811416625977\n",
      "epoch 4 batch 2429 loss: 2.3500866889953613\n",
      "epoch 4 batch 2430 loss: 2.5373120307922363\n",
      "epoch 4 batch 2431 loss: 2.492215633392334\n",
      "epoch 4 batch 2432 loss: 2.3393476009368896\n",
      "epoch 4 batch 2433 loss: 2.363753318786621\n",
      "epoch 4 batch 2434 loss: 2.2123005390167236\n",
      "epoch 4 batch 2435 loss: 2.2618236541748047\n",
      "epoch 4 batch 2436 loss: 2.2924139499664307\n",
      "epoch 4 batch 2437 loss: 2.392530679702759\n",
      "epoch 4 batch 2438 loss: 2.0601062774658203\n",
      "epoch 4 batch 2439 loss: 2.446350336074829\n",
      "epoch 4 batch 2440 loss: 2.2626590728759766\n",
      "epoch 4 batch 2441 loss: 2.1878468990325928\n",
      "epoch 4 batch 2442 loss: 2.35880446434021\n",
      "epoch 4 batch 2443 loss: 2.590935468673706\n",
      "epoch 4 batch 2444 loss: 2.5815351009368896\n",
      "epoch 4 batch 2445 loss: 2.2142200469970703\n",
      "epoch 4 batch 2446 loss: 2.420290470123291\n",
      "epoch 4 batch 2447 loss: 2.221682071685791\n",
      "epoch 4 batch 2448 loss: 2.287838935852051\n",
      "epoch 4 batch 2449 loss: 2.153749942779541\n",
      "epoch 4 batch 2450 loss: 2.167591094970703\n",
      "epoch 4 batch 2451 loss: 2.1866326332092285\n",
      "epoch 4 batch 2452 loss: 2.2079098224639893\n",
      "epoch 4 batch 2453 loss: 2.449580192565918\n",
      "epoch 4 batch 2454 loss: 2.0856308937072754\n",
      "epoch 4 batch 2455 loss: 2.2608518600463867\n",
      "epoch 4 batch 2456 loss: 2.5397486686706543\n",
      "epoch 4 batch 2457 loss: 2.4219765663146973\n",
      "epoch 4 batch 2458 loss: 2.5389575958251953\n",
      "epoch 4 batch 2459 loss: 2.334578514099121\n",
      "epoch 4 batch 2460 loss: 2.6844305992126465\n",
      "epoch 4 batch 2461 loss: 2.283414840698242\n",
      "epoch 4 batch 2462 loss: 2.0570151805877686\n",
      "epoch 4 batch 2463 loss: 2.4160327911376953\n",
      "epoch 4 batch 2464 loss: 2.2974796295166016\n",
      "epoch 4 batch 2465 loss: 2.415137767791748\n",
      "epoch 4 batch 2466 loss: 2.071274518966675\n",
      "epoch 4 batch 2467 loss: 2.181331157684326\n",
      "epoch 4 batch 2468 loss: 2.552576780319214\n",
      "epoch 4 batch 2469 loss: 2.556366205215454\n",
      "epoch 4 batch 2470 loss: 2.2735910415649414\n",
      "epoch 4 batch 2471 loss: 2.5020227432250977\n",
      "epoch 4 batch 2472 loss: 2.276526927947998\n",
      "epoch 4 batch 2473 loss: 2.1685712337493896\n",
      "epoch 4 batch 2474 loss: 2.272559881210327\n",
      "epoch 4 batch 2475 loss: 2.300851583480835\n",
      "epoch 4 batch 2476 loss: 2.227987766265869\n",
      "epoch 4 batch 2477 loss: 2.0962605476379395\n",
      "epoch 4 batch 2478 loss: 2.1854915618896484\n",
      "epoch 4 batch 2479 loss: 2.206857681274414\n",
      "epoch 4 batch 2480 loss: 2.3425722122192383\n",
      "epoch 4 batch 2481 loss: 2.373525619506836\n",
      "epoch 4 batch 2482 loss: 2.2560696601867676\n",
      "epoch 4 batch 2483 loss: 2.0237083435058594\n",
      "epoch 4 batch 2484 loss: 2.203286647796631\n",
      "epoch 4 batch 2485 loss: 2.3831539154052734\n",
      "epoch 4 batch 2486 loss: 2.448486328125\n",
      "epoch 4 batch 2487 loss: 2.4000165462493896\n",
      "epoch 4 batch 2488 loss: 2.3061046600341797\n",
      "epoch 4 batch 2489 loss: 2.382845878601074\n",
      "epoch 4 batch 2490 loss: 2.2031636238098145\n",
      "epoch 4 batch 2491 loss: 2.3815083503723145\n",
      "epoch 4 batch 2492 loss: 2.4727818965911865\n",
      "epoch 4 batch 2493 loss: 2.750246047973633\n",
      "epoch 4 batch 2494 loss: 2.3046488761901855\n",
      "epoch 4 batch 2495 loss: 2.452178716659546\n",
      "epoch 4 batch 2496 loss: 2.1331393718719482\n",
      "epoch 4 batch 2497 loss: 2.2293667793273926\n",
      "epoch 4 batch 2498 loss: 2.235491991043091\n",
      "epoch 4 batch 2499 loss: 2.296691656112671\n",
      "epoch 4 batch 2500 loss: 2.2795541286468506\n",
      "epoch 4 batch 2501 loss: 2.4383552074432373\n",
      "epoch 4 batch 2502 loss: 2.306194305419922\n",
      "epoch 4 batch 2503 loss: 2.4038331508636475\n",
      "epoch 4 batch 2504 loss: 2.2966060638427734\n",
      "epoch 4 batch 2505 loss: 2.3440072536468506\n",
      "epoch 4 batch 2506 loss: 2.5633432865142822\n",
      "epoch 4 batch 2507 loss: 2.346797466278076\n",
      "epoch 4 batch 2508 loss: 2.4715816974639893\n",
      "epoch 4 batch 2509 loss: 2.305474281311035\n",
      "epoch 4 batch 2510 loss: 2.1501240730285645\n",
      "epoch 4 batch 2511 loss: 2.3573365211486816\n",
      "epoch 4 batch 2512 loss: 2.3434975147247314\n",
      "epoch 4 batch 2513 loss: 2.106339693069458\n",
      "epoch 4 batch 2514 loss: 2.4009132385253906\n",
      "epoch 4 batch 2515 loss: 2.414813756942749\n",
      "epoch 4 batch 2516 loss: 2.334209680557251\n",
      "epoch 4 batch 2517 loss: 2.6127569675445557\n",
      "epoch 4 batch 2518 loss: 2.0506319999694824\n",
      "epoch 4 batch 2519 loss: 2.4931342601776123\n",
      "epoch 4 batch 2520 loss: 2.248988151550293\n",
      "epoch 4 batch 2521 loss: 2.6030640602111816\n",
      "epoch 4 batch 2522 loss: 2.2633912563323975\n",
      "epoch 4 batch 2523 loss: 2.171571731567383\n",
      "epoch 4 batch 2524 loss: 2.192342758178711\n",
      "epoch 4 batch 2525 loss: 2.3304898738861084\n",
      "epoch 4 batch 2526 loss: 2.119384765625\n",
      "epoch 4 batch 2527 loss: 2.5408754348754883\n",
      "epoch 4 batch 2528 loss: 2.730482578277588\n",
      "epoch 4 batch 2529 loss: 2.180896043777466\n",
      "epoch 4 batch 2530 loss: 2.3010447025299072\n",
      "epoch 4 batch 2531 loss: 2.257718563079834\n",
      "epoch 4 batch 2532 loss: 2.3301680088043213\n",
      "epoch 4 batch 2533 loss: 2.7558414936065674\n",
      "epoch 4 batch 2534 loss: 2.1827502250671387\n",
      "epoch 4 batch 2535 loss: 2.2095134258270264\n",
      "epoch 4 batch 2536 loss: 2.394608497619629\n",
      "epoch 4 batch 2537 loss: 2.326249599456787\n",
      "epoch 4 batch 2538 loss: 2.4433345794677734\n",
      "epoch 4 batch 2539 loss: 2.385044574737549\n",
      "epoch 4 batch 2540 loss: 2.325885772705078\n",
      "epoch 4 batch 2541 loss: 2.4317567348480225\n",
      "epoch 4 batch 2542 loss: 2.1891391277313232\n",
      "epoch 4 batch 2543 loss: 2.1942992210388184\n",
      "epoch 4 batch 2544 loss: 2.740283489227295\n",
      "epoch 4 batch 2545 loss: 2.34116268157959\n",
      "epoch 4 batch 2546 loss: 2.2957441806793213\n",
      "epoch 4 batch 2547 loss: 2.435845375061035\n",
      "epoch 4 batch 2548 loss: 2.62398099899292\n",
      "epoch 4 batch 2549 loss: 2.551316976547241\n",
      "epoch 4 batch 2550 loss: 2.241555690765381\n",
      "epoch 4 batch 2551 loss: 2.5323424339294434\n",
      "epoch 4 batch 2552 loss: 2.420851230621338\n",
      "epoch 4 batch 2553 loss: 2.2536416053771973\n",
      "epoch 4 batch 2554 loss: 2.1994214057922363\n",
      "epoch 4 batch 2555 loss: 2.358513355255127\n",
      "epoch 4 batch 2556 loss: 2.1299312114715576\n",
      "epoch 4 batch 2557 loss: 2.2496957778930664\n",
      "epoch 4 batch 2558 loss: 2.2410459518432617\n",
      "epoch 4 batch 2559 loss: 2.255436420440674\n",
      "epoch 4 batch 2560 loss: 2.272784948348999\n",
      "epoch 4 batch 2561 loss: 2.3007419109344482\n",
      "epoch 4 batch 2562 loss: 2.317519187927246\n",
      "epoch 4 batch 2563 loss: 2.5729191303253174\n",
      "epoch 4 batch 2564 loss: 2.3092823028564453\n",
      "epoch 4 batch 2565 loss: 2.3705294132232666\n",
      "epoch 4 batch 2566 loss: 2.5563764572143555\n",
      "epoch 4 batch 2567 loss: 2.566352367401123\n",
      "epoch 4 batch 2568 loss: 2.5479366779327393\n",
      "epoch 4 batch 2569 loss: 2.5071282386779785\n",
      "epoch 4 batch 2570 loss: 2.2935001850128174\n",
      "epoch 4 batch 2571 loss: 2.324282646179199\n",
      "epoch 4 batch 2572 loss: 2.1345138549804688\n",
      "epoch 4 batch 2573 loss: 2.3690524101257324\n",
      "epoch 4 batch 2574 loss: 2.6978020668029785\n",
      "epoch 4 batch 2575 loss: 2.4067282676696777\n",
      "epoch 4 batch 2576 loss: 2.243298053741455\n",
      "epoch 4 batch 2577 loss: 2.19215726852417\n",
      "epoch 4 batch 2578 loss: 2.2292914390563965\n",
      "epoch 4 batch 2579 loss: 2.343459129333496\n",
      "epoch 4 batch 2580 loss: 2.191387176513672\n",
      "epoch 4 batch 2581 loss: 2.5949273109436035\n",
      "epoch 4 batch 2582 loss: 2.0652971267700195\n",
      "epoch 4 batch 2583 loss: 2.4174671173095703\n",
      "epoch 4 batch 2584 loss: 2.1474649906158447\n",
      "epoch 4 batch 2585 loss: 2.1067352294921875\n",
      "epoch 4 batch 2586 loss: 2.332409143447876\n",
      "epoch 4 batch 2587 loss: 2.3627922534942627\n",
      "epoch 4 batch 2588 loss: 2.4067680835723877\n",
      "epoch 4 batch 2589 loss: 2.137467861175537\n",
      "epoch 4 batch 2590 loss: 2.3255372047424316\n",
      "epoch 4 batch 2591 loss: 2.267960548400879\n",
      "epoch 4 batch 2592 loss: 2.5667552947998047\n",
      "epoch 4 batch 2593 loss: 2.0470268726348877\n",
      "epoch 4 batch 2594 loss: 2.7483198642730713\n",
      "epoch 4 batch 2595 loss: 2.2366700172424316\n",
      "epoch 4 batch 2596 loss: 2.3467955589294434\n",
      "epoch 4 batch 2597 loss: 2.29974102973938\n",
      "epoch 4 batch 2598 loss: 2.5583159923553467\n",
      "epoch 4 batch 2599 loss: 2.197963237762451\n",
      "epoch 4 batch 2600 loss: 2.3759632110595703\n",
      "epoch 4 batch 2601 loss: 2.3638176918029785\n",
      "epoch 4 batch 2602 loss: 2.3914833068847656\n",
      "epoch 4 batch 2603 loss: 2.4550020694732666\n",
      "epoch 4 batch 2604 loss: 2.2813823223114014\n",
      "epoch 4 batch 2605 loss: 2.2541205883026123\n",
      "epoch 4 batch 2606 loss: 2.3575713634490967\n",
      "epoch 4 batch 2607 loss: 2.2607204914093018\n",
      "epoch 4 batch 2608 loss: 2.4320268630981445\n",
      "epoch 4 batch 2609 loss: 2.2320737838745117\n",
      "epoch 4 batch 2610 loss: 2.2247514724731445\n",
      "epoch 4 batch 2611 loss: 2.480010509490967\n",
      "epoch 4 batch 2612 loss: 2.563342571258545\n",
      "epoch 4 batch 2613 loss: 2.2027859687805176\n",
      "epoch 4 batch 2614 loss: 2.2681984901428223\n",
      "epoch 4 batch 2615 loss: 2.134934186935425\n",
      "epoch 4 batch 2616 loss: 2.0590734481811523\n",
      "epoch 4 batch 2617 loss: 2.1011476516723633\n",
      "epoch 4 batch 2618 loss: 2.2862300872802734\n",
      "epoch 4 batch 2619 loss: 2.071550130844116\n",
      "epoch 4 batch 2620 loss: 2.333097219467163\n",
      "epoch 4 batch 2621 loss: 2.2790403366088867\n",
      "epoch 4 batch 2622 loss: 2.5553388595581055\n",
      "epoch 4 batch 2623 loss: 2.1286001205444336\n",
      "epoch 4 batch 2624 loss: 2.277158498764038\n",
      "epoch 4 batch 2625 loss: 2.5222771167755127\n",
      "epoch 4 batch 2626 loss: 2.314413070678711\n",
      "epoch 4 batch 2627 loss: 2.0019164085388184\n",
      "epoch 4 batch 2628 loss: 2.506235122680664\n",
      "epoch 4 batch 2629 loss: 2.2527341842651367\n",
      "epoch 4 batch 2630 loss: 2.1714446544647217\n",
      "epoch 4 batch 2631 loss: 2.2585017681121826\n",
      "epoch 4 batch 2632 loss: 2.193622350692749\n",
      "epoch 4 batch 2633 loss: 2.4345171451568604\n",
      "epoch 4 batch 2634 loss: 2.168222427368164\n",
      "epoch 4 batch 2635 loss: 2.444791316986084\n",
      "epoch 4 batch 2636 loss: 2.1100409030914307\n",
      "epoch 4 batch 2637 loss: 2.0097010135650635\n",
      "epoch 4 batch 2638 loss: 1.9737968444824219\n",
      "epoch 4 batch 2639 loss: 2.2809953689575195\n",
      "epoch 4 batch 2640 loss: 2.509732246398926\n",
      "epoch 4 batch 2641 loss: 2.3429367542266846\n",
      "epoch 4 batch 2642 loss: 2.2620761394500732\n",
      "epoch 4 batch 2643 loss: 2.0940237045288086\n",
      "epoch 4 batch 2644 loss: 2.1922192573547363\n",
      "epoch 4 batch 2645 loss: 2.3702006340026855\n",
      "epoch 4 batch 2646 loss: 2.02671480178833\n",
      "epoch 4 batch 2647 loss: 2.4896724224090576\n",
      "epoch 4 batch 2648 loss: 2.1305971145629883\n",
      "epoch 4 batch 2649 loss: 2.3501834869384766\n",
      "epoch 4 batch 2650 loss: 2.1937458515167236\n",
      "epoch 4 batch 2651 loss: 2.215696334838867\n",
      "epoch 4 batch 2652 loss: 2.2184982299804688\n",
      "epoch 4 batch 2653 loss: 2.450687885284424\n",
      "epoch 4 batch 2654 loss: 2.18158221244812\n",
      "epoch 4 batch 2655 loss: 2.4107298851013184\n",
      "epoch 4 batch 2656 loss: 2.068352222442627\n",
      "epoch 4 batch 2657 loss: 2.180670738220215\n",
      "epoch 4 batch 2658 loss: 2.7888965606689453\n",
      "epoch 4 batch 2659 loss: 2.169095516204834\n",
      "epoch 4 batch 2660 loss: 2.2747316360473633\n",
      "epoch 4 batch 2661 loss: 2.305722236633301\n",
      "epoch 4 batch 2662 loss: 2.6029279232025146\n",
      "epoch 4 batch 2663 loss: 2.6035304069519043\n",
      "epoch 4 batch 2664 loss: 2.2988338470458984\n",
      "epoch 4 batch 2665 loss: 2.188613176345825\n",
      "epoch 4 batch 2666 loss: 2.3722407817840576\n",
      "epoch 4 batch 2667 loss: 2.8001880645751953\n",
      "epoch 4 batch 2668 loss: 2.4973549842834473\n",
      "epoch 4 batch 2669 loss: 2.5193886756896973\n",
      "epoch 4 batch 2670 loss: 2.103705883026123\n",
      "epoch 4 batch 2671 loss: 2.599696159362793\n",
      "epoch 4 batch 2672 loss: 2.225065231323242\n",
      "epoch 4 batch 2673 loss: 2.0589661598205566\n",
      "epoch 4 batch 2674 loss: 2.4038734436035156\n",
      "epoch 4 batch 2675 loss: 2.278501510620117\n",
      "epoch 4 batch 2676 loss: 2.1769931316375732\n",
      "epoch 4 batch 2677 loss: 2.1292972564697266\n",
      "epoch 4 batch 2678 loss: 2.3271312713623047\n",
      "epoch 4 batch 2679 loss: 2.183798313140869\n",
      "epoch 4 batch 2680 loss: 2.3576488494873047\n",
      "epoch 4 batch 2681 loss: 2.3795089721679688\n",
      "epoch 4 batch 2682 loss: 2.39349365234375\n",
      "epoch 4 batch 2683 loss: 2.349125385284424\n",
      "epoch 4 batch 2684 loss: 2.3103511333465576\n",
      "epoch 4 batch 2685 loss: 2.6343212127685547\n",
      "epoch 4 batch 2686 loss: 2.2936744689941406\n",
      "epoch 4 batch 2687 loss: 2.3147192001342773\n",
      "epoch 4 batch 2688 loss: 2.276945114135742\n",
      "epoch 4 batch 2689 loss: 2.4582083225250244\n",
      "epoch 4 batch 2690 loss: 2.038619041442871\n",
      "epoch 4 batch 2691 loss: 2.4884066581726074\n",
      "epoch 4 batch 2692 loss: 2.125990390777588\n",
      "epoch 4 batch 2693 loss: 2.386793851852417\n",
      "epoch 4 batch 2694 loss: 2.4144935607910156\n",
      "epoch 4 batch 2695 loss: 2.3092408180236816\n",
      "epoch 4 batch 2696 loss: 2.3975303173065186\n",
      "epoch 4 batch 2697 loss: 2.2523128986358643\n",
      "epoch 4 batch 2698 loss: 2.204415798187256\n",
      "epoch 4 batch 2699 loss: 2.3851187229156494\n",
      "epoch 4 batch 2700 loss: 2.306051731109619\n",
      "epoch 4 batch 2701 loss: 2.2832953929901123\n",
      "epoch 4 batch 2702 loss: 2.061615467071533\n",
      "epoch 4 batch 2703 loss: 2.390603542327881\n",
      "epoch 4 batch 2704 loss: 2.231894016265869\n",
      "epoch 4 batch 2705 loss: 2.542201042175293\n",
      "epoch 4 batch 2706 loss: 2.4123573303222656\n",
      "epoch 4 batch 2707 loss: 2.2319650650024414\n",
      "epoch 4 batch 2708 loss: 2.458691120147705\n",
      "epoch 4 batch 2709 loss: 2.1669766902923584\n",
      "epoch 4 batch 2710 loss: 2.299469470977783\n",
      "epoch 4 batch 2711 loss: 2.5765459537506104\n",
      "epoch 4 batch 2712 loss: 2.3172380924224854\n",
      "epoch 4 batch 2713 loss: 2.179210901260376\n",
      "epoch 4 batch 2714 loss: 2.2574286460876465\n",
      "epoch 4 batch 2715 loss: 2.124375820159912\n",
      "epoch 4 batch 2716 loss: 2.554905891418457\n",
      "epoch 4 batch 2717 loss: 2.431399345397949\n",
      "epoch 4 batch 2718 loss: 2.156099319458008\n",
      "epoch 4 batch 2719 loss: 2.2918593883514404\n",
      "epoch 4 batch 2720 loss: 2.459414005279541\n",
      "epoch 4 batch 2721 loss: 2.3965141773223877\n",
      "epoch 4 batch 2722 loss: 2.348775863647461\n",
      "epoch 4 batch 2723 loss: 2.6052370071411133\n",
      "epoch 4 batch 2724 loss: 2.421010971069336\n",
      "epoch 4 batch 2725 loss: 2.5686652660369873\n",
      "epoch 4 batch 2726 loss: 2.4366812705993652\n",
      "epoch 4 batch 2727 loss: 2.1875689029693604\n",
      "epoch 4 batch 2728 loss: 2.2781643867492676\n",
      "epoch 4 batch 2729 loss: 2.2987709045410156\n",
      "epoch 4 batch 2730 loss: 2.166842222213745\n",
      "epoch 4 batch 2731 loss: 2.0855844020843506\n",
      "epoch 4 batch 2732 loss: 2.3535263538360596\n",
      "epoch 4 batch 2733 loss: 2.240809440612793\n",
      "epoch 4 batch 2734 loss: 2.5569658279418945\n",
      "epoch 4 batch 2735 loss: 2.434962749481201\n",
      "epoch 4 batch 2736 loss: 2.1129233837127686\n",
      "epoch 4 batch 2737 loss: 2.5548956394195557\n",
      "epoch 4 batch 2738 loss: 2.265911817550659\n",
      "epoch 4 batch 2739 loss: 2.449652671813965\n",
      "epoch 4 batch 2740 loss: 2.4994401931762695\n",
      "epoch 4 batch 2741 loss: 2.346083164215088\n",
      "epoch 4 batch 2742 loss: 2.4989514350891113\n",
      "epoch 4 batch 2743 loss: 2.4961400032043457\n",
      "epoch 4 batch 2744 loss: 2.2604987621307373\n",
      "epoch 4 batch 2745 loss: 2.482851505279541\n",
      "epoch 4 batch 2746 loss: 2.196023464202881\n",
      "epoch 4 batch 2747 loss: 2.3194398880004883\n",
      "epoch 4 batch 2748 loss: 2.154104471206665\n",
      "epoch 4 batch 2749 loss: 2.1135854721069336\n",
      "epoch 4 batch 2750 loss: 2.330306053161621\n",
      "epoch 4 batch 2751 loss: 2.0657553672790527\n",
      "epoch 4 batch 2752 loss: 2.322599411010742\n",
      "epoch 4 batch 2753 loss: 2.298950672149658\n",
      "epoch 4 batch 2754 loss: 2.3230743408203125\n",
      "epoch 4 batch 2755 loss: 2.2637128829956055\n",
      "epoch 4 batch 2756 loss: 2.245821475982666\n",
      "epoch 4 batch 2757 loss: 2.034669876098633\n",
      "epoch 4 batch 2758 loss: 2.2336723804473877\n",
      "epoch 4 batch 2759 loss: 2.28377628326416\n",
      "epoch 4 batch 2760 loss: 2.09586238861084\n",
      "epoch 4 batch 2761 loss: 2.437359094619751\n",
      "epoch 4 batch 2762 loss: 2.464930534362793\n",
      "epoch 4 batch 2763 loss: 2.1690833568573\n",
      "epoch 4 batch 2764 loss: 2.4471728801727295\n",
      "epoch 4 batch 2765 loss: 2.1764445304870605\n",
      "epoch 4 batch 2766 loss: 2.1408259868621826\n",
      "epoch 4 batch 2767 loss: 2.2943639755249023\n",
      "epoch 4 batch 2768 loss: 2.1595606803894043\n",
      "epoch 4 batch 2769 loss: 2.3032758235931396\n",
      "epoch 4 batch 2770 loss: 2.316242218017578\n",
      "epoch 4 batch 2771 loss: 2.2631819248199463\n",
      "epoch 4 batch 2772 loss: 2.4189653396606445\n",
      "epoch 4 batch 2773 loss: 2.3111729621887207\n",
      "epoch 4 batch 2774 loss: 2.2967453002929688\n",
      "epoch 4 batch 2775 loss: 1.9841941595077515\n",
      "epoch 4 batch 2776 loss: 2.464886426925659\n",
      "epoch 4 batch 2777 loss: 2.3632307052612305\n",
      "epoch 4 batch 2778 loss: 2.596144199371338\n",
      "epoch 4 batch 2779 loss: 2.2066409587860107\n",
      "epoch 4 batch 2780 loss: 2.403933048248291\n",
      "epoch 4 batch 2781 loss: 2.455237627029419\n",
      "epoch 4 batch 2782 loss: 2.3553643226623535\n",
      "epoch 4 batch 2783 loss: 2.183502674102783\n",
      "epoch 4 batch 2784 loss: 2.1734819412231445\n",
      "epoch 4 batch 2785 loss: 2.525336742401123\n",
      "epoch 4 batch 2786 loss: 2.2218494415283203\n",
      "epoch 4 batch 2787 loss: 2.4586739540100098\n",
      "epoch 4 batch 2788 loss: 2.228635311126709\n",
      "epoch 4 batch 2789 loss: 2.4631381034851074\n",
      "epoch 4 batch 2790 loss: 2.4051549434661865\n",
      "epoch 4 batch 2791 loss: 2.4392197132110596\n",
      "epoch 4 batch 2792 loss: 2.3981480598449707\n",
      "epoch 4 batch 2793 loss: 2.3813695907592773\n",
      "epoch 4 batch 2794 loss: 2.216844081878662\n",
      "epoch 4 batch 2795 loss: 2.5006606578826904\n",
      "epoch 4 batch 2796 loss: 2.321810007095337\n",
      "epoch 4 batch 2797 loss: 2.2987704277038574\n",
      "epoch 4 batch 2798 loss: 2.273038864135742\n",
      "epoch 4 batch 2799 loss: 2.2654497623443604\n",
      "epoch 4 batch 2800 loss: 2.3703551292419434\n",
      "epoch 4 batch 2801 loss: 2.2691903114318848\n",
      "epoch 4 batch 2802 loss: 2.5546302795410156\n",
      "epoch 4 batch 2803 loss: 2.2734134197235107\n",
      "epoch 4 batch 2804 loss: 2.417776584625244\n",
      "epoch 4 batch 2805 loss: 2.263213872909546\n",
      "epoch 4 batch 2806 loss: 2.242579936981201\n",
      "epoch 4 batch 2807 loss: 2.165579319000244\n",
      "epoch 4 batch 2808 loss: 2.2835025787353516\n",
      "epoch 4 batch 2809 loss: 2.127915859222412\n",
      "epoch 4 batch 2810 loss: 2.207066059112549\n",
      "epoch 4 batch 2811 loss: 2.2246508598327637\n",
      "epoch 4 batch 2812 loss: 2.162245750427246\n",
      "epoch 4 batch 2813 loss: 2.3978734016418457\n",
      "epoch 4 batch 2814 loss: 2.3192830085754395\n",
      "epoch 4 batch 2815 loss: 2.42887806892395\n",
      "epoch 4 batch 2816 loss: 2.0887327194213867\n",
      "epoch 4 batch 2817 loss: 2.529895067214966\n",
      "epoch 4 batch 2818 loss: 2.2824182510375977\n",
      "epoch 4 batch 2819 loss: 2.30975079536438\n",
      "epoch 4 batch 2820 loss: 2.2927486896514893\n",
      "epoch 4 batch 2821 loss: 2.2903518676757812\n",
      "epoch 4 batch 2822 loss: 2.4325695037841797\n",
      "epoch 4 batch 2823 loss: 2.2714791297912598\n",
      "epoch 4 batch 2824 loss: 2.41047739982605\n",
      "epoch 4 batch 2825 loss: 1.9808058738708496\n",
      "epoch 4 batch 2826 loss: 2.140774726867676\n",
      "epoch 4 batch 2827 loss: 2.147134780883789\n",
      "epoch 4 batch 2828 loss: 2.0769121646881104\n",
      "epoch 4 batch 2829 loss: 2.2002081871032715\n",
      "epoch 4 batch 2830 loss: 2.351393222808838\n",
      "epoch 4 batch 2831 loss: 2.471245288848877\n",
      "epoch 4 batch 2832 loss: 2.351681709289551\n",
      "epoch 4 batch 2833 loss: 2.5032262802124023\n",
      "epoch 4 batch 2834 loss: 2.5319790840148926\n",
      "epoch 4 batch 2835 loss: 2.298521041870117\n",
      "epoch 4 batch 2836 loss: 2.555774211883545\n",
      "epoch 4 batch 2837 loss: 2.250927448272705\n",
      "epoch 4 batch 2838 loss: 2.3637990951538086\n",
      "epoch 4 batch 2839 loss: 2.0808022022247314\n",
      "epoch 4 batch 2840 loss: 2.250175952911377\n",
      "epoch 4 batch 2841 loss: 2.413355827331543\n",
      "epoch 4 batch 2842 loss: 2.2948079109191895\n",
      "epoch 4 batch 2843 loss: 2.235410213470459\n",
      "epoch 4 batch 2844 loss: 2.2913482189178467\n",
      "epoch 4 batch 2845 loss: 2.1788125038146973\n",
      "epoch 4 batch 2846 loss: 2.3563787937164307\n",
      "epoch 4 batch 2847 loss: 2.3228094577789307\n",
      "epoch 4 batch 2848 loss: 2.395879030227661\n",
      "epoch 4 batch 2849 loss: 2.5766279697418213\n",
      "epoch 4 batch 2850 loss: 2.3738508224487305\n",
      "epoch 4 batch 2851 loss: 2.181657314300537\n",
      "epoch 4 batch 2852 loss: 2.5246949195861816\n",
      "epoch 4 batch 2853 loss: 2.350053310394287\n",
      "epoch 4 batch 2854 loss: 2.279357433319092\n",
      "epoch 4 batch 2855 loss: 2.2742552757263184\n",
      "epoch 4 batch 2856 loss: 2.214691400527954\n",
      "epoch 4 batch 2857 loss: 2.4167070388793945\n",
      "epoch 4 batch 2858 loss: 2.175774574279785\n",
      "epoch 4 batch 2859 loss: 2.5602407455444336\n",
      "epoch 4 batch 2860 loss: 2.087855100631714\n",
      "epoch 4 batch 2861 loss: 2.281184196472168\n",
      "epoch 4 batch 2862 loss: 2.292769432067871\n",
      "epoch 4 batch 2863 loss: 2.431293487548828\n",
      "epoch 4 batch 2864 loss: 2.4237866401672363\n",
      "epoch 4 batch 2865 loss: 2.4338061809539795\n",
      "epoch 4 batch 2866 loss: 2.2375311851501465\n",
      "epoch 4 batch 2867 loss: 2.2173824310302734\n",
      "epoch 4 batch 2868 loss: 2.2614405155181885\n",
      "epoch 4 batch 2869 loss: 2.349724292755127\n",
      "epoch 4 batch 2870 loss: 2.4186394214630127\n",
      "epoch 4 batch 2871 loss: 2.5343356132507324\n",
      "epoch 4 batch 2872 loss: 2.4311234951019287\n",
      "epoch 4 batch 2873 loss: 2.1373958587646484\n",
      "epoch 4 batch 2874 loss: 2.2017602920532227\n",
      "epoch 4 batch 2875 loss: 2.2043495178222656\n",
      "epoch 4 batch 2876 loss: 2.801760673522949\n",
      "epoch 4 batch 2877 loss: 2.1931684017181396\n",
      "epoch 4 batch 2878 loss: 2.111194372177124\n",
      "epoch 4 batch 2879 loss: 2.44205379486084\n",
      "epoch 4 batch 2880 loss: 2.3773298263549805\n",
      "epoch 4 batch 2881 loss: 2.167673110961914\n",
      "epoch 4 batch 2882 loss: 2.324212074279785\n",
      "epoch 4 batch 2883 loss: 2.0543947219848633\n",
      "epoch 4 batch 2884 loss: 2.2130627632141113\n",
      "epoch 4 batch 2885 loss: 2.428730010986328\n",
      "epoch 4 batch 2886 loss: 2.230802536010742\n",
      "epoch 4 batch 2887 loss: 2.3192543983459473\n",
      "epoch 4 batch 2888 loss: 2.3772201538085938\n",
      "epoch 4 batch 2889 loss: 2.30543851852417\n",
      "epoch 4 batch 2890 loss: 2.3091320991516113\n",
      "epoch 4 batch 2891 loss: 2.4595489501953125\n",
      "epoch 4 batch 2892 loss: 2.5516185760498047\n",
      "epoch 4 batch 2893 loss: 2.4890522956848145\n",
      "epoch 4 batch 2894 loss: 2.145641326904297\n",
      "epoch 4 batch 2895 loss: 2.3247528076171875\n",
      "epoch 4 batch 2896 loss: 2.191281795501709\n",
      "epoch 4 batch 2897 loss: 2.5292699337005615\n",
      "epoch 4 batch 2898 loss: 2.200295925140381\n",
      "epoch 4 batch 2899 loss: 2.419663429260254\n",
      "epoch 4 batch 2900 loss: 2.381028652191162\n",
      "epoch 4 batch 2901 loss: 2.3701701164245605\n",
      "epoch 4 batch 2902 loss: 2.438847541809082\n",
      "epoch 4 batch 2903 loss: 2.3000481128692627\n",
      "epoch 4 batch 2904 loss: 2.381779909133911\n",
      "epoch 4 batch 2905 loss: 2.1569204330444336\n",
      "epoch 4 batch 2906 loss: 2.197281837463379\n",
      "epoch 4 batch 2907 loss: 2.121558666229248\n",
      "epoch 4 batch 2908 loss: 2.22275447845459\n",
      "epoch 4 batch 2909 loss: 2.1534481048583984\n",
      "epoch 4 batch 2910 loss: 2.0213048458099365\n",
      "epoch 4 batch 2911 loss: 2.153972625732422\n",
      "epoch 4 batch 2912 loss: 2.6007986068725586\n",
      "epoch 4 batch 2913 loss: 2.265167474746704\n",
      "epoch 4 batch 2914 loss: 2.449866533279419\n",
      "epoch 4 batch 2915 loss: 2.2873785495758057\n",
      "epoch 4 batch 2916 loss: 2.337200880050659\n",
      "epoch 4 batch 2917 loss: 2.321173667907715\n",
      "epoch 4 batch 2918 loss: 2.3106820583343506\n",
      "epoch 4 batch 2919 loss: 2.594696044921875\n",
      "epoch 4 batch 2920 loss: 2.4790005683898926\n",
      "epoch 4 batch 2921 loss: 2.695478916168213\n",
      "epoch 4 batch 2922 loss: 2.382796287536621\n",
      "epoch 4 batch 2923 loss: 2.2545666694641113\n",
      "epoch 4 batch 2924 loss: 2.3455333709716797\n",
      "epoch 4 batch 2925 loss: 2.094935655593872\n",
      "epoch 4 batch 2926 loss: 2.579909563064575\n",
      "epoch 4 batch 2927 loss: 2.349501609802246\n",
      "epoch 4 batch 2928 loss: 2.347344398498535\n",
      "epoch 4 batch 2929 loss: 2.40610408782959\n",
      "epoch 4 batch 2930 loss: 2.2373714447021484\n",
      "epoch 4 batch 2931 loss: 2.50849986076355\n",
      "epoch 4 batch 2932 loss: 2.4420104026794434\n",
      "epoch 4 batch 2933 loss: 2.4883406162261963\n",
      "epoch 4 batch 2934 loss: 2.1982083320617676\n",
      "epoch 4 batch 2935 loss: 2.342883348464966\n",
      "epoch 4 batch 2936 loss: 2.1847023963928223\n",
      "epoch 4 batch 2937 loss: 2.3800625801086426\n",
      "epoch 4 batch 2938 loss: 2.393155336380005\n",
      "epoch 4 batch 2939 loss: 2.4347848892211914\n",
      "epoch 4 batch 2940 loss: 2.4163308143615723\n",
      "epoch 4 batch 2941 loss: 2.352111339569092\n",
      "epoch 4 batch 2942 loss: 2.3076727390289307\n",
      "epoch 4 batch 2943 loss: 2.670387029647827\n",
      "epoch 4 batch 2944 loss: 2.401397943496704\n",
      "epoch 4 batch 2945 loss: 2.296816825866699\n",
      "epoch 4 batch 2946 loss: 2.2174253463745117\n",
      "epoch 4 batch 2947 loss: 2.4448399543762207\n",
      "epoch 4 batch 2948 loss: 2.2158570289611816\n",
      "epoch 4 batch 2949 loss: 2.182305097579956\n",
      "epoch 4 batch 2950 loss: 2.4212608337402344\n",
      "epoch 4 batch 2951 loss: 2.1968469619750977\n",
      "epoch 4 batch 2952 loss: 2.5666422843933105\n",
      "epoch 4 batch 2953 loss: 2.195467948913574\n",
      "epoch 4 batch 2954 loss: 2.2707557678222656\n",
      "epoch 4 batch 2955 loss: 2.1777005195617676\n",
      "epoch 4 batch 2956 loss: 2.5370306968688965\n",
      "epoch 4 batch 2957 loss: 2.3823928833007812\n",
      "epoch 4 batch 2958 loss: 2.397235155105591\n",
      "epoch 4 batch 2959 loss: 2.4296059608459473\n",
      "epoch 4 batch 2960 loss: 2.2451834678649902\n",
      "epoch 4 batch 2961 loss: 2.4088518619537354\n",
      "epoch 4 batch 2962 loss: 2.283623695373535\n",
      "epoch 4 batch 2963 loss: 2.251627206802368\n",
      "epoch 4 batch 2964 loss: 2.372776508331299\n",
      "epoch 4 batch 2965 loss: 2.3741519451141357\n",
      "epoch 4 batch 2966 loss: 2.444118022918701\n",
      "epoch 4 batch 2967 loss: 2.3011345863342285\n",
      "epoch 4 batch 2968 loss: 2.334167003631592\n",
      "epoch 4 batch 2969 loss: 2.3755781650543213\n",
      "epoch 4 batch 2970 loss: 2.521134614944458\n",
      "epoch 4 batch 2971 loss: 2.319669723510742\n",
      "epoch 4 batch 2972 loss: 2.297947883605957\n",
      "epoch 4 batch 2973 loss: 2.213623523712158\n",
      "epoch 4 batch 2974 loss: 2.862600088119507\n",
      "epoch 4 batch 2975 loss: 2.3923933506011963\n",
      "epoch 4 batch 2976 loss: 2.2344179153442383\n",
      "epoch 4 batch 2977 loss: 2.415985584259033\n",
      "epoch 4 batch 2978 loss: 2.2600417137145996\n",
      "epoch 4 batch 2979 loss: 2.4013354778289795\n",
      "epoch 4 batch 2980 loss: 2.2835307121276855\n",
      "epoch 4 batch 2981 loss: 2.2820777893066406\n",
      "epoch 4 batch 2982 loss: 2.410419225692749\n",
      "epoch 4 batch 2983 loss: 2.2329659461975098\n",
      "epoch 4 batch 2984 loss: 2.1410677433013916\n",
      "epoch 4 batch 2985 loss: 2.551629066467285\n",
      "epoch 4 batch 2986 loss: 2.1574485301971436\n",
      "epoch 4 batch 2987 loss: 2.3695716857910156\n",
      "epoch 4 batch 2988 loss: 2.523716926574707\n",
      "epoch 4 batch 2989 loss: 2.358057737350464\n",
      "epoch 4 batch 2990 loss: 2.2673444747924805\n",
      "epoch 4 batch 2991 loss: 2.163407802581787\n",
      "epoch 4 batch 2992 loss: 2.379060983657837\n",
      "epoch 4 batch 2993 loss: 2.193429470062256\n",
      "epoch 4 batch 2994 loss: 2.334689140319824\n",
      "epoch 4 batch 2995 loss: 2.7844221591949463\n",
      "epoch 4 batch 2996 loss: 2.3290517330169678\n",
      "epoch 4 batch 2997 loss: 2.6410326957702637\n",
      "epoch 4 batch 2998 loss: 2.2804131507873535\n",
      "epoch 4 batch 2999 loss: 2.1102609634399414\n",
      "epoch 4 batch 3000 loss: 2.2523608207702637\n",
      "epoch 4 batch 3001 loss: 2.1155526638031006\n",
      "epoch 4 batch 3002 loss: 2.3676929473876953\n",
      "epoch 4 batch 3003 loss: 2.2205514907836914\n",
      "epoch 4 batch 3004 loss: 2.433021306991577\n",
      "epoch 4 batch 3005 loss: 2.5693812370300293\n",
      "epoch 4 batch 3006 loss: 2.3342390060424805\n",
      "epoch 4 batch 3007 loss: 2.125519037246704\n",
      "epoch 4 batch 3008 loss: 2.4736509323120117\n",
      "epoch 4 batch 3009 loss: 2.4830679893493652\n",
      "epoch 4 batch 3010 loss: 2.2023706436157227\n",
      "epoch 4 batch 3011 loss: 2.218254327774048\n",
      "epoch 4 batch 3012 loss: 2.5062336921691895\n",
      "epoch 4 batch 3013 loss: 2.1565496921539307\n",
      "epoch 4 batch 3014 loss: 2.378890037536621\n",
      "epoch 4 batch 3015 loss: 2.2340822219848633\n",
      "epoch 4 batch 3016 loss: 2.3420753479003906\n",
      "epoch 4 batch 3017 loss: 2.29840087890625\n",
      "epoch 4 batch 3018 loss: 2.1697754859924316\n",
      "epoch 4 batch 3019 loss: 2.329770088195801\n",
      "epoch 4 batch 3020 loss: 2.5685529708862305\n",
      "epoch 4 batch 3021 loss: 2.2351808547973633\n",
      "epoch 4 batch 3022 loss: 2.3304362297058105\n",
      "epoch 4 batch 3023 loss: 2.270406723022461\n",
      "epoch 4 batch 3024 loss: 2.2202260494232178\n",
      "epoch 4 batch 3025 loss: 2.600587844848633\n",
      "epoch 4 batch 3026 loss: 2.3611485958099365\n",
      "epoch 4 batch 3027 loss: 2.434998035430908\n",
      "epoch 4 batch 3028 loss: 2.4505057334899902\n",
      "epoch 4 batch 3029 loss: 2.3418221473693848\n",
      "epoch 4 batch 3030 loss: 2.0497794151306152\n",
      "epoch 4 batch 3031 loss: 2.247389316558838\n",
      "epoch 4 batch 3032 loss: 2.3358466625213623\n",
      "epoch 4 batch 3033 loss: 2.55922269821167\n",
      "epoch 4 batch 3034 loss: 2.353120803833008\n",
      "epoch 4 batch 3035 loss: 2.4283907413482666\n",
      "epoch 4 batch 3036 loss: 2.3997554779052734\n",
      "epoch 4 batch 3037 loss: 2.4554338455200195\n",
      "epoch 4 batch 3038 loss: 2.2207937240600586\n",
      "epoch 4 batch 3039 loss: 2.1544699668884277\n",
      "epoch 4 batch 3040 loss: 2.1832237243652344\n",
      "epoch 4 batch 3041 loss: 2.483584403991699\n",
      "epoch 4 batch 3042 loss: 2.3826394081115723\n",
      "epoch 4 batch 3043 loss: 2.32002329826355\n",
      "epoch 4 batch 3044 loss: 2.4364328384399414\n",
      "epoch 4 batch 3045 loss: 2.129542350769043\n",
      "epoch 4 batch 3046 loss: 2.470881700515747\n",
      "epoch 4 batch 3047 loss: 2.313185214996338\n",
      "epoch 4 batch 3048 loss: 2.5187082290649414\n",
      "epoch 4 batch 3049 loss: 2.2019052505493164\n",
      "epoch 4 batch 3050 loss: 2.371541976928711\n",
      "epoch 4 batch 3051 loss: 2.3750951290130615\n",
      "epoch 4 batch 3052 loss: 2.1219770908355713\n",
      "epoch 4 batch 3053 loss: 2.2333779335021973\n",
      "epoch 4 batch 3054 loss: 2.380849599838257\n",
      "epoch 4 batch 3055 loss: 2.390749216079712\n",
      "epoch 4 batch 3056 loss: 2.435084342956543\n",
      "epoch 4 batch 3057 loss: 2.4309511184692383\n",
      "epoch 4 batch 3058 loss: 2.2282657623291016\n",
      "epoch 4 batch 3059 loss: 2.3479785919189453\n",
      "epoch 4 batch 3060 loss: 2.2086703777313232\n",
      "epoch 4 batch 3061 loss: 2.2688822746276855\n",
      "epoch 4 batch 3062 loss: 2.4914212226867676\n",
      "epoch 4 batch 3063 loss: 2.239759922027588\n",
      "epoch 4 batch 3064 loss: 2.267132043838501\n",
      "epoch 4 batch 3065 loss: 2.1659889221191406\n",
      "epoch 4 batch 3066 loss: 2.1558637619018555\n",
      "epoch 4 batch 3067 loss: 2.4280550479888916\n",
      "epoch 4 batch 3068 loss: 2.1627111434936523\n",
      "epoch 4 batch 3069 loss: 2.184196710586548\n",
      "epoch 4 batch 3070 loss: 2.3029074668884277\n",
      "epoch 4 batch 3071 loss: 2.4433083534240723\n",
      "epoch 4 batch 3072 loss: 2.4057536125183105\n",
      "epoch 4 batch 3073 loss: 2.204282283782959\n",
      "epoch 4 batch 3074 loss: 2.279041290283203\n",
      "epoch 4 batch 3075 loss: 2.1872243881225586\n",
      "epoch 4 batch 3076 loss: 2.2601170539855957\n",
      "epoch 4 batch 3077 loss: 2.3695268630981445\n",
      "epoch 4 batch 3078 loss: 2.3826537132263184\n",
      "epoch 4 batch 3079 loss: 2.204007148742676\n",
      "epoch 4 batch 3080 loss: 2.3365318775177\n",
      "epoch 4 batch 3081 loss: 2.479024887084961\n",
      "epoch 4 batch 3082 loss: 2.303772449493408\n",
      "epoch 4 batch 3083 loss: 2.1649017333984375\n",
      "epoch 4 batch 3084 loss: 2.291597843170166\n",
      "epoch 4 batch 3085 loss: 2.283966541290283\n",
      "epoch 4 batch 3086 loss: 2.5609734058380127\n",
      "epoch 4 batch 3087 loss: 2.3688464164733887\n",
      "epoch 4 batch 3088 loss: 2.078026294708252\n",
      "epoch 4 batch 3089 loss: 2.1597018241882324\n",
      "epoch 4 batch 3090 loss: 2.488028049468994\n",
      "epoch 4 batch 3091 loss: 2.3377037048339844\n",
      "epoch 4 batch 3092 loss: 2.2073118686676025\n",
      "epoch 4 batch 3093 loss: 2.486694097518921\n",
      "epoch 4 batch 3094 loss: 2.4788601398468018\n",
      "epoch 4 batch 3095 loss: 2.643221378326416\n",
      "epoch 4 batch 3096 loss: 2.1705994606018066\n",
      "epoch 4 batch 3097 loss: 2.1792445182800293\n",
      "epoch 4 batch 3098 loss: 2.410902738571167\n",
      "epoch 4 batch 3099 loss: 2.411738395690918\n",
      "epoch 4 batch 3100 loss: 2.4164137840270996\n",
      "epoch 4 batch 3101 loss: 2.3472700119018555\n",
      "epoch 4 batch 3102 loss: 2.0582263469696045\n",
      "epoch 4 batch 3103 loss: 2.4822475910186768\n",
      "epoch 4 batch 3104 loss: 2.518303394317627\n",
      "epoch 4 batch 3105 loss: 2.6322288513183594\n",
      "epoch 4 batch 3106 loss: 2.1666831970214844\n",
      "epoch 4 batch 3107 loss: 2.193375825881958\n",
      "epoch 4 batch 3108 loss: 2.3366124629974365\n",
      "epoch 4 batch 3109 loss: 2.3917391300201416\n",
      "epoch 4 batch 3110 loss: 2.3179454803466797\n",
      "epoch 4 batch 3111 loss: 2.179255247116089\n",
      "epoch 4 batch 3112 loss: 2.2823331356048584\n",
      "epoch 4 batch 3113 loss: 2.416621685028076\n",
      "epoch 4 batch 3114 loss: 2.348090171813965\n",
      "epoch 4 batch 3115 loss: 2.3471968173980713\n",
      "epoch 4 batch 3116 loss: 2.2881996631622314\n",
      "epoch 4 batch 3117 loss: 2.2481560707092285\n",
      "epoch 4 batch 3118 loss: 2.087559461593628\n",
      "epoch 4 batch 3119 loss: 2.3760876655578613\n",
      "epoch 4 batch 3120 loss: 2.3316593170166016\n",
      "epoch 4 batch 3121 loss: 2.6461119651794434\n",
      "epoch 4 batch 3122 loss: 2.228361129760742\n",
      "epoch 4 batch 3123 loss: 2.186469316482544\n",
      "epoch 4 batch 3124 loss: 2.193596363067627\n",
      "epoch loss: 2.3413851940917967\n",
      "epoch 5 batch 0 loss: 2.0471978187561035\n",
      "epoch 5 batch 1 loss: 2.34210205078125\n",
      "epoch 5 batch 2 loss: 2.35564923286438\n",
      "epoch 5 batch 3 loss: 2.166341781616211\n",
      "epoch 5 batch 4 loss: 2.386584520339966\n",
      "epoch 5 batch 5 loss: 2.259554624557495\n",
      "epoch 5 batch 6 loss: 2.2011842727661133\n",
      "epoch 5 batch 7 loss: 2.3476040363311768\n",
      "epoch 5 batch 8 loss: 2.4594244956970215\n",
      "epoch 5 batch 9 loss: 2.2766008377075195\n",
      "epoch 5 batch 10 loss: 2.2081193923950195\n",
      "epoch 5 batch 11 loss: 2.3537657260894775\n",
      "epoch 5 batch 12 loss: 2.4628007411956787\n",
      "epoch 5 batch 13 loss: 2.2181365489959717\n",
      "epoch 5 batch 14 loss: 2.3541460037231445\n",
      "epoch 5 batch 15 loss: 2.2936744689941406\n",
      "epoch 5 batch 16 loss: 2.2802207469940186\n",
      "epoch 5 batch 17 loss: 2.3985157012939453\n",
      "epoch 5 batch 18 loss: 2.0926613807678223\n",
      "epoch 5 batch 19 loss: 2.6115148067474365\n",
      "epoch 5 batch 20 loss: 2.5033023357391357\n",
      "epoch 5 batch 21 loss: 2.177304744720459\n",
      "epoch 5 batch 22 loss: 2.262786388397217\n",
      "epoch 5 batch 23 loss: 2.40372371673584\n",
      "epoch 5 batch 24 loss: 2.1030428409576416\n",
      "epoch 5 batch 25 loss: 2.0840797424316406\n",
      "epoch 5 batch 26 loss: 2.450533628463745\n",
      "epoch 5 batch 27 loss: 2.3514132499694824\n",
      "epoch 5 batch 28 loss: 2.3118762969970703\n",
      "epoch 5 batch 29 loss: 2.2393105030059814\n",
      "epoch 5 batch 30 loss: 2.084505796432495\n",
      "epoch 5 batch 31 loss: 2.2381937503814697\n",
      "epoch 5 batch 32 loss: 2.275998830795288\n",
      "epoch 5 batch 33 loss: 2.3272929191589355\n",
      "epoch 5 batch 34 loss: 2.277480363845825\n",
      "epoch 5 batch 35 loss: 2.20383358001709\n",
      "epoch 5 batch 36 loss: 2.3306665420532227\n",
      "epoch 5 batch 37 loss: 2.340912342071533\n",
      "epoch 5 batch 38 loss: 2.415019989013672\n",
      "epoch 5 batch 39 loss: 2.2438182830810547\n",
      "epoch 5 batch 40 loss: 2.111128568649292\n",
      "epoch 5 batch 41 loss: 2.4503283500671387\n",
      "epoch 5 batch 42 loss: 2.0713906288146973\n",
      "epoch 5 batch 43 loss: 2.067850112915039\n",
      "epoch 5 batch 44 loss: 2.522984504699707\n",
      "epoch 5 batch 45 loss: 2.2133796215057373\n",
      "epoch 5 batch 46 loss: 2.3897342681884766\n",
      "epoch 5 batch 47 loss: 2.181149959564209\n",
      "epoch 5 batch 48 loss: 2.558407783508301\n",
      "epoch 5 batch 49 loss: 2.3480215072631836\n",
      "epoch 5 batch 50 loss: 2.3692588806152344\n",
      "epoch 5 batch 51 loss: 2.221940279006958\n",
      "epoch 5 batch 52 loss: 2.3181982040405273\n",
      "epoch 5 batch 53 loss: 2.3157851696014404\n",
      "epoch 5 batch 54 loss: 2.077892780303955\n",
      "epoch 5 batch 55 loss: 2.2638864517211914\n",
      "epoch 5 batch 56 loss: 2.6809940338134766\n",
      "epoch 5 batch 57 loss: 2.1789255142211914\n",
      "epoch 5 batch 58 loss: 2.2668962478637695\n",
      "epoch 5 batch 59 loss: 2.3551692962646484\n",
      "epoch 5 batch 60 loss: 2.057840347290039\n",
      "epoch 5 batch 61 loss: 2.5470752716064453\n",
      "epoch 5 batch 62 loss: 2.1913557052612305\n",
      "epoch 5 batch 63 loss: 2.273486852645874\n",
      "epoch 5 batch 64 loss: 2.2415597438812256\n",
      "epoch 5 batch 65 loss: 2.6609506607055664\n",
      "epoch 5 batch 66 loss: 2.4815902709960938\n",
      "epoch 5 batch 67 loss: 2.3338656425476074\n",
      "epoch 5 batch 68 loss: 2.3768908977508545\n",
      "epoch 5 batch 69 loss: 2.3908016681671143\n",
      "epoch 5 batch 70 loss: 2.2078561782836914\n",
      "epoch 5 batch 71 loss: 2.334352493286133\n",
      "epoch 5 batch 72 loss: 2.199896812438965\n",
      "epoch 5 batch 73 loss: 2.426565647125244\n",
      "epoch 5 batch 74 loss: 2.3117284774780273\n",
      "epoch 5 batch 75 loss: 2.0045690536499023\n",
      "epoch 5 batch 76 loss: 2.275729179382324\n",
      "epoch 5 batch 77 loss: 2.186828136444092\n",
      "epoch 5 batch 78 loss: 2.4530906677246094\n",
      "epoch 5 batch 79 loss: 2.423205852508545\n",
      "epoch 5 batch 80 loss: 2.3696155548095703\n",
      "epoch 5 batch 81 loss: 2.229775905609131\n",
      "epoch 5 batch 82 loss: 2.2332189083099365\n",
      "epoch 5 batch 83 loss: 2.543738842010498\n",
      "epoch 5 batch 84 loss: 2.239304542541504\n",
      "epoch 5 batch 85 loss: 2.265544891357422\n",
      "epoch 5 batch 86 loss: 2.413209915161133\n",
      "epoch 5 batch 87 loss: 2.144054889678955\n",
      "epoch 5 batch 88 loss: 2.4313716888427734\n",
      "epoch 5 batch 89 loss: 2.3216452598571777\n",
      "epoch 5 batch 90 loss: 2.388211250305176\n",
      "epoch 5 batch 91 loss: 2.4341423511505127\n",
      "epoch 5 batch 92 loss: 2.4651434421539307\n",
      "epoch 5 batch 93 loss: 2.3236939907073975\n",
      "epoch 5 batch 94 loss: 2.2173829078674316\n",
      "epoch 5 batch 95 loss: 2.4679622650146484\n",
      "epoch 5 batch 96 loss: 2.193082809448242\n",
      "epoch 5 batch 97 loss: 2.228435516357422\n",
      "epoch 5 batch 98 loss: 2.1918725967407227\n",
      "epoch 5 batch 99 loss: 2.266174793243408\n",
      "epoch 5 batch 100 loss: 2.3418221473693848\n",
      "epoch 5 batch 101 loss: 2.317375659942627\n",
      "epoch 5 batch 102 loss: 2.062948226928711\n",
      "epoch 5 batch 103 loss: 2.3693134784698486\n",
      "epoch 5 batch 104 loss: 2.4153618812561035\n",
      "epoch 5 batch 105 loss: 2.3789896965026855\n",
      "epoch 5 batch 106 loss: 2.174520492553711\n",
      "epoch 5 batch 107 loss: 2.0241599082946777\n",
      "epoch 5 batch 108 loss: 2.4078707695007324\n",
      "epoch 5 batch 109 loss: 2.026247024536133\n",
      "epoch 5 batch 110 loss: 2.3031609058380127\n",
      "epoch 5 batch 111 loss: 2.3088297843933105\n",
      "epoch 5 batch 112 loss: 2.473803997039795\n",
      "epoch 5 batch 113 loss: 2.314547061920166\n",
      "epoch 5 batch 114 loss: 2.4470245838165283\n",
      "epoch 5 batch 115 loss: 2.3551080226898193\n",
      "epoch 5 batch 116 loss: 2.27535343170166\n",
      "epoch 5 batch 117 loss: 2.0597357749938965\n",
      "epoch 5 batch 118 loss: 2.3009395599365234\n",
      "epoch 5 batch 119 loss: 2.3133647441864014\n",
      "epoch 5 batch 120 loss: 2.3583433628082275\n",
      "epoch 5 batch 121 loss: 2.187451124191284\n",
      "epoch 5 batch 122 loss: 2.1548309326171875\n",
      "epoch 5 batch 123 loss: 2.4423937797546387\n",
      "epoch 5 batch 124 loss: 2.448580265045166\n",
      "epoch 5 batch 125 loss: 2.172837734222412\n",
      "epoch 5 batch 126 loss: 2.2999327182769775\n",
      "epoch 5 batch 127 loss: 2.1678285598754883\n",
      "epoch 5 batch 128 loss: 2.2112488746643066\n",
      "epoch 5 batch 129 loss: 2.4518706798553467\n",
      "epoch 5 batch 130 loss: 2.6113646030426025\n",
      "epoch 5 batch 131 loss: 2.2741575241088867\n",
      "epoch 5 batch 132 loss: 2.400071144104004\n",
      "epoch 5 batch 133 loss: 2.1295783519744873\n",
      "epoch 5 batch 134 loss: 2.542623996734619\n",
      "epoch 5 batch 135 loss: 2.172478199005127\n",
      "epoch 5 batch 136 loss: 2.2958855628967285\n",
      "epoch 5 batch 137 loss: 2.124419689178467\n",
      "epoch 5 batch 138 loss: 2.3012309074401855\n",
      "epoch 5 batch 139 loss: 2.4386024475097656\n",
      "epoch 5 batch 140 loss: 2.5022776126861572\n",
      "epoch 5 batch 141 loss: 2.4503679275512695\n",
      "epoch 5 batch 142 loss: 2.6073718070983887\n",
      "epoch 5 batch 143 loss: 2.0713748931884766\n",
      "epoch 5 batch 144 loss: 2.2374043464660645\n",
      "epoch 5 batch 145 loss: 2.3825278282165527\n",
      "epoch 5 batch 146 loss: 2.473386764526367\n",
      "epoch 5 batch 147 loss: 2.386107921600342\n",
      "epoch 5 batch 148 loss: 2.401698589324951\n",
      "epoch 5 batch 149 loss: 2.262103319168091\n",
      "epoch 5 batch 150 loss: 2.1579225063323975\n",
      "epoch 5 batch 151 loss: 2.416032552719116\n",
      "epoch 5 batch 152 loss: 2.4526467323303223\n",
      "epoch 5 batch 153 loss: 2.3570053577423096\n",
      "epoch 5 batch 154 loss: 2.232252836227417\n",
      "epoch 5 batch 155 loss: 2.0463180541992188\n",
      "epoch 5 batch 156 loss: 2.273169994354248\n",
      "epoch 5 batch 157 loss: 2.4284677505493164\n",
      "epoch 5 batch 158 loss: 2.507772445678711\n",
      "epoch 5 batch 159 loss: 2.390167713165283\n",
      "epoch 5 batch 160 loss: 2.336538076400757\n",
      "epoch 5 batch 161 loss: 2.217209815979004\n",
      "epoch 5 batch 162 loss: 2.404191017150879\n",
      "epoch 5 batch 163 loss: 2.3248934745788574\n",
      "epoch 5 batch 164 loss: 2.3622477054595947\n",
      "epoch 5 batch 165 loss: 2.387455940246582\n",
      "epoch 5 batch 166 loss: 2.127243995666504\n",
      "epoch 5 batch 167 loss: 2.365373134613037\n",
      "epoch 5 batch 168 loss: 2.2263898849487305\n",
      "epoch 5 batch 169 loss: 2.129019021987915\n",
      "epoch 5 batch 170 loss: 2.4340553283691406\n",
      "epoch 5 batch 171 loss: 2.318331718444824\n",
      "epoch 5 batch 172 loss: 2.274573802947998\n",
      "epoch 5 batch 173 loss: 2.1921987533569336\n",
      "epoch 5 batch 174 loss: 2.071871280670166\n",
      "epoch 5 batch 175 loss: 2.2121195793151855\n",
      "epoch 5 batch 176 loss: 2.4501514434814453\n",
      "epoch 5 batch 177 loss: 2.432987689971924\n",
      "epoch 5 batch 178 loss: 2.2839765548706055\n",
      "epoch 5 batch 179 loss: 2.235710859298706\n",
      "epoch 5 batch 180 loss: 2.2132887840270996\n",
      "epoch 5 batch 181 loss: 2.2106096744537354\n",
      "epoch 5 batch 182 loss: 2.4096555709838867\n",
      "epoch 5 batch 183 loss: 2.599470615386963\n",
      "epoch 5 batch 184 loss: 2.420567512512207\n",
      "epoch 5 batch 185 loss: 2.2688066959381104\n",
      "epoch 5 batch 186 loss: 2.1520028114318848\n",
      "epoch 5 batch 187 loss: 2.3379316329956055\n",
      "epoch 5 batch 188 loss: 2.518064498901367\n",
      "epoch 5 batch 189 loss: 2.3856186866760254\n",
      "epoch 5 batch 190 loss: 2.2098379135131836\n",
      "epoch 5 batch 191 loss: 2.249800205230713\n",
      "epoch 5 batch 192 loss: 2.2908132076263428\n",
      "epoch 5 batch 193 loss: 2.3255205154418945\n",
      "epoch 5 batch 194 loss: 2.2452969551086426\n",
      "epoch 5 batch 195 loss: 2.5023837089538574\n",
      "epoch 5 batch 196 loss: 2.086836338043213\n",
      "epoch 5 batch 197 loss: 2.1787779331207275\n",
      "epoch 5 batch 198 loss: 2.4303388595581055\n",
      "epoch 5 batch 199 loss: 2.220094919204712\n",
      "epoch 5 batch 200 loss: 2.507779359817505\n",
      "epoch 5 batch 201 loss: 2.1731269359588623\n",
      "epoch 5 batch 202 loss: 2.344602584838867\n",
      "epoch 5 batch 203 loss: 2.221137285232544\n",
      "epoch 5 batch 204 loss: 2.266249418258667\n",
      "epoch 5 batch 205 loss: 2.35750150680542\n",
      "epoch 5 batch 206 loss: 2.217723846435547\n",
      "epoch 5 batch 207 loss: 2.1135692596435547\n",
      "epoch 5 batch 208 loss: 2.1877260208129883\n",
      "epoch 5 batch 209 loss: 2.10786771774292\n",
      "epoch 5 batch 210 loss: 2.200458526611328\n",
      "epoch 5 batch 211 loss: 2.33284854888916\n",
      "epoch 5 batch 212 loss: 2.2411179542541504\n",
      "epoch 5 batch 213 loss: 2.3940863609313965\n",
      "epoch 5 batch 214 loss: 2.3824539184570312\n",
      "epoch 5 batch 215 loss: 2.1439337730407715\n",
      "epoch 5 batch 216 loss: 2.2670934200286865\n",
      "epoch 5 batch 217 loss: 2.2669053077697754\n",
      "epoch 5 batch 218 loss: 2.010575294494629\n",
      "epoch 5 batch 219 loss: 2.2202024459838867\n",
      "epoch 5 batch 220 loss: 2.1991114616394043\n",
      "epoch 5 batch 221 loss: 2.302736282348633\n",
      "epoch 5 batch 222 loss: 2.272217273712158\n",
      "epoch 5 batch 223 loss: 2.2500429153442383\n",
      "epoch 5 batch 224 loss: 2.32712721824646\n",
      "epoch 5 batch 225 loss: 2.2691574096679688\n",
      "epoch 5 batch 226 loss: 2.272307872772217\n",
      "epoch 5 batch 227 loss: 2.282036066055298\n",
      "epoch 5 batch 228 loss: 2.4031338691711426\n",
      "epoch 5 batch 229 loss: 2.184195041656494\n",
      "epoch 5 batch 230 loss: 2.463484764099121\n",
      "epoch 5 batch 231 loss: 2.420700788497925\n",
      "epoch 5 batch 232 loss: 2.2531747817993164\n",
      "epoch 5 batch 233 loss: 2.170130968093872\n",
      "epoch 5 batch 234 loss: 2.2224838733673096\n",
      "epoch 5 batch 235 loss: 2.295980930328369\n",
      "epoch 5 batch 236 loss: 2.3121910095214844\n",
      "epoch 5 batch 237 loss: 2.1765012741088867\n",
      "epoch 5 batch 238 loss: 2.3356289863586426\n",
      "epoch 5 batch 239 loss: 2.3134264945983887\n",
      "epoch 5 batch 240 loss: 2.291989326477051\n",
      "epoch 5 batch 241 loss: 2.283095598220825\n",
      "epoch 5 batch 242 loss: 2.578380584716797\n",
      "epoch 5 batch 243 loss: 2.289940357208252\n",
      "epoch 5 batch 244 loss: 2.4912028312683105\n",
      "epoch 5 batch 245 loss: 2.3464858531951904\n",
      "epoch 5 batch 246 loss: 2.2514967918395996\n",
      "epoch 5 batch 247 loss: 2.147091865539551\n",
      "epoch 5 batch 248 loss: 2.5923664569854736\n",
      "epoch 5 batch 249 loss: 2.4606661796569824\n",
      "epoch 5 batch 250 loss: 2.334886312484741\n",
      "epoch 5 batch 251 loss: 2.54058837890625\n",
      "epoch 5 batch 252 loss: 2.1332359313964844\n",
      "epoch 5 batch 253 loss: 2.095226526260376\n",
      "epoch 5 batch 254 loss: 2.0830159187316895\n",
      "epoch 5 batch 255 loss: 2.2111117839813232\n",
      "epoch 5 batch 256 loss: 2.0898900032043457\n",
      "epoch 5 batch 257 loss: 2.23384165763855\n",
      "epoch 5 batch 258 loss: 2.3363125324249268\n",
      "epoch 5 batch 259 loss: 2.265923261642456\n",
      "epoch 5 batch 260 loss: 2.340542793273926\n",
      "epoch 5 batch 261 loss: 2.1100046634674072\n",
      "epoch 5 batch 262 loss: 2.157724380493164\n",
      "epoch 5 batch 263 loss: 2.5354833602905273\n",
      "epoch 5 batch 264 loss: 2.230384349822998\n",
      "epoch 5 batch 265 loss: 2.4199631214141846\n",
      "epoch 5 batch 266 loss: 2.5688681602478027\n",
      "epoch 5 batch 267 loss: 2.4684085845947266\n",
      "epoch 5 batch 268 loss: 2.2282423973083496\n",
      "epoch 5 batch 269 loss: 2.3097095489501953\n",
      "epoch 5 batch 270 loss: 2.5400187969207764\n",
      "epoch 5 batch 271 loss: 2.271782875061035\n",
      "epoch 5 batch 272 loss: 2.2021498680114746\n",
      "epoch 5 batch 273 loss: 2.2621891498565674\n",
      "epoch 5 batch 274 loss: 2.447614908218384\n",
      "epoch 5 batch 275 loss: 2.3589553833007812\n",
      "epoch 5 batch 276 loss: 2.31715726852417\n",
      "epoch 5 batch 277 loss: 2.2081387042999268\n",
      "epoch 5 batch 278 loss: 2.1760783195495605\n",
      "epoch 5 batch 279 loss: 2.1558098793029785\n",
      "epoch 5 batch 280 loss: 2.142876625061035\n",
      "epoch 5 batch 281 loss: 2.339259147644043\n",
      "epoch 5 batch 282 loss: 2.3824782371520996\n",
      "epoch 5 batch 283 loss: 2.473823308944702\n",
      "epoch 5 batch 284 loss: 2.5739409923553467\n",
      "epoch 5 batch 285 loss: 2.282197952270508\n",
      "epoch 5 batch 286 loss: 2.2986817359924316\n",
      "epoch 5 batch 287 loss: 2.4073946475982666\n",
      "epoch 5 batch 288 loss: 2.3735742568969727\n",
      "epoch 5 batch 289 loss: 2.5164637565612793\n",
      "epoch 5 batch 290 loss: 2.1514720916748047\n",
      "epoch 5 batch 291 loss: 2.352419376373291\n",
      "epoch 5 batch 292 loss: 2.264960527420044\n",
      "epoch 5 batch 293 loss: 2.356794595718384\n",
      "epoch 5 batch 294 loss: 2.2687015533447266\n",
      "epoch 5 batch 295 loss: 2.3494575023651123\n",
      "epoch 5 batch 296 loss: 2.263010025024414\n",
      "epoch 5 batch 297 loss: 2.2433276176452637\n",
      "epoch 5 batch 298 loss: 2.3900656700134277\n",
      "epoch 5 batch 299 loss: 2.358067512512207\n",
      "epoch 5 batch 300 loss: 2.255237102508545\n",
      "epoch 5 batch 301 loss: 2.183443546295166\n",
      "epoch 5 batch 302 loss: 2.097660541534424\n",
      "epoch 5 batch 303 loss: 2.287137508392334\n",
      "epoch 5 batch 304 loss: 2.301501750946045\n",
      "epoch 5 batch 305 loss: 2.3275351524353027\n",
      "epoch 5 batch 306 loss: 2.274338722229004\n",
      "epoch 5 batch 307 loss: 2.408700704574585\n",
      "epoch 5 batch 308 loss: 2.7565090656280518\n",
      "epoch 5 batch 309 loss: 2.227726459503174\n",
      "epoch 5 batch 310 loss: 2.330988883972168\n",
      "epoch 5 batch 311 loss: 2.6910042762756348\n",
      "epoch 5 batch 312 loss: 2.2857186794281006\n",
      "epoch 5 batch 313 loss: 2.49837064743042\n",
      "epoch 5 batch 314 loss: 2.4173645973205566\n",
      "epoch 5 batch 315 loss: 2.42384672164917\n",
      "epoch 5 batch 316 loss: 2.2179677486419678\n",
      "epoch 5 batch 317 loss: 2.4582483768463135\n",
      "epoch 5 batch 318 loss: 2.335655927658081\n",
      "epoch 5 batch 319 loss: 2.2210419178009033\n",
      "epoch 5 batch 320 loss: 2.4875760078430176\n",
      "epoch 5 batch 321 loss: 2.250491142272949\n",
      "epoch 5 batch 322 loss: 2.2114028930664062\n",
      "epoch 5 batch 323 loss: 2.177811622619629\n",
      "epoch 5 batch 324 loss: 2.4113731384277344\n",
      "epoch 5 batch 325 loss: 2.4411721229553223\n",
      "epoch 5 batch 326 loss: 2.109738826751709\n",
      "epoch 5 batch 327 loss: 2.2429938316345215\n",
      "epoch 5 batch 328 loss: 2.1619434356689453\n",
      "epoch 5 batch 329 loss: 2.438417911529541\n",
      "epoch 5 batch 330 loss: 2.3341410160064697\n",
      "epoch 5 batch 331 loss: 2.319974899291992\n",
      "epoch 5 batch 332 loss: 2.1988253593444824\n",
      "epoch 5 batch 333 loss: 2.356245994567871\n",
      "epoch 5 batch 334 loss: 2.3634986877441406\n",
      "epoch 5 batch 335 loss: 2.400752305984497\n",
      "epoch 5 batch 336 loss: 2.1003518104553223\n",
      "epoch 5 batch 337 loss: 2.2546298503875732\n",
      "epoch 5 batch 338 loss: 2.604527473449707\n",
      "epoch 5 batch 339 loss: 2.1394424438476562\n",
      "epoch 5 batch 340 loss: 2.3317956924438477\n",
      "epoch 5 batch 341 loss: 2.259598970413208\n",
      "epoch 5 batch 342 loss: 2.282543182373047\n",
      "epoch 5 batch 343 loss: 2.48272705078125\n",
      "epoch 5 batch 344 loss: 2.3315868377685547\n",
      "epoch 5 batch 345 loss: 2.5141243934631348\n",
      "epoch 5 batch 346 loss: 2.3762621879577637\n",
      "epoch 5 batch 347 loss: 2.1311349868774414\n",
      "epoch 5 batch 348 loss: 2.3939290046691895\n",
      "epoch 5 batch 349 loss: 2.1828322410583496\n",
      "epoch 5 batch 350 loss: 2.140016794204712\n",
      "epoch 5 batch 351 loss: 2.2769546508789062\n",
      "epoch 5 batch 352 loss: 2.1504225730895996\n",
      "epoch 5 batch 353 loss: 2.198385238647461\n",
      "epoch 5 batch 354 loss: 2.0796778202056885\n",
      "epoch 5 batch 355 loss: 2.342963695526123\n",
      "epoch 5 batch 356 loss: 2.6107254028320312\n",
      "epoch 5 batch 357 loss: 2.3331987857818604\n",
      "epoch 5 batch 358 loss: 2.2183449268341064\n",
      "epoch 5 batch 359 loss: 2.1352131366729736\n",
      "epoch 5 batch 360 loss: 2.2928035259246826\n",
      "epoch 5 batch 361 loss: 2.297482967376709\n",
      "epoch 5 batch 362 loss: 2.307532787322998\n",
      "epoch 5 batch 363 loss: 2.2056355476379395\n",
      "epoch 5 batch 364 loss: 2.0487475395202637\n",
      "epoch 5 batch 365 loss: 2.2501912117004395\n",
      "epoch 5 batch 366 loss: 2.3670172691345215\n",
      "epoch 5 batch 367 loss: 2.1848244667053223\n",
      "epoch 5 batch 368 loss: 2.5394811630249023\n",
      "epoch 5 batch 369 loss: 2.302025556564331\n",
      "epoch 5 batch 370 loss: 2.3608269691467285\n",
      "epoch 5 batch 371 loss: 2.4660308361053467\n",
      "epoch 5 batch 372 loss: 2.5722570419311523\n",
      "epoch 5 batch 373 loss: 2.2642440795898438\n",
      "epoch 5 batch 374 loss: 2.144683837890625\n",
      "epoch 5 batch 375 loss: 2.0856807231903076\n",
      "epoch 5 batch 376 loss: 2.089223861694336\n",
      "epoch 5 batch 377 loss: 2.320732355117798\n",
      "epoch 5 batch 378 loss: 2.2177822589874268\n",
      "epoch 5 batch 379 loss: 2.061310291290283\n",
      "epoch 5 batch 380 loss: 2.3175811767578125\n",
      "epoch 5 batch 381 loss: 2.0526459217071533\n",
      "epoch 5 batch 382 loss: 2.4582738876342773\n",
      "epoch 5 batch 383 loss: 2.2238950729370117\n",
      "epoch 5 batch 384 loss: 2.3671836853027344\n",
      "epoch 5 batch 385 loss: 2.2122035026550293\n",
      "epoch 5 batch 386 loss: 2.286900043487549\n",
      "epoch 5 batch 387 loss: 2.183357000350952\n",
      "epoch 5 batch 388 loss: 2.3700428009033203\n",
      "epoch 5 batch 389 loss: 2.2522802352905273\n",
      "epoch 5 batch 390 loss: 2.4147541522979736\n",
      "epoch 5 batch 391 loss: 2.450286626815796\n",
      "epoch 5 batch 392 loss: 2.2638773918151855\n",
      "epoch 5 batch 393 loss: 2.3535423278808594\n",
      "epoch 5 batch 394 loss: 2.2900588512420654\n",
      "epoch 5 batch 395 loss: 2.133660316467285\n",
      "epoch 5 batch 396 loss: 2.310124397277832\n",
      "epoch 5 batch 397 loss: 2.1526083946228027\n",
      "epoch 5 batch 398 loss: 2.039077043533325\n",
      "epoch 5 batch 399 loss: 2.0676944255828857\n",
      "epoch 5 batch 400 loss: 2.0790719985961914\n",
      "epoch 5 batch 401 loss: 2.05230712890625\n",
      "epoch 5 batch 402 loss: 2.1224734783172607\n",
      "epoch 5 batch 403 loss: 2.439918041229248\n",
      "epoch 5 batch 404 loss: 2.510730028152466\n",
      "epoch 5 batch 405 loss: 2.2213010787963867\n",
      "epoch 5 batch 406 loss: 2.5586886405944824\n",
      "epoch 5 batch 407 loss: 2.5106937885284424\n",
      "epoch 5 batch 408 loss: 2.3949766159057617\n",
      "epoch 5 batch 409 loss: 2.328695297241211\n",
      "epoch 5 batch 410 loss: 2.10961651802063\n",
      "epoch 5 batch 411 loss: 2.1565446853637695\n",
      "epoch 5 batch 412 loss: 2.3755526542663574\n",
      "epoch 5 batch 413 loss: 2.2622780799865723\n",
      "epoch 5 batch 414 loss: 2.40002179145813\n",
      "epoch 5 batch 415 loss: 2.197230577468872\n",
      "epoch 5 batch 416 loss: 2.4746623039245605\n",
      "epoch 5 batch 417 loss: 2.3046393394470215\n",
      "epoch 5 batch 418 loss: 2.366410493850708\n",
      "epoch 5 batch 419 loss: 2.2533602714538574\n",
      "epoch 5 batch 420 loss: 2.4182088375091553\n",
      "epoch 5 batch 421 loss: 2.2257168292999268\n",
      "epoch 5 batch 422 loss: 2.4230806827545166\n",
      "epoch 5 batch 423 loss: 2.2073214054107666\n",
      "epoch 5 batch 424 loss: 2.5425491333007812\n",
      "epoch 5 batch 425 loss: 2.2804059982299805\n",
      "epoch 5 batch 426 loss: 2.411466360092163\n",
      "epoch 5 batch 427 loss: 2.3918259143829346\n",
      "epoch 5 batch 428 loss: 2.2285397052764893\n",
      "epoch 5 batch 429 loss: 2.591860771179199\n",
      "epoch 5 batch 430 loss: 2.2803261280059814\n",
      "epoch 5 batch 431 loss: 2.5024173259735107\n",
      "epoch 5 batch 432 loss: 2.210353136062622\n",
      "epoch 5 batch 433 loss: 2.145524740219116\n",
      "epoch 5 batch 434 loss: 2.3139593601226807\n",
      "epoch 5 batch 435 loss: 2.239755153656006\n",
      "epoch 5 batch 436 loss: 2.404945135116577\n",
      "epoch 5 batch 437 loss: 2.471374034881592\n",
      "epoch 5 batch 438 loss: 2.3529343605041504\n",
      "epoch 5 batch 439 loss: 2.4318342208862305\n",
      "epoch 5 batch 440 loss: 2.374161720275879\n",
      "epoch 5 batch 441 loss: 2.478461742401123\n",
      "epoch 5 batch 442 loss: 2.360931634902954\n",
      "epoch 5 batch 443 loss: 2.1149511337280273\n",
      "epoch 5 batch 444 loss: 2.3851256370544434\n",
      "epoch 5 batch 445 loss: 2.50059175491333\n",
      "epoch 5 batch 446 loss: 2.335597038269043\n",
      "epoch 5 batch 447 loss: 2.4574930667877197\n",
      "epoch 5 batch 448 loss: 2.3183302879333496\n",
      "epoch 5 batch 449 loss: 2.2669105529785156\n",
      "epoch 5 batch 450 loss: 2.326046943664551\n",
      "epoch 5 batch 451 loss: 2.5003600120544434\n",
      "epoch 5 batch 452 loss: 2.5245189666748047\n",
      "epoch 5 batch 453 loss: 2.264416456222534\n",
      "epoch 5 batch 454 loss: 2.346646785736084\n",
      "epoch 5 batch 455 loss: 2.1656136512756348\n",
      "epoch 5 batch 456 loss: 2.4376792907714844\n",
      "epoch 5 batch 457 loss: 2.317272901535034\n",
      "epoch 5 batch 458 loss: 2.5180554389953613\n",
      "epoch 5 batch 459 loss: 2.412863254547119\n",
      "epoch 5 batch 460 loss: 2.325589179992676\n",
      "epoch 5 batch 461 loss: 2.3711695671081543\n",
      "epoch 5 batch 462 loss: 2.371702194213867\n",
      "epoch 5 batch 463 loss: 2.501824378967285\n",
      "epoch 5 batch 464 loss: 2.4574952125549316\n",
      "epoch 5 batch 465 loss: 2.316974639892578\n",
      "epoch 5 batch 466 loss: 2.809494972229004\n",
      "epoch 5 batch 467 loss: 2.384667158126831\n",
      "epoch 5 batch 468 loss: 2.223909378051758\n",
      "epoch 5 batch 469 loss: 2.321371555328369\n",
      "epoch 5 batch 470 loss: 2.513256549835205\n",
      "epoch 5 batch 471 loss: 2.3771440982818604\n",
      "epoch 5 batch 472 loss: 2.2138113975524902\n",
      "epoch 5 batch 473 loss: 2.3315937519073486\n",
      "epoch 5 batch 474 loss: 2.1123709678649902\n",
      "epoch 5 batch 475 loss: 2.11167573928833\n",
      "epoch 5 batch 476 loss: 2.8226094245910645\n",
      "epoch 5 batch 477 loss: 2.4250080585479736\n",
      "epoch 5 batch 478 loss: 2.1558847427368164\n",
      "epoch 5 batch 479 loss: 2.365313768386841\n",
      "epoch 5 batch 480 loss: 2.3888816833496094\n",
      "epoch 5 batch 481 loss: 2.542510747909546\n",
      "epoch 5 batch 482 loss: 2.3771250247955322\n",
      "epoch 5 batch 483 loss: 2.3453540802001953\n",
      "epoch 5 batch 484 loss: 2.349623203277588\n",
      "epoch 5 batch 485 loss: 2.3817667961120605\n",
      "epoch 5 batch 486 loss: 2.3238372802734375\n",
      "epoch 5 batch 487 loss: 2.1939620971679688\n",
      "epoch 5 batch 488 loss: 2.3878378868103027\n",
      "epoch 5 batch 489 loss: 2.569800615310669\n",
      "epoch 5 batch 490 loss: 2.230095624923706\n",
      "epoch 5 batch 491 loss: 2.3646774291992188\n",
      "epoch 5 batch 492 loss: 2.4942305088043213\n",
      "epoch 5 batch 493 loss: 2.1877145767211914\n",
      "epoch 5 batch 494 loss: 2.4353973865509033\n",
      "epoch 5 batch 495 loss: 2.3260769844055176\n",
      "epoch 5 batch 496 loss: 2.3224050998687744\n",
      "epoch 5 batch 497 loss: 2.2953062057495117\n",
      "epoch 5 batch 498 loss: 2.223642587661743\n",
      "epoch 5 batch 499 loss: 2.5030770301818848\n",
      "epoch 5 batch 500 loss: 2.4190149307250977\n",
      "epoch 5 batch 501 loss: 2.3219714164733887\n",
      "epoch 5 batch 502 loss: 2.4495577812194824\n",
      "epoch 5 batch 503 loss: 2.3203835487365723\n",
      "epoch 5 batch 504 loss: 2.3571455478668213\n",
      "epoch 5 batch 505 loss: 2.4947404861450195\n",
      "epoch 5 batch 506 loss: 2.1853532791137695\n",
      "epoch 5 batch 507 loss: 2.3635692596435547\n",
      "epoch 5 batch 508 loss: 2.286986827850342\n",
      "epoch 5 batch 509 loss: 2.6996073722839355\n",
      "epoch 5 batch 510 loss: 2.5356037616729736\n",
      "epoch 5 batch 511 loss: 2.3835649490356445\n",
      "epoch 5 batch 512 loss: 2.2925989627838135\n",
      "epoch 5 batch 513 loss: 2.320209503173828\n",
      "epoch 5 batch 514 loss: 2.3958544731140137\n",
      "epoch 5 batch 515 loss: 2.3018414974212646\n",
      "epoch 5 batch 516 loss: 2.327681064605713\n",
      "epoch 5 batch 517 loss: 2.0716960430145264\n",
      "epoch 5 batch 518 loss: 2.395777702331543\n",
      "epoch 5 batch 519 loss: 2.237501621246338\n",
      "epoch 5 batch 520 loss: 2.180048704147339\n",
      "epoch 5 batch 521 loss: 2.270451545715332\n",
      "epoch 5 batch 522 loss: 2.569234848022461\n",
      "epoch 5 batch 523 loss: 2.1360113620758057\n",
      "epoch 5 batch 524 loss: 2.3440823554992676\n",
      "epoch 5 batch 525 loss: 2.0482020378112793\n",
      "epoch 5 batch 526 loss: 2.520843267440796\n",
      "epoch 5 batch 527 loss: 2.2772951126098633\n",
      "epoch 5 batch 528 loss: 2.21224045753479\n",
      "epoch 5 batch 529 loss: 2.453678607940674\n",
      "epoch 5 batch 530 loss: 2.0490331649780273\n",
      "epoch 5 batch 531 loss: 2.2096967697143555\n",
      "epoch 5 batch 532 loss: 2.1134018898010254\n",
      "epoch 5 batch 533 loss: 2.203171730041504\n",
      "epoch 5 batch 534 loss: 2.2834489345550537\n",
      "epoch 5 batch 535 loss: 2.175602436065674\n",
      "epoch 5 batch 536 loss: 2.231046199798584\n",
      "epoch 5 batch 537 loss: 2.5109453201293945\n",
      "epoch 5 batch 538 loss: 2.4997076988220215\n",
      "epoch 5 batch 539 loss: 2.4048514366149902\n",
      "epoch 5 batch 540 loss: 2.152693033218384\n",
      "epoch 5 batch 541 loss: 2.115967273712158\n",
      "epoch 5 batch 542 loss: 2.1981751918792725\n",
      "epoch 5 batch 543 loss: 2.1981492042541504\n",
      "epoch 5 batch 544 loss: 2.388963222503662\n",
      "epoch 5 batch 545 loss: 2.5000720024108887\n",
      "epoch 5 batch 546 loss: 2.034708023071289\n",
      "epoch 5 batch 547 loss: 2.529775381088257\n",
      "epoch 5 batch 548 loss: 2.235518455505371\n",
      "epoch 5 batch 549 loss: 2.301234722137451\n",
      "epoch 5 batch 550 loss: 2.243251323699951\n",
      "epoch 5 batch 551 loss: 2.173089027404785\n",
      "epoch 5 batch 552 loss: 2.0413098335266113\n",
      "epoch 5 batch 553 loss: 2.366720199584961\n",
      "epoch 5 batch 554 loss: 2.364523410797119\n",
      "epoch 5 batch 555 loss: 1.9933819770812988\n",
      "epoch 5 batch 556 loss: 2.3357129096984863\n",
      "epoch 5 batch 557 loss: 2.0790772438049316\n",
      "epoch 5 batch 558 loss: 2.3461756706237793\n",
      "epoch 5 batch 559 loss: 2.298309326171875\n",
      "epoch 5 batch 560 loss: 2.5133633613586426\n",
      "epoch 5 batch 561 loss: 2.3698296546936035\n",
      "epoch 5 batch 562 loss: 2.0422003269195557\n",
      "epoch 5 batch 563 loss: 2.6012773513793945\n",
      "epoch 5 batch 564 loss: 2.3005764484405518\n",
      "epoch 5 batch 565 loss: 2.1607799530029297\n",
      "epoch 5 batch 566 loss: 2.4258694648742676\n",
      "epoch 5 batch 567 loss: 2.2929983139038086\n",
      "epoch 5 batch 568 loss: 2.1618051528930664\n",
      "epoch 5 batch 569 loss: 2.1697750091552734\n",
      "epoch 5 batch 570 loss: 2.5186753273010254\n",
      "epoch 5 batch 571 loss: 2.203394889831543\n",
      "epoch 5 batch 572 loss: 2.469961166381836\n",
      "epoch 5 batch 573 loss: 2.2699337005615234\n",
      "epoch 5 batch 574 loss: 2.1498920917510986\n",
      "epoch 5 batch 575 loss: 2.311641216278076\n",
      "epoch 5 batch 576 loss: 2.1846745014190674\n",
      "epoch 5 batch 577 loss: 2.191603660583496\n",
      "epoch 5 batch 578 loss: 2.3466663360595703\n",
      "epoch 5 batch 579 loss: 2.5186851024627686\n",
      "epoch 5 batch 580 loss: 2.439265251159668\n",
      "epoch 5 batch 581 loss: 2.3947463035583496\n",
      "epoch 5 batch 582 loss: 2.343942642211914\n",
      "epoch 5 batch 583 loss: 2.300380229949951\n",
      "epoch 5 batch 584 loss: 2.3299813270568848\n",
      "epoch 5 batch 585 loss: 2.2138671875\n",
      "epoch 5 batch 586 loss: 2.541602849960327\n",
      "epoch 5 batch 587 loss: 2.461177349090576\n",
      "epoch 5 batch 588 loss: 2.347637176513672\n",
      "epoch 5 batch 589 loss: 2.376936197280884\n",
      "epoch 5 batch 590 loss: 2.2327451705932617\n",
      "epoch 5 batch 591 loss: 2.2076151371002197\n",
      "epoch 5 batch 592 loss: 2.310910701751709\n",
      "epoch 5 batch 593 loss: 2.4538793563842773\n",
      "epoch 5 batch 594 loss: 2.067058563232422\n",
      "epoch 5 batch 595 loss: 2.3665482997894287\n",
      "epoch 5 batch 596 loss: 2.549834728240967\n",
      "epoch 5 batch 597 loss: 2.4856488704681396\n",
      "epoch 5 batch 598 loss: 2.5326333045959473\n",
      "epoch 5 batch 599 loss: 2.320455551147461\n",
      "epoch 5 batch 600 loss: 2.230506181716919\n",
      "epoch 5 batch 601 loss: 2.1199374198913574\n",
      "epoch 5 batch 602 loss: 2.4400508403778076\n",
      "epoch 5 batch 603 loss: 2.3157849311828613\n",
      "epoch 5 batch 604 loss: 2.302070379257202\n",
      "epoch 5 batch 605 loss: 2.20670223236084\n",
      "epoch 5 batch 606 loss: 2.165238857269287\n",
      "epoch 5 batch 607 loss: 2.3250207901000977\n",
      "epoch 5 batch 608 loss: 2.363546848297119\n",
      "epoch 5 batch 609 loss: 2.546431303024292\n",
      "epoch 5 batch 610 loss: 2.220607042312622\n",
      "epoch 5 batch 611 loss: 2.252570867538452\n",
      "epoch 5 batch 612 loss: 2.37296724319458\n",
      "epoch 5 batch 613 loss: 2.2431869506835938\n",
      "epoch 5 batch 614 loss: 2.488433837890625\n",
      "epoch 5 batch 615 loss: 2.3814339637756348\n",
      "epoch 5 batch 616 loss: 2.338650703430176\n",
      "epoch 5 batch 617 loss: 2.2195801734924316\n",
      "epoch 5 batch 618 loss: 2.42551851272583\n",
      "epoch 5 batch 619 loss: 2.3035945892333984\n",
      "epoch 5 batch 620 loss: 2.286712169647217\n",
      "epoch 5 batch 621 loss: 2.30078125\n",
      "epoch 5 batch 622 loss: 2.3557043075561523\n",
      "epoch 5 batch 623 loss: 2.3549952507019043\n",
      "epoch 5 batch 624 loss: 2.362273693084717\n",
      "epoch 5 batch 625 loss: 2.2216806411743164\n",
      "epoch 5 batch 626 loss: 2.150516986846924\n",
      "epoch 5 batch 627 loss: 2.345412254333496\n",
      "epoch 5 batch 628 loss: 2.223252296447754\n",
      "epoch 5 batch 629 loss: 2.602349281311035\n",
      "epoch 5 batch 630 loss: 2.22446346282959\n",
      "epoch 5 batch 631 loss: 2.2111377716064453\n",
      "epoch 5 batch 632 loss: 2.4206323623657227\n",
      "epoch 5 batch 633 loss: 2.2353873252868652\n",
      "epoch 5 batch 634 loss: 2.200840950012207\n",
      "epoch 5 batch 635 loss: 2.400350570678711\n",
      "epoch 5 batch 636 loss: 2.294025421142578\n",
      "epoch 5 batch 637 loss: 2.6478562355041504\n",
      "epoch 5 batch 638 loss: 2.270653486251831\n",
      "epoch 5 batch 639 loss: 2.3291244506835938\n",
      "epoch 5 batch 640 loss: 2.347804069519043\n",
      "epoch 5 batch 641 loss: 2.211954116821289\n",
      "epoch 5 batch 642 loss: 2.3580827713012695\n",
      "epoch 5 batch 643 loss: 2.5303421020507812\n",
      "epoch 5 batch 644 loss: 2.296867609024048\n",
      "epoch 5 batch 645 loss: 2.2418887615203857\n",
      "epoch 5 batch 646 loss: 2.4015111923217773\n",
      "epoch 5 batch 647 loss: 2.3578155040740967\n",
      "epoch 5 batch 648 loss: 2.1788666248321533\n",
      "epoch 5 batch 649 loss: 2.022954225540161\n",
      "epoch 5 batch 650 loss: 2.3216164112091064\n",
      "epoch 5 batch 651 loss: 2.3844540119171143\n",
      "epoch 5 batch 652 loss: 2.522536516189575\n",
      "epoch 5 batch 653 loss: 2.320992946624756\n",
      "epoch 5 batch 654 loss: 2.3049395084381104\n",
      "epoch 5 batch 655 loss: 2.2561678886413574\n",
      "epoch 5 batch 656 loss: 2.437260389328003\n",
      "epoch 5 batch 657 loss: 1.9736331701278687\n",
      "epoch 5 batch 658 loss: 2.2818562984466553\n",
      "epoch 5 batch 659 loss: 2.386136531829834\n",
      "epoch 5 batch 660 loss: 2.28873872756958\n",
      "epoch 5 batch 661 loss: 2.1289966106414795\n",
      "epoch 5 batch 662 loss: 2.424099922180176\n",
      "epoch 5 batch 663 loss: 2.216545581817627\n",
      "epoch 5 batch 664 loss: 2.0919535160064697\n",
      "epoch 5 batch 665 loss: 2.2601230144500732\n",
      "epoch 5 batch 666 loss: 2.1568093299865723\n",
      "epoch 5 batch 667 loss: 2.2330451011657715\n",
      "epoch 5 batch 668 loss: 2.440181016921997\n",
      "epoch 5 batch 669 loss: 2.4532828330993652\n",
      "epoch 5 batch 670 loss: 2.009803056716919\n",
      "epoch 5 batch 671 loss: 2.246135711669922\n",
      "epoch 5 batch 672 loss: 2.099977970123291\n",
      "epoch 5 batch 673 loss: 2.1320641040802\n",
      "epoch 5 batch 674 loss: 2.517270088195801\n",
      "epoch 5 batch 675 loss: 2.1604180335998535\n",
      "epoch 5 batch 676 loss: 2.2685670852661133\n",
      "epoch 5 batch 677 loss: 2.260885715484619\n",
      "epoch 5 batch 678 loss: 2.368101119995117\n",
      "epoch 5 batch 679 loss: 2.3944251537323\n",
      "epoch 5 batch 680 loss: 2.4422640800476074\n",
      "epoch 5 batch 681 loss: 2.2639002799987793\n",
      "epoch 5 batch 682 loss: 2.2501659393310547\n",
      "epoch 5 batch 683 loss: 2.250420093536377\n",
      "epoch 5 batch 684 loss: 2.289323329925537\n",
      "epoch 5 batch 685 loss: 2.2843525409698486\n",
      "epoch 5 batch 686 loss: 2.1938869953155518\n",
      "epoch 5 batch 687 loss: 2.2541937828063965\n",
      "epoch 5 batch 688 loss: 2.3813319206237793\n",
      "epoch 5 batch 689 loss: 2.3517725467681885\n",
      "epoch 5 batch 690 loss: 2.234405040740967\n",
      "epoch 5 batch 691 loss: 2.4425101280212402\n",
      "epoch 5 batch 692 loss: 2.1005618572235107\n",
      "epoch 5 batch 693 loss: 2.4581334590911865\n",
      "epoch 5 batch 694 loss: 2.7239646911621094\n",
      "epoch 5 batch 695 loss: 2.4966506958007812\n",
      "epoch 5 batch 696 loss: 2.2174313068389893\n",
      "epoch 5 batch 697 loss: 2.3835701942443848\n",
      "epoch 5 batch 698 loss: 2.5980801582336426\n",
      "epoch 5 batch 699 loss: 2.2689146995544434\n",
      "epoch 5 batch 700 loss: 2.529216766357422\n",
      "epoch 5 batch 701 loss: 2.2739830017089844\n",
      "epoch 5 batch 702 loss: 2.4010794162750244\n",
      "epoch 5 batch 703 loss: 2.4031620025634766\n",
      "epoch 5 batch 704 loss: 2.2280349731445312\n",
      "epoch 5 batch 705 loss: 2.2338218688964844\n",
      "epoch 5 batch 706 loss: 2.422578811645508\n",
      "epoch 5 batch 707 loss: 2.2479891777038574\n",
      "epoch 5 batch 708 loss: 2.1938889026641846\n",
      "epoch 5 batch 709 loss: 2.0738344192504883\n",
      "epoch 5 batch 710 loss: 2.488233804702759\n",
      "epoch 5 batch 711 loss: 2.413423776626587\n",
      "epoch 5 batch 712 loss: 2.452651262283325\n",
      "epoch 5 batch 713 loss: 2.352546215057373\n",
      "epoch 5 batch 714 loss: 2.463115692138672\n",
      "epoch 5 batch 715 loss: 2.4252829551696777\n",
      "epoch 5 batch 716 loss: 2.3284454345703125\n",
      "epoch 5 batch 717 loss: 2.6750917434692383\n",
      "epoch 5 batch 718 loss: 2.2809689044952393\n",
      "epoch 5 batch 719 loss: 2.3107593059539795\n",
      "epoch 5 batch 720 loss: 2.5263075828552246\n",
      "epoch 5 batch 721 loss: 2.317777633666992\n",
      "epoch 5 batch 722 loss: 2.4434635639190674\n",
      "epoch 5 batch 723 loss: 2.244117259979248\n",
      "epoch 5 batch 724 loss: 2.1654109954833984\n",
      "epoch 5 batch 725 loss: 2.266530752182007\n",
      "epoch 5 batch 726 loss: 2.117551326751709\n",
      "epoch 5 batch 727 loss: 2.339829206466675\n",
      "epoch 5 batch 728 loss: 2.346846103668213\n",
      "epoch 5 batch 729 loss: 2.151179790496826\n",
      "epoch 5 batch 730 loss: 2.170243740081787\n",
      "epoch 5 batch 731 loss: 2.41011381149292\n",
      "epoch 5 batch 732 loss: 2.2828288078308105\n",
      "epoch 5 batch 733 loss: 2.17295241355896\n",
      "epoch 5 batch 734 loss: 2.411616325378418\n",
      "epoch 5 batch 735 loss: 2.26936674118042\n",
      "epoch 5 batch 736 loss: 2.2643613815307617\n",
      "epoch 5 batch 737 loss: 2.2339296340942383\n",
      "epoch 5 batch 738 loss: 2.2133994102478027\n",
      "epoch 5 batch 739 loss: 2.2367141246795654\n",
      "epoch 5 batch 740 loss: 2.1913678646087646\n",
      "epoch 5 batch 741 loss: 2.2453103065490723\n",
      "epoch 5 batch 742 loss: 2.24222993850708\n",
      "epoch 5 batch 743 loss: 2.2398037910461426\n",
      "epoch 5 batch 744 loss: 2.4665958881378174\n",
      "epoch 5 batch 745 loss: 2.5171122550964355\n",
      "epoch 5 batch 746 loss: 2.2751407623291016\n",
      "epoch 5 batch 747 loss: 2.072108745574951\n",
      "epoch 5 batch 748 loss: 2.15573787689209\n",
      "epoch 5 batch 749 loss: 2.25225830078125\n",
      "epoch 5 batch 750 loss: 2.132538318634033\n",
      "epoch 5 batch 751 loss: 2.105930805206299\n",
      "epoch 5 batch 752 loss: 2.239553451538086\n",
      "epoch 5 batch 753 loss: 2.403866767883301\n",
      "epoch 5 batch 754 loss: 2.2722361087799072\n",
      "epoch 5 batch 755 loss: 2.092252254486084\n",
      "epoch 5 batch 756 loss: 2.111344814300537\n",
      "epoch 5 batch 757 loss: 2.0034539699554443\n",
      "epoch 5 batch 758 loss: 2.4610767364501953\n",
      "epoch 5 batch 759 loss: 2.4063072204589844\n",
      "epoch 5 batch 760 loss: 2.220304489135742\n",
      "epoch 5 batch 761 loss: 2.426819324493408\n",
      "epoch 5 batch 762 loss: 2.2121191024780273\n",
      "epoch 5 batch 763 loss: 2.23777437210083\n",
      "epoch 5 batch 764 loss: 2.4423270225524902\n",
      "epoch 5 batch 765 loss: 2.179060459136963\n",
      "epoch 5 batch 766 loss: 2.4455018043518066\n",
      "epoch 5 batch 767 loss: 2.3024511337280273\n",
      "epoch 5 batch 768 loss: 2.444521903991699\n",
      "epoch 5 batch 769 loss: 2.0294909477233887\n",
      "epoch 5 batch 770 loss: 2.2153453826904297\n",
      "epoch 5 batch 771 loss: 2.1888461112976074\n",
      "epoch 5 batch 772 loss: 2.1408586502075195\n",
      "epoch 5 batch 773 loss: 2.0522048473358154\n",
      "epoch 5 batch 774 loss: 2.3203320503234863\n",
      "epoch 5 batch 775 loss: 2.560987949371338\n",
      "epoch 5 batch 776 loss: 2.4133453369140625\n",
      "epoch 5 batch 777 loss: 2.3733460903167725\n",
      "epoch 5 batch 778 loss: 2.267162799835205\n",
      "epoch 5 batch 779 loss: 2.3263137340545654\n",
      "epoch 5 batch 780 loss: 2.327012062072754\n",
      "epoch 5 batch 781 loss: 2.27396821975708\n",
      "epoch 5 batch 782 loss: 2.7039918899536133\n",
      "epoch 5 batch 783 loss: 2.161658525466919\n",
      "epoch 5 batch 784 loss: 2.2396183013916016\n",
      "epoch 5 batch 785 loss: 2.214540958404541\n",
      "epoch 5 batch 786 loss: 2.303744316101074\n",
      "epoch 5 batch 787 loss: 2.5081591606140137\n",
      "epoch 5 batch 788 loss: 2.3258609771728516\n",
      "epoch 5 batch 789 loss: 2.1066064834594727\n",
      "epoch 5 batch 790 loss: 2.5102572441101074\n",
      "epoch 5 batch 791 loss: 2.2921829223632812\n",
      "epoch 5 batch 792 loss: 2.1389951705932617\n",
      "epoch 5 batch 793 loss: 2.173204183578491\n",
      "epoch 5 batch 794 loss: 2.3658018112182617\n",
      "epoch 5 batch 795 loss: 2.588585376739502\n",
      "epoch 5 batch 796 loss: 2.476646900177002\n",
      "epoch 5 batch 797 loss: 2.353205680847168\n",
      "epoch 5 batch 798 loss: 2.3120789527893066\n",
      "epoch 5 batch 799 loss: 2.2466139793395996\n",
      "epoch 5 batch 800 loss: 2.145962953567505\n",
      "epoch 5 batch 801 loss: 2.179287910461426\n",
      "epoch 5 batch 802 loss: 2.204160213470459\n",
      "epoch 5 batch 803 loss: 2.508204460144043\n",
      "epoch 5 batch 804 loss: 2.2423956394195557\n",
      "epoch 5 batch 805 loss: 2.4402782917022705\n",
      "epoch 5 batch 806 loss: 2.17889142036438\n",
      "epoch 5 batch 807 loss: 2.1888251304626465\n",
      "epoch 5 batch 808 loss: 2.409208059310913\n",
      "epoch 5 batch 809 loss: 2.228703498840332\n",
      "epoch 5 batch 810 loss: 2.52048397064209\n",
      "epoch 5 batch 811 loss: 2.238893508911133\n",
      "epoch 5 batch 812 loss: 2.568289279937744\n",
      "epoch 5 batch 813 loss: 2.304072856903076\n",
      "epoch 5 batch 814 loss: 2.4700355529785156\n",
      "epoch 5 batch 815 loss: 2.258361339569092\n",
      "epoch 5 batch 816 loss: 2.314931869506836\n",
      "epoch 5 batch 817 loss: 2.368861198425293\n",
      "epoch 5 batch 818 loss: 2.2784743309020996\n",
      "epoch 5 batch 819 loss: 2.198978900909424\n",
      "epoch 5 batch 820 loss: 2.6292033195495605\n",
      "epoch 5 batch 821 loss: 2.293426036834717\n",
      "epoch 5 batch 822 loss: 2.0621159076690674\n",
      "epoch 5 batch 823 loss: 2.450216293334961\n",
      "epoch 5 batch 824 loss: 2.198256492614746\n",
      "epoch 5 batch 825 loss: 2.097728729248047\n",
      "epoch 5 batch 826 loss: 2.4031198024749756\n",
      "epoch 5 batch 827 loss: 2.2809832096099854\n",
      "epoch 5 batch 828 loss: 2.300551414489746\n",
      "epoch 5 batch 829 loss: 2.1702699661254883\n",
      "epoch 5 batch 830 loss: 2.1707887649536133\n",
      "epoch 5 batch 831 loss: 2.283437967300415\n",
      "epoch 5 batch 832 loss: 2.33085298538208\n",
      "epoch 5 batch 833 loss: 2.163540840148926\n",
      "epoch 5 batch 834 loss: 2.35613751411438\n",
      "epoch 5 batch 835 loss: 2.482675790786743\n",
      "epoch 5 batch 836 loss: 2.1291005611419678\n",
      "epoch 5 batch 837 loss: 2.232656240463257\n",
      "epoch 5 batch 838 loss: 2.7637224197387695\n",
      "epoch 5 batch 839 loss: 2.268019199371338\n",
      "epoch 5 batch 840 loss: 2.1641554832458496\n",
      "epoch 5 batch 841 loss: 2.604701042175293\n",
      "epoch 5 batch 842 loss: 2.367241382598877\n",
      "epoch 5 batch 843 loss: 2.195164203643799\n",
      "epoch 5 batch 844 loss: 2.3318536281585693\n",
      "epoch 5 batch 845 loss: 2.333261489868164\n",
      "epoch 5 batch 846 loss: 2.2964208126068115\n",
      "epoch 5 batch 847 loss: 2.1267848014831543\n",
      "epoch 5 batch 848 loss: 2.5739707946777344\n",
      "epoch 5 batch 849 loss: 2.3166842460632324\n",
      "epoch 5 batch 850 loss: 2.2630372047424316\n",
      "epoch 5 batch 851 loss: 2.3472418785095215\n",
      "epoch 5 batch 852 loss: 2.3882482051849365\n",
      "epoch 5 batch 853 loss: 2.2170002460479736\n",
      "epoch 5 batch 854 loss: 2.388622283935547\n",
      "epoch 5 batch 855 loss: 2.3053104877471924\n",
      "epoch 5 batch 856 loss: 2.311718225479126\n",
      "epoch 5 batch 857 loss: 2.5160140991210938\n",
      "epoch 5 batch 858 loss: 2.0959513187408447\n",
      "epoch 5 batch 859 loss: 2.464717388153076\n",
      "epoch 5 batch 860 loss: 2.33365535736084\n",
      "epoch 5 batch 861 loss: 2.1712894439697266\n",
      "epoch 5 batch 862 loss: 2.261692523956299\n",
      "epoch 5 batch 863 loss: 2.6413674354553223\n",
      "epoch 5 batch 864 loss: 2.440410614013672\n",
      "epoch 5 batch 865 loss: 2.4942264556884766\n",
      "epoch 5 batch 866 loss: 2.2365529537200928\n",
      "epoch 5 batch 867 loss: 2.2273051738739014\n",
      "epoch 5 batch 868 loss: 2.06958270072937\n",
      "epoch 5 batch 869 loss: 2.1824216842651367\n",
      "epoch 5 batch 870 loss: 2.3128294944763184\n",
      "epoch 5 batch 871 loss: 2.4705276489257812\n",
      "epoch 5 batch 872 loss: 2.0377564430236816\n",
      "epoch 5 batch 873 loss: 2.3837482929229736\n",
      "epoch 5 batch 874 loss: 2.254866123199463\n",
      "epoch 5 batch 875 loss: 2.2605347633361816\n",
      "epoch 5 batch 876 loss: 2.3615424633026123\n",
      "epoch 5 batch 877 loss: 2.4368081092834473\n",
      "epoch 5 batch 878 loss: 2.632911205291748\n",
      "epoch 5 batch 879 loss: 2.056112289428711\n",
      "epoch 5 batch 880 loss: 2.217501640319824\n",
      "epoch 5 batch 881 loss: 2.1940696239471436\n",
      "epoch 5 batch 882 loss: 2.5457162857055664\n",
      "epoch 5 batch 883 loss: 2.357964515686035\n",
      "epoch 5 batch 884 loss: 2.4001007080078125\n",
      "epoch 5 batch 885 loss: 2.3742008209228516\n",
      "epoch 5 batch 886 loss: 2.2550458908081055\n",
      "epoch 5 batch 887 loss: 2.213930130004883\n",
      "epoch 5 batch 888 loss: 2.2973787784576416\n",
      "epoch 5 batch 889 loss: 2.201674461364746\n",
      "epoch 5 batch 890 loss: 2.299071788787842\n",
      "epoch 5 batch 891 loss: 2.5040645599365234\n",
      "epoch 5 batch 892 loss: 2.501633644104004\n",
      "epoch 5 batch 893 loss: 2.45511531829834\n",
      "epoch 5 batch 894 loss: 2.3888041973114014\n",
      "epoch 5 batch 895 loss: 2.555215358734131\n",
      "epoch 5 batch 896 loss: 2.3182597160339355\n",
      "epoch 5 batch 897 loss: 2.52089262008667\n",
      "epoch 5 batch 898 loss: 2.3089914321899414\n",
      "epoch 5 batch 899 loss: 2.418621063232422\n",
      "epoch 5 batch 900 loss: 2.1376290321350098\n",
      "epoch 5 batch 901 loss: 2.2883143424987793\n",
      "epoch 5 batch 902 loss: 2.1252622604370117\n",
      "epoch 5 batch 903 loss: 2.3883724212646484\n",
      "epoch 5 batch 904 loss: 2.496528148651123\n",
      "epoch 5 batch 905 loss: 2.2926411628723145\n",
      "epoch 5 batch 906 loss: 2.1547608375549316\n",
      "epoch 5 batch 907 loss: 2.449310779571533\n",
      "epoch 5 batch 908 loss: 2.3051462173461914\n",
      "epoch 5 batch 909 loss: 2.259608745574951\n",
      "epoch 5 batch 910 loss: 2.3581180572509766\n",
      "epoch 5 batch 911 loss: 2.287390947341919\n",
      "epoch 5 batch 912 loss: 2.440703868865967\n",
      "epoch 5 batch 913 loss: 2.5599732398986816\n",
      "epoch 5 batch 914 loss: 2.2917590141296387\n",
      "epoch 5 batch 915 loss: 2.4225571155548096\n",
      "epoch 5 batch 916 loss: 2.378251791000366\n",
      "epoch 5 batch 917 loss: 2.2153687477111816\n",
      "epoch 5 batch 918 loss: 2.4404568672180176\n",
      "epoch 5 batch 919 loss: 2.4340600967407227\n",
      "epoch 5 batch 920 loss: 2.0732030868530273\n",
      "epoch 5 batch 921 loss: 2.09203839302063\n",
      "epoch 5 batch 922 loss: 2.4578661918640137\n",
      "epoch 5 batch 923 loss: 2.352400779724121\n",
      "epoch 5 batch 924 loss: 2.3407340049743652\n",
      "epoch 5 batch 925 loss: 2.245039224624634\n",
      "epoch 5 batch 926 loss: 2.3970136642456055\n",
      "epoch 5 batch 927 loss: 2.387479305267334\n",
      "epoch 5 batch 928 loss: 2.2531347274780273\n",
      "epoch 5 batch 929 loss: 2.360431671142578\n",
      "epoch 5 batch 930 loss: 2.0916385650634766\n",
      "epoch 5 batch 931 loss: 2.254070520401001\n",
      "epoch 5 batch 932 loss: 2.2459094524383545\n",
      "epoch 5 batch 933 loss: 2.2966835498809814\n",
      "epoch 5 batch 934 loss: 2.0203380584716797\n",
      "epoch 5 batch 935 loss: 2.4005041122436523\n",
      "epoch 5 batch 936 loss: 2.263676643371582\n",
      "epoch 5 batch 937 loss: 2.5191242694854736\n",
      "epoch 5 batch 938 loss: 2.22255277633667\n",
      "epoch 5 batch 939 loss: 2.2509658336639404\n",
      "epoch 5 batch 940 loss: 2.3205134868621826\n",
      "epoch 5 batch 941 loss: 2.179133415222168\n",
      "epoch 5 batch 942 loss: 2.0633935928344727\n",
      "epoch 5 batch 943 loss: 2.1921796798706055\n",
      "epoch 5 batch 944 loss: 2.0625128746032715\n",
      "epoch 5 batch 945 loss: 2.1049585342407227\n",
      "epoch 5 batch 946 loss: 2.4862051010131836\n",
      "epoch 5 batch 947 loss: 2.523869037628174\n",
      "epoch 5 batch 948 loss: 2.455916404724121\n",
      "epoch 5 batch 949 loss: 2.3375067710876465\n",
      "epoch 5 batch 950 loss: 2.2892842292785645\n",
      "epoch 5 batch 951 loss: 2.2454142570495605\n",
      "epoch 5 batch 952 loss: 2.2471437454223633\n",
      "epoch 5 batch 953 loss: 2.2804245948791504\n",
      "epoch 5 batch 954 loss: 2.3064584732055664\n",
      "epoch 5 batch 955 loss: 2.419912815093994\n",
      "epoch 5 batch 956 loss: 2.3216304779052734\n",
      "epoch 5 batch 957 loss: 2.1627144813537598\n",
      "epoch 5 batch 958 loss: 2.054544448852539\n",
      "epoch 5 batch 959 loss: 2.386610984802246\n",
      "epoch 5 batch 960 loss: 2.432476282119751\n",
      "epoch 5 batch 961 loss: 2.0694007873535156\n",
      "epoch 5 batch 962 loss: 2.271583318710327\n",
      "epoch 5 batch 963 loss: 2.3407483100891113\n",
      "epoch 5 batch 964 loss: 2.186793327331543\n",
      "epoch 5 batch 965 loss: 2.3238682746887207\n",
      "epoch 5 batch 966 loss: 2.3486135005950928\n",
      "epoch 5 batch 967 loss: 2.236768960952759\n",
      "epoch 5 batch 968 loss: 2.3490405082702637\n",
      "epoch 5 batch 969 loss: 2.084538698196411\n",
      "epoch 5 batch 970 loss: 2.2269439697265625\n",
      "epoch 5 batch 971 loss: 2.624312162399292\n",
      "epoch 5 batch 972 loss: 2.4917163848876953\n",
      "epoch 5 batch 973 loss: 2.4279983043670654\n",
      "epoch 5 batch 974 loss: 2.2737600803375244\n",
      "epoch 5 batch 975 loss: 2.334704875946045\n",
      "epoch 5 batch 976 loss: 2.2527196407318115\n",
      "epoch 5 batch 977 loss: 2.1246767044067383\n",
      "epoch 5 batch 978 loss: 2.5403475761413574\n",
      "epoch 5 batch 979 loss: 2.152803897857666\n",
      "epoch 5 batch 980 loss: 2.196406364440918\n",
      "epoch 5 batch 981 loss: 2.342012405395508\n",
      "epoch 5 batch 982 loss: 2.3638477325439453\n",
      "epoch 5 batch 983 loss: 1.9858460426330566\n",
      "epoch 5 batch 984 loss: 2.619279384613037\n",
      "epoch 5 batch 985 loss: 2.5700888633728027\n",
      "epoch 5 batch 986 loss: 2.4582409858703613\n",
      "epoch 5 batch 987 loss: 2.3081116676330566\n",
      "epoch 5 batch 988 loss: 2.4237260818481445\n",
      "epoch 5 batch 989 loss: 2.290593147277832\n",
      "epoch 5 batch 990 loss: 2.2358479499816895\n",
      "epoch 5 batch 991 loss: 2.1718900203704834\n",
      "epoch 5 batch 992 loss: 2.2585337162017822\n",
      "epoch 5 batch 993 loss: 2.339029312133789\n",
      "epoch 5 batch 994 loss: 2.4667444229125977\n",
      "epoch 5 batch 995 loss: 2.06349515914917\n",
      "epoch 5 batch 996 loss: 2.1419858932495117\n",
      "epoch 5 batch 997 loss: 2.170048713684082\n",
      "epoch 5 batch 998 loss: 2.3412251472473145\n",
      "epoch 5 batch 999 loss: 2.36733341217041\n",
      "epoch 5 batch 1000 loss: 2.245016574859619\n",
      "epoch 5 batch 1001 loss: 2.1158604621887207\n",
      "epoch 5 batch 1002 loss: 2.249878406524658\n",
      "epoch 5 batch 1003 loss: 2.207115888595581\n",
      "epoch 5 batch 1004 loss: 2.35941743850708\n",
      "epoch 5 batch 1005 loss: 2.3125596046447754\n",
      "epoch 5 batch 1006 loss: 2.2936344146728516\n",
      "epoch 5 batch 1007 loss: 2.26315975189209\n",
      "epoch 5 batch 1008 loss: 2.5431432723999023\n",
      "epoch 5 batch 1009 loss: 2.392134666442871\n",
      "epoch 5 batch 1010 loss: 2.1324493885040283\n",
      "epoch 5 batch 1011 loss: 2.2085652351379395\n",
      "epoch 5 batch 1012 loss: 2.453558921813965\n",
      "epoch 5 batch 1013 loss: 1.9426467418670654\n",
      "epoch 5 batch 1014 loss: 2.214630365371704\n",
      "epoch 5 batch 1015 loss: 2.1262452602386475\n",
      "epoch 5 batch 1016 loss: 2.2414636611938477\n",
      "epoch 5 batch 1017 loss: 2.1345949172973633\n",
      "epoch 5 batch 1018 loss: 2.4230082035064697\n",
      "epoch 5 batch 1019 loss: 2.3412563800811768\n",
      "epoch 5 batch 1020 loss: 2.4980006217956543\n",
      "epoch 5 batch 1021 loss: 2.348641872406006\n",
      "epoch 5 batch 1022 loss: 2.541250228881836\n",
      "epoch 5 batch 1023 loss: 2.2192254066467285\n",
      "epoch 5 batch 1024 loss: 2.2597718238830566\n",
      "epoch 5 batch 1025 loss: 2.301532745361328\n",
      "epoch 5 batch 1026 loss: 2.3254895210266113\n",
      "epoch 5 batch 1027 loss: 2.3101720809936523\n",
      "epoch 5 batch 1028 loss: 2.3130335807800293\n",
      "epoch 5 batch 1029 loss: 2.431796073913574\n",
      "epoch 5 batch 1030 loss: 2.2423079013824463\n",
      "epoch 5 batch 1031 loss: 2.0860700607299805\n",
      "epoch 5 batch 1032 loss: 2.534471035003662\n",
      "epoch 5 batch 1033 loss: 2.112138509750366\n",
      "epoch 5 batch 1034 loss: 2.333078622817993\n",
      "epoch 5 batch 1035 loss: 2.1962103843688965\n",
      "epoch 5 batch 1036 loss: 2.40818452835083\n",
      "epoch 5 batch 1037 loss: 2.190887928009033\n",
      "epoch 5 batch 1038 loss: 2.298835277557373\n",
      "epoch 5 batch 1039 loss: 2.197554111480713\n",
      "epoch 5 batch 1040 loss: 2.5070905685424805\n",
      "epoch 5 batch 1041 loss: 2.3006718158721924\n",
      "epoch 5 batch 1042 loss: 2.2450056076049805\n",
      "epoch 5 batch 1043 loss: 2.547832489013672\n",
      "epoch 5 batch 1044 loss: 2.2642874717712402\n",
      "epoch 5 batch 1045 loss: 2.3472492694854736\n",
      "epoch 5 batch 1046 loss: 2.0720956325531006\n",
      "epoch 5 batch 1047 loss: 2.369265556335449\n",
      "epoch 5 batch 1048 loss: 2.2773046493530273\n",
      "epoch 5 batch 1049 loss: 2.298335075378418\n",
      "epoch 5 batch 1050 loss: 2.351707935333252\n",
      "epoch 5 batch 1051 loss: 2.3642306327819824\n",
      "epoch 5 batch 1052 loss: 2.291374683380127\n",
      "epoch 5 batch 1053 loss: 2.5139689445495605\n",
      "epoch 5 batch 1054 loss: 2.27728533744812\n",
      "epoch 5 batch 1055 loss: 2.2195212841033936\n",
      "epoch 5 batch 1056 loss: 2.404038667678833\n",
      "epoch 5 batch 1057 loss: 2.419790506362915\n",
      "epoch 5 batch 1058 loss: 2.3552069664001465\n",
      "epoch 5 batch 1059 loss: 2.3053417205810547\n",
      "epoch 5 batch 1060 loss: 2.160438060760498\n",
      "epoch 5 batch 1061 loss: 2.201399326324463\n",
      "epoch 5 batch 1062 loss: 2.2524821758270264\n",
      "epoch 5 batch 1063 loss: 2.1674487590789795\n",
      "epoch 5 batch 1064 loss: 2.496461868286133\n",
      "epoch 5 batch 1065 loss: 2.209319829940796\n",
      "epoch 5 batch 1066 loss: 2.033416748046875\n",
      "epoch 5 batch 1067 loss: 2.246624231338501\n",
      "epoch 5 batch 1068 loss: 2.423175573348999\n",
      "epoch 5 batch 1069 loss: 2.2317094802856445\n",
      "epoch 5 batch 1070 loss: 2.3812570571899414\n",
      "epoch 5 batch 1071 loss: 2.336430549621582\n",
      "epoch 5 batch 1072 loss: 2.314465045928955\n",
      "epoch 5 batch 1073 loss: 2.1783969402313232\n",
      "epoch 5 batch 1074 loss: 2.5429701805114746\n",
      "epoch 5 batch 1075 loss: 2.165189743041992\n",
      "epoch 5 batch 1076 loss: 2.4161062240600586\n",
      "epoch 5 batch 1077 loss: 2.3833746910095215\n",
      "epoch 5 batch 1078 loss: 2.133340835571289\n",
      "epoch 5 batch 1079 loss: 2.4678914546966553\n",
      "epoch 5 batch 1080 loss: 2.3575990200042725\n",
      "epoch 5 batch 1081 loss: 2.307919502258301\n",
      "epoch 5 batch 1082 loss: 2.143282175064087\n",
      "epoch 5 batch 1083 loss: 2.3410556316375732\n",
      "epoch 5 batch 1084 loss: 2.3649165630340576\n",
      "epoch 5 batch 1085 loss: 2.094456434249878\n",
      "epoch 5 batch 1086 loss: 2.369492530822754\n",
      "epoch 5 batch 1087 loss: 2.4640963077545166\n",
      "epoch 5 batch 1088 loss: 1.9621403217315674\n",
      "epoch 5 batch 1089 loss: 2.333972692489624\n",
      "epoch 5 batch 1090 loss: 2.4374876022338867\n",
      "epoch 5 batch 1091 loss: 2.274857521057129\n",
      "epoch 5 batch 1092 loss: 2.3745713233947754\n",
      "epoch 5 batch 1093 loss: 2.6780831813812256\n",
      "epoch 5 batch 1094 loss: 2.1971752643585205\n",
      "epoch 5 batch 1095 loss: 2.236783742904663\n",
      "epoch 5 batch 1096 loss: 2.211304187774658\n",
      "epoch 5 batch 1097 loss: 2.2829935550689697\n",
      "epoch 5 batch 1098 loss: 2.4828896522521973\n",
      "epoch 5 batch 1099 loss: 2.20845890045166\n",
      "epoch 5 batch 1100 loss: 2.430717945098877\n",
      "epoch 5 batch 1101 loss: 2.3149938583374023\n",
      "epoch 5 batch 1102 loss: 2.4578254222869873\n",
      "epoch 5 batch 1103 loss: 2.2528347969055176\n",
      "epoch 5 batch 1104 loss: 2.1935877799987793\n",
      "epoch 5 batch 1105 loss: 2.0061097145080566\n",
      "epoch 5 batch 1106 loss: 2.319888114929199\n",
      "epoch 5 batch 1107 loss: 2.3294668197631836\n",
      "epoch 5 batch 1108 loss: 2.2684221267700195\n",
      "epoch 5 batch 1109 loss: 2.097540855407715\n",
      "epoch 5 batch 1110 loss: 2.3591203689575195\n",
      "epoch 5 batch 1111 loss: 2.1472110748291016\n",
      "epoch 5 batch 1112 loss: 2.496612548828125\n",
      "epoch 5 batch 1113 loss: 2.4885218143463135\n",
      "epoch 5 batch 1114 loss: 2.56498646736145\n",
      "epoch 5 batch 1115 loss: 2.269761085510254\n",
      "epoch 5 batch 1116 loss: 2.1683645248413086\n",
      "epoch 5 batch 1117 loss: 2.142119884490967\n",
      "epoch 5 batch 1118 loss: 2.307398557662964\n",
      "epoch 5 batch 1119 loss: 2.5325160026550293\n",
      "epoch 5 batch 1120 loss: 2.4858715534210205\n",
      "epoch 5 batch 1121 loss: 2.3204054832458496\n",
      "epoch 5 batch 1122 loss: 2.3855319023132324\n",
      "epoch 5 batch 1123 loss: 2.4193520545959473\n",
      "epoch 5 batch 1124 loss: 2.2277631759643555\n",
      "epoch 5 batch 1125 loss: 2.082059621810913\n",
      "epoch 5 batch 1126 loss: 2.174999475479126\n",
      "epoch 5 batch 1127 loss: 2.1965866088867188\n",
      "epoch 5 batch 1128 loss: 2.5458059310913086\n",
      "epoch 5 batch 1129 loss: 2.2097344398498535\n",
      "epoch 5 batch 1130 loss: 2.333339214324951\n",
      "epoch 5 batch 1131 loss: 2.251387119293213\n",
      "epoch 5 batch 1132 loss: 2.422403335571289\n",
      "epoch 5 batch 1133 loss: 2.077794075012207\n",
      "epoch 5 batch 1134 loss: 2.5321731567382812\n",
      "epoch 5 batch 1135 loss: 2.3000078201293945\n",
      "epoch 5 batch 1136 loss: 2.132006883621216\n",
      "epoch 5 batch 1137 loss: 2.276533603668213\n",
      "epoch 5 batch 1138 loss: 2.344511032104492\n",
      "epoch 5 batch 1139 loss: 2.298102378845215\n",
      "epoch 5 batch 1140 loss: 2.3650107383728027\n",
      "epoch 5 batch 1141 loss: 2.163552761077881\n",
      "epoch 5 batch 1142 loss: 2.0465707778930664\n",
      "epoch 5 batch 1143 loss: 2.380033016204834\n",
      "epoch 5 batch 1144 loss: 2.263917922973633\n",
      "epoch 5 batch 1145 loss: 2.3329315185546875\n",
      "epoch 5 batch 1146 loss: 2.44946551322937\n",
      "epoch 5 batch 1147 loss: 2.4054317474365234\n",
      "epoch 5 batch 1148 loss: 2.2940471172332764\n",
      "epoch 5 batch 1149 loss: 2.0890445709228516\n",
      "epoch 5 batch 1150 loss: 2.28714656829834\n",
      "epoch 5 batch 1151 loss: 2.3989782333374023\n",
      "epoch 5 batch 1152 loss: 2.5541226863861084\n",
      "epoch 5 batch 1153 loss: 2.218663215637207\n",
      "epoch 5 batch 1154 loss: 2.2220938205718994\n",
      "epoch 5 batch 1155 loss: 2.6268773078918457\n",
      "epoch 5 batch 1156 loss: 2.396432399749756\n",
      "epoch 5 batch 1157 loss: 2.1259968280792236\n",
      "epoch 5 batch 1158 loss: 2.370242118835449\n",
      "epoch 5 batch 1159 loss: 2.435722827911377\n",
      "epoch 5 batch 1160 loss: 2.0785765647888184\n",
      "epoch 5 batch 1161 loss: 2.4484915733337402\n",
      "epoch 5 batch 1162 loss: 2.2999954223632812\n",
      "epoch 5 batch 1163 loss: 2.4835543632507324\n",
      "epoch 5 batch 1164 loss: 2.1977286338806152\n",
      "epoch 5 batch 1165 loss: 2.293459177017212\n",
      "epoch 5 batch 1166 loss: 2.419506549835205\n",
      "epoch 5 batch 1167 loss: 2.4798154830932617\n",
      "epoch 5 batch 1168 loss: 2.2061352729797363\n",
      "epoch 5 batch 1169 loss: 2.221259355545044\n",
      "epoch 5 batch 1170 loss: 2.3144586086273193\n",
      "epoch 5 batch 1171 loss: 2.3656036853790283\n",
      "epoch 5 batch 1172 loss: 2.1764960289001465\n",
      "epoch 5 batch 1173 loss: 2.1347084045410156\n",
      "epoch 5 batch 1174 loss: 2.36014986038208\n",
      "epoch 5 batch 1175 loss: 2.3829073905944824\n",
      "epoch 5 batch 1176 loss: 2.3380939960479736\n",
      "epoch 5 batch 1177 loss: 2.3723926544189453\n",
      "epoch 5 batch 1178 loss: 2.3075509071350098\n",
      "epoch 5 batch 1179 loss: 2.7874560356140137\n",
      "epoch 5 batch 1180 loss: 2.429901599884033\n",
      "epoch 5 batch 1181 loss: 2.122748374938965\n",
      "epoch 5 batch 1182 loss: 2.451251745223999\n",
      "epoch 5 batch 1183 loss: 2.4071598052978516\n",
      "epoch 5 batch 1184 loss: 2.142195224761963\n",
      "epoch 5 batch 1185 loss: 2.282198905944824\n",
      "epoch 5 batch 1186 loss: 2.4057834148406982\n",
      "epoch 5 batch 1187 loss: 2.1874828338623047\n",
      "epoch 5 batch 1188 loss: 2.3985776901245117\n",
      "epoch 5 batch 1189 loss: 2.139525890350342\n",
      "epoch 5 batch 1190 loss: 2.414714813232422\n",
      "epoch 5 batch 1191 loss: 2.2643613815307617\n",
      "epoch 5 batch 1192 loss: 2.461883068084717\n",
      "epoch 5 batch 1193 loss: 2.1323060989379883\n",
      "epoch 5 batch 1194 loss: 2.2222375869750977\n",
      "epoch 5 batch 1195 loss: 2.291937828063965\n",
      "epoch 5 batch 1196 loss: 2.5448832511901855\n",
      "epoch 5 batch 1197 loss: 2.1959798336029053\n",
      "epoch 5 batch 1198 loss: 2.4711830615997314\n",
      "epoch 5 batch 1199 loss: 2.445237636566162\n",
      "epoch 5 batch 1200 loss: 2.403101921081543\n",
      "epoch 5 batch 1201 loss: 2.393435478210449\n",
      "epoch 5 batch 1202 loss: 2.1228630542755127\n",
      "epoch 5 batch 1203 loss: 2.0682878494262695\n",
      "epoch 5 batch 1204 loss: 2.2583415508270264\n",
      "epoch 5 batch 1205 loss: 2.4140124320983887\n",
      "epoch 5 batch 1206 loss: 2.4382593631744385\n",
      "epoch 5 batch 1207 loss: 2.235039710998535\n",
      "epoch 5 batch 1208 loss: 2.2271695137023926\n",
      "epoch 5 batch 1209 loss: 2.1206488609313965\n",
      "epoch 5 batch 1210 loss: 2.317152500152588\n",
      "epoch 5 batch 1211 loss: 2.3858447074890137\n",
      "epoch 5 batch 1212 loss: 2.0435688495635986\n",
      "epoch 5 batch 1213 loss: 2.4222960472106934\n",
      "epoch 5 batch 1214 loss: 2.3212034702301025\n",
      "epoch 5 batch 1215 loss: 2.0811548233032227\n",
      "epoch 5 batch 1216 loss: 2.389108180999756\n",
      "epoch 5 batch 1217 loss: 2.3982229232788086\n",
      "epoch 5 batch 1218 loss: 2.5774788856506348\n",
      "epoch 5 batch 1219 loss: 2.3179359436035156\n",
      "epoch 5 batch 1220 loss: 2.0071868896484375\n",
      "epoch 5 batch 1221 loss: 2.1237597465515137\n",
      "epoch 5 batch 1222 loss: 2.388101577758789\n",
      "epoch 5 batch 1223 loss: 2.326728582382202\n",
      "epoch 5 batch 1224 loss: 2.4450085163116455\n",
      "epoch 5 batch 1225 loss: 2.3050458431243896\n",
      "epoch 5 batch 1226 loss: 2.3104519844055176\n",
      "epoch 5 batch 1227 loss: 2.3233468532562256\n",
      "epoch 5 batch 1228 loss: 2.206660747528076\n",
      "epoch 5 batch 1229 loss: 2.1546471118927\n",
      "epoch 5 batch 1230 loss: 2.353048801422119\n",
      "epoch 5 batch 1231 loss: 2.417372941970825\n",
      "epoch 5 batch 1232 loss: 2.4274299144744873\n",
      "epoch 5 batch 1233 loss: 2.162097215652466\n",
      "epoch 5 batch 1234 loss: 2.2929158210754395\n",
      "epoch 5 batch 1235 loss: 2.3844525814056396\n",
      "epoch 5 batch 1236 loss: 2.3116636276245117\n",
      "epoch 5 batch 1237 loss: 2.1601996421813965\n",
      "epoch 5 batch 1238 loss: 2.1656434535980225\n",
      "epoch 5 batch 1239 loss: 2.268455982208252\n",
      "epoch 5 batch 1240 loss: 2.1904079914093018\n",
      "epoch 5 batch 1241 loss: 2.134850263595581\n",
      "epoch 5 batch 1242 loss: 2.4456048011779785\n",
      "epoch 5 batch 1243 loss: 2.0503110885620117\n",
      "epoch 5 batch 1244 loss: 2.160916328430176\n",
      "epoch 5 batch 1245 loss: 2.302933692932129\n",
      "epoch 5 batch 1246 loss: 2.529407501220703\n",
      "epoch 5 batch 1247 loss: 2.3429384231567383\n",
      "epoch 5 batch 1248 loss: 2.208315372467041\n",
      "epoch 5 batch 1249 loss: 2.298712730407715\n",
      "epoch 5 batch 1250 loss: 2.68477463722229\n",
      "epoch 5 batch 1251 loss: 2.2903013229370117\n",
      "epoch 5 batch 1252 loss: 2.249223470687866\n",
      "epoch 5 batch 1253 loss: 2.3414766788482666\n",
      "epoch 5 batch 1254 loss: 2.241849184036255\n",
      "epoch 5 batch 1255 loss: 2.131430149078369\n",
      "epoch 5 batch 1256 loss: 2.2109899520874023\n",
      "epoch 5 batch 1257 loss: 2.5689282417297363\n",
      "epoch 5 batch 1258 loss: 2.239576816558838\n",
      "epoch 5 batch 1259 loss: 2.1569948196411133\n",
      "epoch 5 batch 1260 loss: 2.1764137744903564\n",
      "epoch 5 batch 1261 loss: 2.227623701095581\n",
      "epoch 5 batch 1262 loss: 2.2066380977630615\n",
      "epoch 5 batch 1263 loss: 2.314769983291626\n",
      "epoch 5 batch 1264 loss: 2.1606385707855225\n",
      "epoch 5 batch 1265 loss: 2.176168441772461\n",
      "epoch 5 batch 1266 loss: 2.3987679481506348\n",
      "epoch 5 batch 1267 loss: 2.4516873359680176\n",
      "epoch 5 batch 1268 loss: 2.3218884468078613\n",
      "epoch 5 batch 1269 loss: 2.3714852333068848\n",
      "epoch 5 batch 1270 loss: 2.3002841472625732\n",
      "epoch 5 batch 1271 loss: 2.358914852142334\n",
      "epoch 5 batch 1272 loss: 2.2702481746673584\n",
      "epoch 5 batch 1273 loss: 2.2961761951446533\n",
      "epoch 5 batch 1274 loss: 2.074918746948242\n",
      "epoch 5 batch 1275 loss: 2.1797611713409424\n",
      "epoch 5 batch 1276 loss: 2.449444055557251\n",
      "epoch 5 batch 1277 loss: 2.20194673538208\n",
      "epoch 5 batch 1278 loss: 2.101301908493042\n",
      "epoch 5 batch 1279 loss: 2.223717212677002\n",
      "epoch 5 batch 1280 loss: 2.2561471462249756\n",
      "epoch 5 batch 1281 loss: 2.101370096206665\n",
      "epoch 5 batch 1282 loss: 2.3687634468078613\n",
      "epoch 5 batch 1283 loss: 2.1491827964782715\n",
      "epoch 5 batch 1284 loss: 2.3149499893188477\n",
      "epoch 5 batch 1285 loss: 2.055633544921875\n",
      "epoch 5 batch 1286 loss: 2.1933276653289795\n",
      "epoch 5 batch 1287 loss: 2.3193752765655518\n",
      "epoch 5 batch 1288 loss: 2.2652766704559326\n",
      "epoch 5 batch 1289 loss: 2.3059616088867188\n",
      "epoch 5 batch 1290 loss: 2.1283326148986816\n",
      "epoch 5 batch 1291 loss: 2.2693839073181152\n",
      "epoch 5 batch 1292 loss: 2.303773880004883\n",
      "epoch 5 batch 1293 loss: 2.560059070587158\n",
      "epoch 5 batch 1294 loss: 2.2634174823760986\n",
      "epoch 5 batch 1295 loss: 2.3507699966430664\n",
      "epoch 5 batch 1296 loss: 2.267549991607666\n",
      "epoch 5 batch 1297 loss: 2.3062007427215576\n",
      "epoch 5 batch 1298 loss: 2.2009997367858887\n",
      "epoch 5 batch 1299 loss: 2.1031906604766846\n",
      "epoch 5 batch 1300 loss: 2.3489484786987305\n",
      "epoch 5 batch 1301 loss: 2.1696743965148926\n",
      "epoch 5 batch 1302 loss: 2.4045021533966064\n",
      "epoch 5 batch 1303 loss: 2.5046446323394775\n",
      "epoch 5 batch 1304 loss: 2.1017112731933594\n",
      "epoch 5 batch 1305 loss: 2.069859504699707\n",
      "epoch 5 batch 1306 loss: 2.179858684539795\n",
      "epoch 5 batch 1307 loss: 2.219888210296631\n",
      "epoch 5 batch 1308 loss: 2.319364309310913\n",
      "epoch 5 batch 1309 loss: 2.5101447105407715\n",
      "epoch 5 batch 1310 loss: 2.2393338680267334\n",
      "epoch 5 batch 1311 loss: 2.240189552307129\n",
      "epoch 5 batch 1312 loss: 2.111354112625122\n",
      "epoch 5 batch 1313 loss: 2.367513656616211\n",
      "epoch 5 batch 1314 loss: 2.3678574562072754\n",
      "epoch 5 batch 1315 loss: 2.1885416507720947\n",
      "epoch 5 batch 1316 loss: 2.2638418674468994\n",
      "epoch 5 batch 1317 loss: 2.1421942710876465\n",
      "epoch 5 batch 1318 loss: 2.243358612060547\n",
      "epoch 5 batch 1319 loss: 2.274860382080078\n",
      "epoch 5 batch 1320 loss: 2.4437336921691895\n",
      "epoch 5 batch 1321 loss: 2.386218547821045\n",
      "epoch 5 batch 1322 loss: 2.335495948791504\n",
      "epoch 5 batch 1323 loss: 2.0424387454986572\n",
      "epoch 5 batch 1324 loss: 2.384746551513672\n",
      "epoch 5 batch 1325 loss: 2.310685157775879\n",
      "epoch 5 batch 1326 loss: 2.399714708328247\n",
      "epoch 5 batch 1327 loss: 2.1901302337646484\n",
      "epoch 5 batch 1328 loss: 2.2038965225219727\n",
      "epoch 5 batch 1329 loss: 2.1615962982177734\n",
      "epoch 5 batch 1330 loss: 2.3291802406311035\n",
      "epoch 5 batch 1331 loss: 2.4321937561035156\n",
      "epoch 5 batch 1332 loss: 2.214245319366455\n",
      "epoch 5 batch 1333 loss: 2.1515252590179443\n",
      "epoch 5 batch 1334 loss: 2.4697012901306152\n",
      "epoch 5 batch 1335 loss: 2.4036309719085693\n",
      "epoch 5 batch 1336 loss: 2.242058038711548\n",
      "epoch 5 batch 1337 loss: 2.3299500942230225\n",
      "epoch 5 batch 1338 loss: 2.2505555152893066\n",
      "epoch 5 batch 1339 loss: 2.16373872756958\n",
      "epoch 5 batch 1340 loss: 2.4972176551818848\n",
      "epoch 5 batch 1341 loss: 2.1836507320404053\n",
      "epoch 5 batch 1342 loss: 2.2322463989257812\n",
      "epoch 5 batch 1343 loss: 2.216315746307373\n",
      "epoch 5 batch 1344 loss: 1.9531185626983643\n",
      "epoch 5 batch 1345 loss: 2.15291690826416\n",
      "epoch 5 batch 1346 loss: 2.169137477874756\n",
      "epoch 5 batch 1347 loss: 2.3145127296447754\n",
      "epoch 5 batch 1348 loss: 2.2722115516662598\n",
      "epoch 5 batch 1349 loss: 2.3877124786376953\n",
      "epoch 5 batch 1350 loss: 2.258866548538208\n",
      "epoch 5 batch 1351 loss: 2.5453977584838867\n",
      "epoch 5 batch 1352 loss: 2.269970417022705\n",
      "epoch 5 batch 1353 loss: 2.5013961791992188\n",
      "epoch 5 batch 1354 loss: 2.218583583831787\n",
      "epoch 5 batch 1355 loss: 2.2083685398101807\n",
      "epoch 5 batch 1356 loss: 2.2136683464050293\n",
      "epoch 5 batch 1357 loss: 2.264449119567871\n",
      "epoch 5 batch 1358 loss: 2.3878262042999268\n",
      "epoch 5 batch 1359 loss: 2.296496868133545\n",
      "epoch 5 batch 1360 loss: 2.542212963104248\n",
      "epoch 5 batch 1361 loss: 2.48384952545166\n",
      "epoch 5 batch 1362 loss: 2.583408832550049\n",
      "epoch 5 batch 1363 loss: 2.2655229568481445\n",
      "epoch 5 batch 1364 loss: 2.4130773544311523\n",
      "epoch 5 batch 1365 loss: 2.222843647003174\n",
      "epoch 5 batch 1366 loss: 2.4561429023742676\n",
      "epoch 5 batch 1367 loss: 2.39326810836792\n",
      "epoch 5 batch 1368 loss: 2.06559419631958\n",
      "epoch 5 batch 1369 loss: 2.082551956176758\n",
      "epoch 5 batch 1370 loss: 2.4399795532226562\n",
      "epoch 5 batch 1371 loss: 2.497957229614258\n",
      "epoch 5 batch 1372 loss: 2.2470688819885254\n",
      "epoch 5 batch 1373 loss: 2.321049690246582\n",
      "epoch 5 batch 1374 loss: 2.2172322273254395\n",
      "epoch 5 batch 1375 loss: 2.2583184242248535\n",
      "epoch 5 batch 1376 loss: 1.9977351427078247\n",
      "epoch 5 batch 1377 loss: 2.1192092895507812\n",
      "epoch 5 batch 1378 loss: 2.656980514526367\n",
      "epoch 5 batch 1379 loss: 2.1714930534362793\n",
      "epoch 5 batch 1380 loss: 2.2306528091430664\n",
      "epoch 5 batch 1381 loss: 2.1761903762817383\n",
      "epoch 5 batch 1382 loss: 2.2249834537506104\n",
      "epoch 5 batch 1383 loss: 2.0670664310455322\n",
      "epoch 5 batch 1384 loss: 2.2510464191436768\n",
      "epoch 5 batch 1385 loss: 2.122385263442993\n",
      "epoch 5 batch 1386 loss: 2.317439079284668\n",
      "epoch 5 batch 1387 loss: 2.322028398513794\n",
      "epoch 5 batch 1388 loss: 2.343540668487549\n",
      "epoch 5 batch 1389 loss: 2.1645865440368652\n",
      "epoch 5 batch 1390 loss: 2.1670093536376953\n",
      "epoch 5 batch 1391 loss: 2.383355140686035\n",
      "epoch 5 batch 1392 loss: 2.3769164085388184\n",
      "epoch 5 batch 1393 loss: 2.2592225074768066\n",
      "epoch 5 batch 1394 loss: 2.372966766357422\n",
      "epoch 5 batch 1395 loss: 2.2061562538146973\n",
      "epoch 5 batch 1396 loss: 2.204951524734497\n",
      "epoch 5 batch 1397 loss: 2.4669899940490723\n",
      "epoch 5 batch 1398 loss: 2.232914447784424\n",
      "epoch 5 batch 1399 loss: 2.475687026977539\n",
      "epoch 5 batch 1400 loss: 2.057096481323242\n",
      "epoch 5 batch 1401 loss: 2.4289474487304688\n",
      "epoch 5 batch 1402 loss: 2.382755994796753\n",
      "epoch 5 batch 1403 loss: 2.2211194038391113\n",
      "epoch 5 batch 1404 loss: 2.352926731109619\n",
      "epoch 5 batch 1405 loss: 2.3644039630889893\n",
      "epoch 5 batch 1406 loss: 2.0674843788146973\n",
      "epoch 5 batch 1407 loss: 2.3410353660583496\n",
      "epoch 5 batch 1408 loss: 2.2879977226257324\n",
      "epoch 5 batch 1409 loss: 2.1692323684692383\n",
      "epoch 5 batch 1410 loss: 2.3380327224731445\n",
      "epoch 5 batch 1411 loss: 2.2134616374969482\n",
      "epoch 5 batch 1412 loss: 2.6949193477630615\n",
      "epoch 5 batch 1413 loss: 2.2707529067993164\n",
      "epoch 5 batch 1414 loss: 2.2390859127044678\n",
      "epoch 5 batch 1415 loss: 2.3090224266052246\n",
      "epoch 5 batch 1416 loss: 2.2126646041870117\n",
      "epoch 5 batch 1417 loss: 2.6016182899475098\n",
      "epoch 5 batch 1418 loss: 2.222881317138672\n",
      "epoch 5 batch 1419 loss: 2.1494569778442383\n",
      "epoch 5 batch 1420 loss: 2.363852024078369\n",
      "epoch 5 batch 1421 loss: 2.1539676189422607\n",
      "epoch 5 batch 1422 loss: 2.189572811126709\n",
      "epoch 5 batch 1423 loss: 2.095365524291992\n",
      "epoch 5 batch 1424 loss: 2.258636713027954\n",
      "epoch 5 batch 1425 loss: 2.5013206005096436\n",
      "epoch 5 batch 1426 loss: 2.2058520317077637\n",
      "epoch 5 batch 1427 loss: 2.3552942276000977\n",
      "epoch 5 batch 1428 loss: 2.328460216522217\n",
      "epoch 5 batch 1429 loss: 2.1865522861480713\n",
      "epoch 5 batch 1430 loss: 2.0969505310058594\n",
      "epoch 5 batch 1431 loss: 2.265746593475342\n",
      "epoch 5 batch 1432 loss: 2.1061346530914307\n",
      "epoch 5 batch 1433 loss: 2.2164134979248047\n",
      "epoch 5 batch 1434 loss: 2.388338088989258\n",
      "epoch 5 batch 1435 loss: 2.4940428733825684\n",
      "epoch 5 batch 1436 loss: 2.4752182960510254\n",
      "epoch 5 batch 1437 loss: 2.5772855281829834\n",
      "epoch 5 batch 1438 loss: 2.3082833290100098\n",
      "epoch 5 batch 1439 loss: 2.230762004852295\n",
      "epoch 5 batch 1440 loss: 2.1013288497924805\n",
      "epoch 5 batch 1441 loss: 2.1825318336486816\n",
      "epoch 5 batch 1442 loss: 2.288601875305176\n",
      "epoch 5 batch 1443 loss: 2.2501730918884277\n",
      "epoch 5 batch 1444 loss: 2.2439398765563965\n",
      "epoch 5 batch 1445 loss: 2.484463691711426\n",
      "epoch 5 batch 1446 loss: 2.434567451477051\n",
      "epoch 5 batch 1447 loss: 2.36798095703125\n",
      "epoch 5 batch 1448 loss: 2.1287484169006348\n",
      "epoch 5 batch 1449 loss: 2.2934415340423584\n",
      "epoch 5 batch 1450 loss: 2.26430082321167\n",
      "epoch 5 batch 1451 loss: 2.7675232887268066\n",
      "epoch 5 batch 1452 loss: 1.9898827075958252\n",
      "epoch 5 batch 1453 loss: 2.3324289321899414\n",
      "epoch 5 batch 1454 loss: 2.1565017700195312\n",
      "epoch 5 batch 1455 loss: 2.353236675262451\n",
      "epoch 5 batch 1456 loss: 2.0882396697998047\n",
      "epoch 5 batch 1457 loss: 2.267620086669922\n",
      "epoch 5 batch 1458 loss: 2.3702926635742188\n",
      "epoch 5 batch 1459 loss: 2.294123888015747\n",
      "epoch 5 batch 1460 loss: 2.032440662384033\n",
      "epoch 5 batch 1461 loss: 2.4426398277282715\n",
      "epoch 5 batch 1462 loss: 2.3354554176330566\n",
      "epoch 5 batch 1463 loss: 2.1229443550109863\n",
      "epoch 5 batch 1464 loss: 2.30889630317688\n",
      "epoch 5 batch 1465 loss: 2.1933584213256836\n",
      "epoch 5 batch 1466 loss: 2.5239129066467285\n",
      "epoch 5 batch 1467 loss: 2.3644118309020996\n",
      "epoch 5 batch 1468 loss: 2.4187521934509277\n",
      "epoch 5 batch 1469 loss: 2.6296963691711426\n",
      "epoch 5 batch 1470 loss: 2.4759626388549805\n",
      "epoch 5 batch 1471 loss: 2.221433162689209\n",
      "epoch 5 batch 1472 loss: 2.192887783050537\n",
      "epoch 5 batch 1473 loss: 2.390080690383911\n",
      "epoch 5 batch 1474 loss: 2.146162986755371\n",
      "epoch 5 batch 1475 loss: 2.368565559387207\n",
      "epoch 5 batch 1476 loss: 2.105727434158325\n",
      "epoch 5 batch 1477 loss: 2.1469802856445312\n",
      "epoch 5 batch 1478 loss: 2.2461729049682617\n",
      "epoch 5 batch 1479 loss: 2.1136012077331543\n",
      "epoch 5 batch 1480 loss: 2.4124133586883545\n",
      "epoch 5 batch 1481 loss: 2.418694257736206\n",
      "epoch 5 batch 1482 loss: 2.293210744857788\n",
      "epoch 5 batch 1483 loss: 2.437448501586914\n",
      "epoch 5 batch 1484 loss: 2.196404218673706\n",
      "epoch 5 batch 1485 loss: 2.2531800270080566\n",
      "epoch 5 batch 1486 loss: 2.3262553215026855\n",
      "epoch 5 batch 1487 loss: 2.4563522338867188\n",
      "epoch 5 batch 1488 loss: 2.362654685974121\n",
      "epoch 5 batch 1489 loss: 2.4489102363586426\n",
      "epoch 5 batch 1490 loss: 2.230104446411133\n",
      "epoch 5 batch 1491 loss: 2.3394691944122314\n",
      "epoch 5 batch 1492 loss: 2.4157893657684326\n",
      "epoch 5 batch 1493 loss: 2.229180335998535\n",
      "epoch 5 batch 1494 loss: 2.0968017578125\n",
      "epoch 5 batch 1495 loss: 2.3984036445617676\n",
      "epoch 5 batch 1496 loss: 2.1821136474609375\n",
      "epoch 5 batch 1497 loss: 2.337758779525757\n",
      "epoch 5 batch 1498 loss: 2.26891827583313\n",
      "epoch 5 batch 1499 loss: 2.2929487228393555\n",
      "epoch 5 batch 1500 loss: 2.2393178939819336\n",
      "epoch 5 batch 1501 loss: 2.292541265487671\n",
      "epoch 5 batch 1502 loss: 2.145834445953369\n",
      "epoch 5 batch 1503 loss: 2.239971160888672\n",
      "epoch 5 batch 1504 loss: 2.4322707653045654\n",
      "epoch 5 batch 1505 loss: 2.2750043869018555\n",
      "epoch 5 batch 1506 loss: 2.4998250007629395\n",
      "epoch 5 batch 1507 loss: 2.2298192977905273\n",
      "epoch 5 batch 1508 loss: 2.3287200927734375\n",
      "epoch 5 batch 1509 loss: 2.5500259399414062\n",
      "epoch 5 batch 1510 loss: 2.3574280738830566\n",
      "epoch 5 batch 1511 loss: 2.114535331726074\n",
      "epoch 5 batch 1512 loss: 2.2227783203125\n",
      "epoch 5 batch 1513 loss: 2.3242902755737305\n",
      "epoch 5 batch 1514 loss: 2.2906534671783447\n",
      "epoch 5 batch 1515 loss: 2.4425158500671387\n",
      "epoch 5 batch 1516 loss: 2.2274646759033203\n",
      "epoch 5 batch 1517 loss: 2.354701519012451\n",
      "epoch 5 batch 1518 loss: 2.327052116394043\n",
      "epoch 5 batch 1519 loss: 2.272287607192993\n",
      "epoch 5 batch 1520 loss: 2.4514055252075195\n",
      "epoch 5 batch 1521 loss: 2.107017755508423\n",
      "epoch 5 batch 1522 loss: 1.9779410362243652\n",
      "epoch 5 batch 1523 loss: 2.250492572784424\n",
      "epoch 5 batch 1524 loss: 2.298245429992676\n",
      "epoch 5 batch 1525 loss: 2.1090400218963623\n",
      "epoch 5 batch 1526 loss: 2.418355941772461\n",
      "epoch 5 batch 1527 loss: 2.246041774749756\n",
      "epoch 5 batch 1528 loss: 2.0416431427001953\n",
      "epoch 5 batch 1529 loss: 2.3289713859558105\n",
      "epoch 5 batch 1530 loss: 2.138294219970703\n",
      "epoch 5 batch 1531 loss: 2.343127489089966\n",
      "epoch 5 batch 1532 loss: 2.3210291862487793\n",
      "epoch 5 batch 1533 loss: 2.3017656803131104\n",
      "epoch 5 batch 1534 loss: 2.430105686187744\n",
      "epoch 5 batch 1535 loss: 2.481627941131592\n",
      "epoch 5 batch 1536 loss: 2.383373737335205\n",
      "epoch 5 batch 1537 loss: 2.3647520542144775\n",
      "epoch 5 batch 1538 loss: 2.3030853271484375\n",
      "epoch 5 batch 1539 loss: 2.329820156097412\n",
      "epoch 5 batch 1540 loss: 2.3223659992218018\n",
      "epoch 5 batch 1541 loss: 2.5610833168029785\n",
      "epoch 5 batch 1542 loss: 2.512589931488037\n",
      "epoch 5 batch 1543 loss: 2.077521324157715\n",
      "epoch 5 batch 1544 loss: 2.177842617034912\n",
      "epoch 5 batch 1545 loss: 2.463160991668701\n",
      "epoch 5 batch 1546 loss: 2.3722026348114014\n",
      "epoch 5 batch 1547 loss: 2.334867000579834\n",
      "epoch 5 batch 1548 loss: 2.2241175174713135\n",
      "epoch 5 batch 1549 loss: 2.1535468101501465\n",
      "epoch 5 batch 1550 loss: 2.2579092979431152\n",
      "epoch 5 batch 1551 loss: 2.2891881465911865\n",
      "epoch 5 batch 1552 loss: 2.573197841644287\n",
      "epoch 5 batch 1553 loss: 2.396731376647949\n",
      "epoch 5 batch 1554 loss: 2.3405678272247314\n",
      "epoch 5 batch 1555 loss: 2.2981619834899902\n",
      "epoch 5 batch 1556 loss: 2.4999783039093018\n",
      "epoch 5 batch 1557 loss: 2.118727684020996\n",
      "epoch 5 batch 1558 loss: 2.233734607696533\n",
      "epoch 5 batch 1559 loss: 2.1930489540100098\n",
      "epoch 5 batch 1560 loss: 2.1486735343933105\n",
      "epoch 5 batch 1561 loss: 2.305598258972168\n",
      "epoch 5 batch 1562 loss: 2.234722137451172\n",
      "epoch 5 batch 1563 loss: 2.0995430946350098\n",
      "epoch 5 batch 1564 loss: 2.11980938911438\n",
      "epoch 5 batch 1565 loss: 2.3006558418273926\n",
      "epoch 5 batch 1566 loss: 2.5286993980407715\n",
      "epoch 5 batch 1567 loss: 2.1424660682678223\n",
      "epoch 5 batch 1568 loss: 2.4470629692077637\n",
      "epoch 5 batch 1569 loss: 2.374422550201416\n",
      "epoch 5 batch 1570 loss: 2.181267023086548\n",
      "epoch 5 batch 1571 loss: 2.222259998321533\n",
      "epoch 5 batch 1572 loss: 2.3969736099243164\n",
      "epoch 5 batch 1573 loss: 2.399143934249878\n",
      "epoch 5 batch 1574 loss: 2.388378620147705\n",
      "epoch 5 batch 1575 loss: 2.3186559677124023\n",
      "epoch 5 batch 1576 loss: 2.156879425048828\n",
      "epoch 5 batch 1577 loss: 2.2976887226104736\n",
      "epoch 5 batch 1578 loss: 2.082669973373413\n",
      "epoch 5 batch 1579 loss: 2.2066001892089844\n",
      "epoch 5 batch 1580 loss: 2.238386631011963\n",
      "epoch 5 batch 1581 loss: 2.4753565788269043\n",
      "epoch 5 batch 1582 loss: 2.4814562797546387\n",
      "epoch 5 batch 1583 loss: 2.1280808448791504\n",
      "epoch 5 batch 1584 loss: 2.3358349800109863\n",
      "epoch 5 batch 1585 loss: 2.112846851348877\n",
      "epoch 5 batch 1586 loss: 2.181335210800171\n",
      "epoch 5 batch 1587 loss: 2.1007752418518066\n",
      "epoch 5 batch 1588 loss: 2.7415452003479004\n",
      "epoch 5 batch 1589 loss: 2.1835315227508545\n",
      "epoch 5 batch 1590 loss: 2.4871933460235596\n",
      "epoch 5 batch 1591 loss: 2.9239935874938965\n",
      "epoch 5 batch 1592 loss: 2.3894472122192383\n",
      "epoch 5 batch 1593 loss: 2.3910069465637207\n",
      "epoch 5 batch 1594 loss: 2.4072928428649902\n",
      "epoch 5 batch 1595 loss: 2.2725932598114014\n",
      "epoch 5 batch 1596 loss: 2.448061466217041\n",
      "epoch 5 batch 1597 loss: 2.0404186248779297\n",
      "epoch 5 batch 1598 loss: 2.466601848602295\n",
      "epoch 5 batch 1599 loss: 2.283794403076172\n",
      "epoch 5 batch 1600 loss: 2.3233656883239746\n",
      "epoch 5 batch 1601 loss: 2.1549267768859863\n",
      "epoch 5 batch 1602 loss: 2.356663227081299\n",
      "epoch 5 batch 1603 loss: 2.253815174102783\n",
      "epoch 5 batch 1604 loss: 2.4470887184143066\n",
      "epoch 5 batch 1605 loss: 2.3574774265289307\n",
      "epoch 5 batch 1606 loss: 2.3314692974090576\n",
      "epoch 5 batch 1607 loss: 2.1633524894714355\n",
      "epoch 5 batch 1608 loss: 2.4567246437072754\n",
      "epoch 5 batch 1609 loss: 2.2654409408569336\n",
      "epoch 5 batch 1610 loss: 2.4132230281829834\n",
      "epoch 5 batch 1611 loss: 2.436401128768921\n",
      "epoch 5 batch 1612 loss: 2.260749578475952\n",
      "epoch 5 batch 1613 loss: 2.3768928050994873\n",
      "epoch 5 batch 1614 loss: 2.421499252319336\n",
      "epoch 5 batch 1615 loss: 2.302335262298584\n",
      "epoch 5 batch 1616 loss: 2.668867826461792\n",
      "epoch 5 batch 1617 loss: 2.3304238319396973\n",
      "epoch 5 batch 1618 loss: 2.428379774093628\n",
      "epoch 5 batch 1619 loss: 2.3509888648986816\n",
      "epoch 5 batch 1620 loss: 2.2489895820617676\n",
      "epoch 5 batch 1621 loss: 2.1934292316436768\n",
      "epoch 5 batch 1622 loss: 2.0241665840148926\n",
      "epoch 5 batch 1623 loss: 2.1454427242279053\n",
      "epoch 5 batch 1624 loss: 2.17315673828125\n",
      "epoch 5 batch 1625 loss: 2.416123867034912\n",
      "epoch 5 batch 1626 loss: 2.459690570831299\n",
      "epoch 5 batch 1627 loss: 2.1401519775390625\n",
      "epoch 5 batch 1628 loss: 2.14096736907959\n",
      "epoch 5 batch 1629 loss: 2.142970085144043\n",
      "epoch 5 batch 1630 loss: 2.0982325077056885\n",
      "epoch 5 batch 1631 loss: 2.188124179840088\n",
      "epoch 5 batch 1632 loss: 2.1371536254882812\n",
      "epoch 5 batch 1633 loss: 2.242488384246826\n",
      "epoch 5 batch 1634 loss: 2.29263973236084\n",
      "epoch 5 batch 1635 loss: 2.0715749263763428\n",
      "epoch 5 batch 1636 loss: 2.1925806999206543\n",
      "epoch 5 batch 1637 loss: 2.1603951454162598\n",
      "epoch 5 batch 1638 loss: 2.2060561180114746\n",
      "epoch 5 batch 1639 loss: 2.1959667205810547\n",
      "epoch 5 batch 1640 loss: 2.1867668628692627\n",
      "epoch 5 batch 1641 loss: 2.084172010421753\n",
      "epoch 5 batch 1642 loss: 2.4945149421691895\n",
      "epoch 5 batch 1643 loss: 2.3559107780456543\n",
      "epoch 5 batch 1644 loss: 2.3171446323394775\n",
      "epoch 5 batch 1645 loss: 2.1209797859191895\n",
      "epoch 5 batch 1646 loss: 2.42503023147583\n",
      "epoch 5 batch 1647 loss: 2.3323898315429688\n",
      "epoch 5 batch 1648 loss: 2.6523656845092773\n",
      "epoch 5 batch 1649 loss: 2.16314435005188\n",
      "epoch 5 batch 1650 loss: 2.183239459991455\n",
      "epoch 5 batch 1651 loss: 2.585637092590332\n",
      "epoch 5 batch 1652 loss: 2.2959365844726562\n",
      "epoch 5 batch 1653 loss: 2.2543044090270996\n",
      "epoch 5 batch 1654 loss: 2.1854562759399414\n",
      "epoch 5 batch 1655 loss: 2.181297540664673\n",
      "epoch 5 batch 1656 loss: 2.1836884021759033\n",
      "epoch 5 batch 1657 loss: 2.244807004928589\n",
      "epoch 5 batch 1658 loss: 2.4102134704589844\n",
      "epoch 5 batch 1659 loss: 2.316514253616333\n",
      "epoch 5 batch 1660 loss: 2.315375804901123\n",
      "epoch 5 batch 1661 loss: 2.1402587890625\n",
      "epoch 5 batch 1662 loss: 2.1315507888793945\n",
      "epoch 5 batch 1663 loss: 2.469560384750366\n",
      "epoch 5 batch 1664 loss: 2.428455352783203\n",
      "epoch 5 batch 1665 loss: 2.2658815383911133\n",
      "epoch 5 batch 1666 loss: 2.11861252784729\n",
      "epoch 5 batch 1667 loss: 2.432767391204834\n",
      "epoch 5 batch 1668 loss: 2.2087960243225098\n",
      "epoch 5 batch 1669 loss: 2.415893077850342\n",
      "epoch 5 batch 1670 loss: 2.05649471282959\n",
      "epoch 5 batch 1671 loss: 2.2885851860046387\n",
      "epoch 5 batch 1672 loss: 2.4991860389709473\n",
      "epoch 5 batch 1673 loss: 2.2306666374206543\n",
      "epoch 5 batch 1674 loss: 2.2221384048461914\n",
      "epoch 5 batch 1675 loss: 2.2273173332214355\n",
      "epoch 5 batch 1676 loss: 2.4283337593078613\n",
      "epoch 5 batch 1677 loss: 2.2525339126586914\n",
      "epoch 5 batch 1678 loss: 2.4017553329467773\n",
      "epoch 5 batch 1679 loss: 2.1922430992126465\n",
      "epoch 5 batch 1680 loss: 2.2891016006469727\n",
      "epoch 5 batch 1681 loss: 2.5089216232299805\n",
      "epoch 5 batch 1682 loss: 2.282477378845215\n",
      "epoch 5 batch 1683 loss: 2.5116071701049805\n",
      "epoch 5 batch 1684 loss: 2.243560791015625\n",
      "epoch 5 batch 1685 loss: 2.163661479949951\n",
      "epoch 5 batch 1686 loss: 2.355541229248047\n",
      "epoch 5 batch 1687 loss: 2.2440319061279297\n",
      "epoch 5 batch 1688 loss: 2.1827445030212402\n",
      "epoch 5 batch 1689 loss: 2.329367160797119\n",
      "epoch 5 batch 1690 loss: 2.145691156387329\n",
      "epoch 5 batch 1691 loss: 2.3928985595703125\n",
      "epoch 5 batch 1692 loss: 2.3635504245758057\n",
      "epoch 5 batch 1693 loss: 2.4631433486938477\n",
      "epoch 5 batch 1694 loss: 2.206827163696289\n",
      "epoch 5 batch 1695 loss: 2.5160341262817383\n",
      "epoch 5 batch 1696 loss: 2.4243693351745605\n",
      "epoch 5 batch 1697 loss: 2.2185606956481934\n",
      "epoch 5 batch 1698 loss: 2.0752744674682617\n",
      "epoch 5 batch 1699 loss: 2.151620864868164\n",
      "epoch 5 batch 1700 loss: 2.199507713317871\n",
      "epoch 5 batch 1701 loss: 2.2958648204803467\n",
      "epoch 5 batch 1702 loss: 2.160409927368164\n",
      "epoch 5 batch 1703 loss: 2.2557778358459473\n",
      "epoch 5 batch 1704 loss: 2.2205426692962646\n",
      "epoch 5 batch 1705 loss: 2.0644328594207764\n",
      "epoch 5 batch 1706 loss: 2.348942995071411\n",
      "epoch 5 batch 1707 loss: 2.1894493103027344\n",
      "epoch 5 batch 1708 loss: 2.3112096786499023\n",
      "epoch 5 batch 1709 loss: 2.132124662399292\n",
      "epoch 5 batch 1710 loss: 2.2520737648010254\n",
      "epoch 5 batch 1711 loss: 2.1229591369628906\n",
      "epoch 5 batch 1712 loss: 2.0482583045959473\n",
      "epoch 5 batch 1713 loss: 2.459383964538574\n",
      "epoch 5 batch 1714 loss: 2.318678379058838\n",
      "epoch 5 batch 1715 loss: 2.1892106533050537\n",
      "epoch 5 batch 1716 loss: 2.242384195327759\n",
      "epoch 5 batch 1717 loss: 2.335381507873535\n",
      "epoch 5 batch 1718 loss: 2.5920472145080566\n",
      "epoch 5 batch 1719 loss: 2.2135391235351562\n",
      "epoch 5 batch 1720 loss: 2.2801766395568848\n",
      "epoch 5 batch 1721 loss: 2.4923758506774902\n",
      "epoch 5 batch 1722 loss: 2.2445950508117676\n",
      "epoch 5 batch 1723 loss: 2.203007698059082\n",
      "epoch 5 batch 1724 loss: 2.205153465270996\n",
      "epoch 5 batch 1725 loss: 2.382852554321289\n",
      "epoch 5 batch 1726 loss: 2.282956600189209\n",
      "epoch 5 batch 1727 loss: 2.076613664627075\n",
      "epoch 5 batch 1728 loss: 2.3140172958374023\n",
      "epoch 5 batch 1729 loss: 2.2414159774780273\n",
      "epoch 5 batch 1730 loss: 2.277130603790283\n",
      "epoch 5 batch 1731 loss: 2.5159952640533447\n",
      "epoch 5 batch 1732 loss: 2.297684669494629\n",
      "epoch 5 batch 1733 loss: 2.2727184295654297\n",
      "epoch 5 batch 1734 loss: 2.361180067062378\n",
      "epoch 5 batch 1735 loss: 2.4159293174743652\n",
      "epoch 5 batch 1736 loss: 2.3761415481567383\n",
      "epoch 5 batch 1737 loss: 2.2309582233428955\n",
      "epoch 5 batch 1738 loss: 2.349456310272217\n",
      "epoch 5 batch 1739 loss: 2.1865365505218506\n",
      "epoch 5 batch 1740 loss: 2.3923957347869873\n",
      "epoch 5 batch 1741 loss: 2.3572254180908203\n",
      "epoch 5 batch 1742 loss: 2.4258313179016113\n",
      "epoch 5 batch 1743 loss: 2.2910850048065186\n",
      "epoch 5 batch 1744 loss: 2.061040163040161\n",
      "epoch 5 batch 1745 loss: 2.169914960861206\n",
      "epoch 5 batch 1746 loss: 2.204746723175049\n",
      "epoch 5 batch 1747 loss: 2.143259048461914\n",
      "epoch 5 batch 1748 loss: 2.752535581588745\n",
      "epoch 5 batch 1749 loss: 2.4740066528320312\n",
      "epoch 5 batch 1750 loss: 2.102322578430176\n",
      "epoch 5 batch 1751 loss: 2.6404857635498047\n",
      "epoch 5 batch 1752 loss: 2.207218647003174\n",
      "epoch 5 batch 1753 loss: 2.2268548011779785\n",
      "epoch 5 batch 1754 loss: 2.328080892562866\n",
      "epoch 5 batch 1755 loss: 2.2289938926696777\n",
      "epoch 5 batch 1756 loss: 2.201793670654297\n",
      "epoch 5 batch 1757 loss: 2.0582358837127686\n",
      "epoch 5 batch 1758 loss: 2.3135933876037598\n",
      "epoch 5 batch 1759 loss: 2.276581287384033\n",
      "epoch 5 batch 1760 loss: 2.418727397918701\n",
      "epoch 5 batch 1761 loss: 2.480198621749878\n",
      "epoch 5 batch 1762 loss: 2.196878433227539\n",
      "epoch 5 batch 1763 loss: 2.2820258140563965\n",
      "epoch 5 batch 1764 loss: 2.4580366611480713\n",
      "epoch 5 batch 1765 loss: 2.236562728881836\n",
      "epoch 5 batch 1766 loss: 2.1836233139038086\n",
      "epoch 5 batch 1767 loss: 2.437298536300659\n",
      "epoch 5 batch 1768 loss: 2.337765693664551\n",
      "epoch 5 batch 1769 loss: 2.081726551055908\n",
      "epoch 5 batch 1770 loss: 2.4971425533294678\n",
      "epoch 5 batch 1771 loss: 2.338350772857666\n",
      "epoch 5 batch 1772 loss: 2.164555788040161\n",
      "epoch 5 batch 1773 loss: 2.269075393676758\n",
      "epoch 5 batch 1774 loss: 2.163421630859375\n",
      "epoch 5 batch 1775 loss: 2.09529447555542\n",
      "epoch 5 batch 1776 loss: 2.15873384475708\n",
      "epoch 5 batch 1777 loss: 2.1893155574798584\n",
      "epoch 5 batch 1778 loss: 2.328002691268921\n",
      "epoch 5 batch 1779 loss: 2.052368640899658\n",
      "epoch 5 batch 1780 loss: 2.427168846130371\n",
      "epoch 5 batch 1781 loss: 2.542205810546875\n",
      "epoch 5 batch 1782 loss: 2.2976317405700684\n",
      "epoch 5 batch 1783 loss: 2.2709078788757324\n",
      "epoch 5 batch 1784 loss: 2.3218188285827637\n",
      "epoch 5 batch 1785 loss: 2.305520534515381\n",
      "epoch 5 batch 1786 loss: 2.363173484802246\n",
      "epoch 5 batch 1787 loss: 2.1588730812072754\n",
      "epoch 5 batch 1788 loss: 2.3574349880218506\n",
      "epoch 5 batch 1789 loss: 2.137352466583252\n",
      "epoch 5 batch 1790 loss: 2.11110258102417\n",
      "epoch 5 batch 1791 loss: 2.2393429279327393\n",
      "epoch 5 batch 1792 loss: 2.3213982582092285\n",
      "epoch 5 batch 1793 loss: 2.406782627105713\n",
      "epoch 5 batch 1794 loss: 2.368562698364258\n",
      "epoch 5 batch 1795 loss: 2.383605480194092\n",
      "epoch 5 batch 1796 loss: 2.554546356201172\n",
      "epoch 5 batch 1797 loss: 2.3559463024139404\n",
      "epoch 5 batch 1798 loss: 2.317182779312134\n",
      "epoch 5 batch 1799 loss: 2.542107105255127\n",
      "epoch 5 batch 1800 loss: 2.080345630645752\n",
      "epoch 5 batch 1801 loss: 1.978684902191162\n",
      "epoch 5 batch 1802 loss: 2.3168435096740723\n",
      "epoch 5 batch 1803 loss: 2.2830419540405273\n",
      "epoch 5 batch 1804 loss: 2.137031078338623\n",
      "epoch 5 batch 1805 loss: 2.3829734325408936\n",
      "epoch 5 batch 1806 loss: 2.4063971042633057\n",
      "epoch 5 batch 1807 loss: 2.2997376918792725\n",
      "epoch 5 batch 1808 loss: 2.3836469650268555\n",
      "epoch 5 batch 1809 loss: 2.261836051940918\n",
      "epoch 5 batch 1810 loss: 2.3689401149749756\n",
      "epoch 5 batch 1811 loss: 2.237396717071533\n",
      "epoch 5 batch 1812 loss: 2.298874616622925\n",
      "epoch 5 batch 1813 loss: 2.1513829231262207\n",
      "epoch 5 batch 1814 loss: 2.263533592224121\n",
      "epoch 5 batch 1815 loss: 2.2000012397766113\n",
      "epoch 5 batch 1816 loss: 1.9719467163085938\n",
      "epoch 5 batch 1817 loss: 2.242722511291504\n",
      "epoch 5 batch 1818 loss: 2.148030996322632\n",
      "epoch 5 batch 1819 loss: 2.3182220458984375\n",
      "epoch 5 batch 1820 loss: 2.440816879272461\n",
      "epoch 5 batch 1821 loss: 2.15549373626709\n",
      "epoch 5 batch 1822 loss: 2.337942123413086\n",
      "epoch 5 batch 1823 loss: 2.238725423812866\n",
      "epoch 5 batch 1824 loss: 2.166780948638916\n",
      "epoch 5 batch 1825 loss: 2.115546703338623\n",
      "epoch 5 batch 1826 loss: 2.190960168838501\n",
      "epoch 5 batch 1827 loss: 2.3330576419830322\n",
      "epoch 5 batch 1828 loss: 2.464451313018799\n",
      "epoch 5 batch 1829 loss: 2.1329498291015625\n",
      "epoch 5 batch 1830 loss: 2.55117130279541\n",
      "epoch 5 batch 1831 loss: 2.463162422180176\n",
      "epoch 5 batch 1832 loss: 2.3397059440612793\n",
      "epoch 5 batch 1833 loss: 2.4573044776916504\n",
      "epoch 5 batch 1834 loss: 2.345255136489868\n",
      "epoch 5 batch 1835 loss: 2.129995346069336\n",
      "epoch 5 batch 1836 loss: 2.205432415008545\n",
      "epoch 5 batch 1837 loss: 2.3188271522521973\n",
      "epoch 5 batch 1838 loss: 2.0938100814819336\n",
      "epoch 5 batch 1839 loss: 2.319046974182129\n",
      "epoch 5 batch 1840 loss: 2.1103439331054688\n",
      "epoch 5 batch 1841 loss: 2.2021737098693848\n",
      "epoch 5 batch 1842 loss: 2.1876578330993652\n",
      "epoch 5 batch 1843 loss: 2.217496395111084\n",
      "epoch 5 batch 1844 loss: 2.470057487487793\n",
      "epoch 5 batch 1845 loss: 2.3174972534179688\n",
      "epoch 5 batch 1846 loss: 2.398620367050171\n",
      "epoch 5 batch 1847 loss: 2.47109055519104\n",
      "epoch 5 batch 1848 loss: 1.9691734313964844\n",
      "epoch 5 batch 1849 loss: 2.3393187522888184\n",
      "epoch 5 batch 1850 loss: 2.1317920684814453\n",
      "epoch 5 batch 1851 loss: 2.363565444946289\n",
      "epoch 5 batch 1852 loss: 2.459902286529541\n",
      "epoch 5 batch 1853 loss: 2.100569248199463\n",
      "epoch 5 batch 1854 loss: 2.144974946975708\n",
      "epoch 5 batch 1855 loss: 2.3590142726898193\n",
      "epoch 5 batch 1856 loss: 2.477400779724121\n",
      "epoch 5 batch 1857 loss: 2.2389450073242188\n",
      "epoch 5 batch 1858 loss: 2.187554359436035\n",
      "epoch 5 batch 1859 loss: 2.2476983070373535\n",
      "epoch 5 batch 1860 loss: 2.162592649459839\n",
      "epoch 5 batch 1861 loss: 2.244729995727539\n",
      "epoch 5 batch 1862 loss: 2.498884677886963\n",
      "epoch 5 batch 1863 loss: 2.256103992462158\n",
      "epoch 5 batch 1864 loss: 2.0782346725463867\n",
      "epoch 5 batch 1865 loss: 2.300344467163086\n",
      "epoch 5 batch 1866 loss: 2.237964391708374\n",
      "epoch 5 batch 1867 loss: 2.3025050163269043\n",
      "epoch 5 batch 1868 loss: 2.4313151836395264\n",
      "epoch 5 batch 1869 loss: 2.270246982574463\n",
      "epoch 5 batch 1870 loss: 2.5681324005126953\n",
      "epoch 5 batch 1871 loss: 2.384284019470215\n",
      "epoch 5 batch 1872 loss: 2.352835178375244\n",
      "epoch 5 batch 1873 loss: 2.1890368461608887\n",
      "epoch 5 batch 1874 loss: 2.2390313148498535\n",
      "epoch 5 batch 1875 loss: 2.22373104095459\n",
      "epoch 5 batch 1876 loss: 2.1427347660064697\n",
      "epoch 5 batch 1877 loss: 2.2485151290893555\n",
      "epoch 5 batch 1878 loss: 2.3079802989959717\n",
      "epoch 5 batch 1879 loss: 2.296288013458252\n",
      "epoch 5 batch 1880 loss: 2.261146068572998\n",
      "epoch 5 batch 1881 loss: 2.238100528717041\n",
      "epoch 5 batch 1882 loss: 2.202996015548706\n",
      "epoch 5 batch 1883 loss: 2.577315330505371\n",
      "epoch 5 batch 1884 loss: 2.120169162750244\n",
      "epoch 5 batch 1885 loss: 2.3584306240081787\n",
      "epoch 5 batch 1886 loss: 2.2886452674865723\n",
      "epoch 5 batch 1887 loss: 2.0791244506835938\n",
      "epoch 5 batch 1888 loss: 2.271785020828247\n",
      "epoch 5 batch 1889 loss: 2.404634714126587\n",
      "epoch 5 batch 1890 loss: 2.303318500518799\n",
      "epoch 5 batch 1891 loss: 2.5494890213012695\n",
      "epoch 5 batch 1892 loss: 2.103602647781372\n",
      "epoch 5 batch 1893 loss: 2.0266315937042236\n",
      "epoch 5 batch 1894 loss: 2.067774534225464\n",
      "epoch 5 batch 1895 loss: 2.013906955718994\n",
      "epoch 5 batch 1896 loss: 2.4513916969299316\n",
      "epoch 5 batch 1897 loss: 2.2020187377929688\n",
      "epoch 5 batch 1898 loss: 2.2353475093841553\n",
      "epoch 5 batch 1899 loss: 2.222370147705078\n",
      "epoch 5 batch 1900 loss: 2.4705584049224854\n",
      "epoch 5 batch 1901 loss: 2.2783806324005127\n",
      "epoch 5 batch 1902 loss: 2.330301284790039\n",
      "epoch 5 batch 1903 loss: 2.3900108337402344\n",
      "epoch 5 batch 1904 loss: 2.1332733631134033\n",
      "epoch 5 batch 1905 loss: 2.2977957725524902\n",
      "epoch 5 batch 1906 loss: 2.448090076446533\n",
      "epoch 5 batch 1907 loss: 2.379565715789795\n",
      "epoch 5 batch 1908 loss: 2.3177638053894043\n",
      "epoch 5 batch 1909 loss: 2.204787254333496\n",
      "epoch 5 batch 1910 loss: 2.243922710418701\n",
      "epoch 5 batch 1911 loss: 2.225050926208496\n",
      "epoch 5 batch 1912 loss: 2.2776668071746826\n",
      "epoch 5 batch 1913 loss: 2.367367744445801\n",
      "epoch 5 batch 1914 loss: 2.240006446838379\n",
      "epoch 5 batch 1915 loss: 2.1038460731506348\n",
      "epoch 5 batch 1916 loss: 2.117621898651123\n",
      "epoch 5 batch 1917 loss: 2.5314390659332275\n",
      "epoch 5 batch 1918 loss: 2.010129451751709\n",
      "epoch 5 batch 1919 loss: 2.1789448261260986\n",
      "epoch 5 batch 1920 loss: 2.1931662559509277\n",
      "epoch 5 batch 1921 loss: 2.4106152057647705\n",
      "epoch 5 batch 1922 loss: 2.453932523727417\n",
      "epoch 5 batch 1923 loss: 2.4515438079833984\n",
      "epoch 5 batch 1924 loss: 2.1411757469177246\n",
      "epoch 5 batch 1925 loss: 2.202848434448242\n",
      "epoch 5 batch 1926 loss: 2.471322774887085\n",
      "epoch 5 batch 1927 loss: 2.2196459770202637\n",
      "epoch 5 batch 1928 loss: 2.4355697631835938\n",
      "epoch 5 batch 1929 loss: 2.262195587158203\n",
      "epoch 5 batch 1930 loss: 2.2578907012939453\n",
      "epoch 5 batch 1931 loss: 2.428919553756714\n",
      "epoch 5 batch 1932 loss: 2.400747299194336\n",
      "epoch 5 batch 1933 loss: 2.3750345706939697\n",
      "epoch 5 batch 1934 loss: 2.292447328567505\n",
      "epoch 5 batch 1935 loss: 2.340662717819214\n",
      "epoch 5 batch 1936 loss: 2.3007349967956543\n",
      "epoch 5 batch 1937 loss: 2.2055697441101074\n",
      "epoch 5 batch 1938 loss: 2.2118139266967773\n",
      "epoch 5 batch 1939 loss: 2.34596586227417\n",
      "epoch 5 batch 1940 loss: 2.1827597618103027\n",
      "epoch 5 batch 1941 loss: 2.163469076156616\n",
      "epoch 5 batch 1942 loss: 2.278630018234253\n",
      "epoch 5 batch 1943 loss: 2.366506338119507\n",
      "epoch 5 batch 1944 loss: 2.246600866317749\n",
      "epoch 5 batch 1945 loss: 2.6103317737579346\n",
      "epoch 5 batch 1946 loss: 2.270603656768799\n",
      "epoch 5 batch 1947 loss: 2.3540456295013428\n",
      "epoch 5 batch 1948 loss: 2.3131682872772217\n",
      "epoch 5 batch 1949 loss: 2.161585569381714\n",
      "epoch 5 batch 1950 loss: 2.39534330368042\n",
      "epoch 5 batch 1951 loss: 2.2212791442871094\n",
      "epoch 5 batch 1952 loss: 2.474569797515869\n",
      "epoch 5 batch 1953 loss: 2.1181020736694336\n",
      "epoch 5 batch 1954 loss: 2.0838465690612793\n",
      "epoch 5 batch 1955 loss: 2.300723075866699\n",
      "epoch 5 batch 1956 loss: 2.2104806900024414\n",
      "epoch 5 batch 1957 loss: 2.420558452606201\n",
      "epoch 5 batch 1958 loss: 2.400911331176758\n",
      "epoch 5 batch 1959 loss: 2.544032335281372\n",
      "epoch 5 batch 1960 loss: 2.1262240409851074\n",
      "epoch 5 batch 1961 loss: 2.2949771881103516\n",
      "epoch 5 batch 1962 loss: 2.460395097732544\n",
      "epoch 5 batch 1963 loss: 2.452475070953369\n",
      "epoch 5 batch 1964 loss: 2.3737664222717285\n",
      "epoch 5 batch 1965 loss: 2.439596176147461\n",
      "epoch 5 batch 1966 loss: 2.2293667793273926\n",
      "epoch 5 batch 1967 loss: 2.311617612838745\n",
      "epoch 5 batch 1968 loss: 2.2476024627685547\n",
      "epoch 5 batch 1969 loss: 2.310051918029785\n",
      "epoch 5 batch 1970 loss: 2.1857481002807617\n",
      "epoch 5 batch 1971 loss: 2.1608400344848633\n",
      "epoch 5 batch 1972 loss: 2.4846105575561523\n",
      "epoch 5 batch 1973 loss: 2.226881504058838\n",
      "epoch 5 batch 1974 loss: 2.370955467224121\n",
      "epoch 5 batch 1975 loss: 2.4103541374206543\n",
      "epoch 5 batch 1976 loss: 2.1399993896484375\n",
      "epoch 5 batch 1977 loss: 2.3119068145751953\n",
      "epoch 5 batch 1978 loss: 2.1567025184631348\n",
      "epoch 5 batch 1979 loss: 2.280951499938965\n",
      "epoch 5 batch 1980 loss: 2.3339738845825195\n",
      "epoch 5 batch 1981 loss: 2.4549379348754883\n",
      "epoch 5 batch 1982 loss: 2.229152202606201\n",
      "epoch 5 batch 1983 loss: 2.2824742794036865\n",
      "epoch 5 batch 1984 loss: 2.6471309661865234\n",
      "epoch 5 batch 1985 loss: 2.3984005451202393\n",
      "epoch 5 batch 1986 loss: 2.2630908489227295\n",
      "epoch 5 batch 1987 loss: 2.277514934539795\n",
      "epoch 5 batch 1988 loss: 2.2749664783477783\n",
      "epoch 5 batch 1989 loss: 2.3176674842834473\n",
      "epoch 5 batch 1990 loss: 2.1564924716949463\n",
      "epoch 5 batch 1991 loss: 2.335500478744507\n",
      "epoch 5 batch 1992 loss: 2.2765049934387207\n",
      "epoch 5 batch 1993 loss: 2.1991872787475586\n",
      "epoch 5 batch 1994 loss: 2.4321985244750977\n",
      "epoch 5 batch 1995 loss: 2.5617213249206543\n",
      "epoch 5 batch 1996 loss: 2.29693603515625\n",
      "epoch 5 batch 1997 loss: 2.338768482208252\n",
      "epoch 5 batch 1998 loss: 2.15372371673584\n",
      "epoch 5 batch 1999 loss: 2.1204679012298584\n",
      "epoch 5 batch 2000 loss: 2.3035476207733154\n",
      "epoch 5 batch 2001 loss: 2.3359408378601074\n",
      "epoch 5 batch 2002 loss: 2.2326929569244385\n",
      "epoch 5 batch 2003 loss: 2.2379517555236816\n",
      "epoch 5 batch 2004 loss: 2.1288440227508545\n",
      "epoch 5 batch 2005 loss: 2.0825538635253906\n",
      "epoch 5 batch 2006 loss: 2.3541970252990723\n",
      "epoch 5 batch 2007 loss: 2.2144603729248047\n",
      "epoch 5 batch 2008 loss: 2.213920831680298\n",
      "epoch 5 batch 2009 loss: 2.2640786170959473\n",
      "epoch 5 batch 2010 loss: 2.4402294158935547\n",
      "epoch 5 batch 2011 loss: 2.3696672916412354\n",
      "epoch 5 batch 2012 loss: 2.2498154640197754\n",
      "epoch 5 batch 2013 loss: 2.195802927017212\n",
      "epoch 5 batch 2014 loss: 2.2796828746795654\n",
      "epoch 5 batch 2015 loss: 2.5806031227111816\n",
      "epoch 5 batch 2016 loss: 2.0896127223968506\n",
      "epoch 5 batch 2017 loss: 2.600557327270508\n",
      "epoch 5 batch 2018 loss: 2.2266032695770264\n",
      "epoch 5 batch 2019 loss: 2.2783315181732178\n",
      "epoch 5 batch 2020 loss: 2.2550048828125\n",
      "epoch 5 batch 2021 loss: 2.196469783782959\n",
      "epoch 5 batch 2022 loss: 2.17094087600708\n",
      "epoch 5 batch 2023 loss: 2.303378105163574\n",
      "epoch 5 batch 2024 loss: 2.2208218574523926\n",
      "epoch 5 batch 2025 loss: 2.387486696243286\n",
      "epoch 5 batch 2026 loss: 2.2327280044555664\n",
      "epoch 5 batch 2027 loss: 2.311903238296509\n",
      "epoch 5 batch 2028 loss: 2.126725912094116\n",
      "epoch 5 batch 2029 loss: 2.1305742263793945\n",
      "epoch 5 batch 2030 loss: 2.369753837585449\n",
      "epoch 5 batch 2031 loss: 2.5353338718414307\n",
      "epoch 5 batch 2032 loss: 2.147797107696533\n",
      "epoch 5 batch 2033 loss: 2.266249179840088\n",
      "epoch 5 batch 2034 loss: 2.453056812286377\n",
      "epoch 5 batch 2035 loss: 2.135636329650879\n",
      "epoch 5 batch 2036 loss: 2.4347825050354004\n",
      "epoch 5 batch 2037 loss: 2.3216304779052734\n",
      "epoch 5 batch 2038 loss: 2.571345090866089\n",
      "epoch 5 batch 2039 loss: 2.416872024536133\n",
      "epoch 5 batch 2040 loss: 2.3263254165649414\n",
      "epoch 5 batch 2041 loss: 2.4888439178466797\n",
      "epoch 5 batch 2042 loss: 2.200744152069092\n",
      "epoch 5 batch 2043 loss: 2.121570110321045\n",
      "epoch 5 batch 2044 loss: 2.130154609680176\n",
      "epoch 5 batch 2045 loss: 2.3649191856384277\n",
      "epoch 5 batch 2046 loss: 2.36645770072937\n",
      "epoch 5 batch 2047 loss: 2.4073264598846436\n",
      "epoch 5 batch 2048 loss: 2.6180741786956787\n",
      "epoch 5 batch 2049 loss: 2.407313823699951\n",
      "epoch 5 batch 2050 loss: 2.4043595790863037\n",
      "epoch 5 batch 2051 loss: 2.0327773094177246\n",
      "epoch 5 batch 2052 loss: 2.1332859992980957\n",
      "epoch 5 batch 2053 loss: 2.3088817596435547\n",
      "epoch 5 batch 2054 loss: 2.3572137355804443\n",
      "epoch 5 batch 2055 loss: 2.50999116897583\n",
      "epoch 5 batch 2056 loss: 2.197411298751831\n",
      "epoch 5 batch 2057 loss: 2.4107346534729004\n",
      "epoch 5 batch 2058 loss: 2.431516408920288\n",
      "epoch 5 batch 2059 loss: 2.178957939147949\n",
      "epoch 5 batch 2060 loss: 2.333099603652954\n",
      "epoch 5 batch 2061 loss: 2.51631498336792\n",
      "epoch 5 batch 2062 loss: 2.627289295196533\n",
      "epoch 5 batch 2063 loss: 2.2782633304595947\n",
      "epoch 5 batch 2064 loss: 2.426065683364868\n",
      "epoch 5 batch 2065 loss: 2.320570468902588\n",
      "epoch 5 batch 2066 loss: 2.255774974822998\n",
      "epoch 5 batch 2067 loss: 2.21812105178833\n",
      "epoch 5 batch 2068 loss: 2.195528984069824\n",
      "epoch 5 batch 2069 loss: 2.412907838821411\n",
      "epoch 5 batch 2070 loss: 2.239340305328369\n",
      "epoch 5 batch 2071 loss: 2.286457061767578\n",
      "epoch 5 batch 2072 loss: 2.017174243927002\n",
      "epoch 5 batch 2073 loss: 2.50899600982666\n",
      "epoch 5 batch 2074 loss: 2.579746961593628\n",
      "epoch 5 batch 2075 loss: 2.2188422679901123\n",
      "epoch 5 batch 2076 loss: 2.1491003036499023\n",
      "epoch 5 batch 2077 loss: 2.24369740486145\n",
      "epoch 5 batch 2078 loss: 2.582825183868408\n",
      "epoch 5 batch 2079 loss: 2.243809700012207\n",
      "epoch 5 batch 2080 loss: 2.3505120277404785\n",
      "epoch 5 batch 2081 loss: 2.1945254802703857\n",
      "epoch 5 batch 2082 loss: 2.3741517066955566\n",
      "epoch 5 batch 2083 loss: 2.3073623180389404\n",
      "epoch 5 batch 2084 loss: 2.357173442840576\n",
      "epoch 5 batch 2085 loss: 2.177128791809082\n",
      "epoch 5 batch 2086 loss: 2.219245672225952\n",
      "epoch 5 batch 2087 loss: 2.491069793701172\n",
      "epoch 5 batch 2088 loss: 2.3335189819335938\n",
      "epoch 5 batch 2089 loss: 2.3976902961730957\n",
      "epoch 5 batch 2090 loss: 2.3031413555145264\n",
      "epoch 5 batch 2091 loss: 2.051868438720703\n",
      "epoch 5 batch 2092 loss: 2.370638370513916\n",
      "epoch 5 batch 2093 loss: 2.420525074005127\n",
      "epoch 5 batch 2094 loss: 2.1998367309570312\n",
      "epoch 5 batch 2095 loss: 2.341228485107422\n",
      "epoch 5 batch 2096 loss: 2.3545541763305664\n",
      "epoch 5 batch 2097 loss: 2.3864121437072754\n",
      "epoch 5 batch 2098 loss: 2.187073230743408\n",
      "epoch 5 batch 2099 loss: 2.2723422050476074\n",
      "epoch 5 batch 2100 loss: 2.201537847518921\n",
      "epoch 5 batch 2101 loss: 2.2261691093444824\n",
      "epoch 5 batch 2102 loss: 2.465862512588501\n",
      "epoch 5 batch 2103 loss: 2.2210092544555664\n",
      "epoch 5 batch 2104 loss: 2.1794486045837402\n",
      "epoch 5 batch 2105 loss: 2.2743334770202637\n",
      "epoch 5 batch 2106 loss: 2.253415107727051\n",
      "epoch 5 batch 2107 loss: 2.171764850616455\n",
      "epoch 5 batch 2108 loss: 2.304882764816284\n",
      "epoch 5 batch 2109 loss: 2.362396717071533\n",
      "epoch 5 batch 2110 loss: 2.3499550819396973\n",
      "epoch 5 batch 2111 loss: 2.388953685760498\n",
      "epoch 5 batch 2112 loss: 2.4855051040649414\n",
      "epoch 5 batch 2113 loss: 2.360104560852051\n",
      "epoch 5 batch 2114 loss: 2.2947275638580322\n",
      "epoch 5 batch 2115 loss: 2.150108575820923\n",
      "epoch 5 batch 2116 loss: 2.1038670539855957\n",
      "epoch 5 batch 2117 loss: 2.5012574195861816\n",
      "epoch 5 batch 2118 loss: 2.155374765396118\n",
      "epoch 5 batch 2119 loss: 2.507227659225464\n",
      "epoch 5 batch 2120 loss: 2.1528725624084473\n",
      "epoch 5 batch 2121 loss: 2.2968432903289795\n",
      "epoch 5 batch 2122 loss: 2.252246379852295\n",
      "epoch 5 batch 2123 loss: 2.431797981262207\n",
      "epoch 5 batch 2124 loss: 2.4150283336639404\n",
      "epoch 5 batch 2125 loss: 2.2570526599884033\n",
      "epoch 5 batch 2126 loss: 2.4981112480163574\n",
      "epoch 5 batch 2127 loss: 2.494422435760498\n",
      "epoch 5 batch 2128 loss: 2.159345865249634\n",
      "epoch 5 batch 2129 loss: 2.1546006202697754\n",
      "epoch 5 batch 2130 loss: 2.198803663253784\n",
      "epoch 5 batch 2131 loss: 2.312291145324707\n",
      "epoch 5 batch 2132 loss: 2.1063477993011475\n",
      "epoch 5 batch 2133 loss: 2.1289377212524414\n",
      "epoch 5 batch 2134 loss: 2.1270647048950195\n",
      "epoch 5 batch 2135 loss: 2.3055381774902344\n",
      "epoch 5 batch 2136 loss: 2.3796675205230713\n",
      "epoch 5 batch 2137 loss: 2.2998499870300293\n",
      "epoch 5 batch 2138 loss: 2.242764472961426\n",
      "epoch 5 batch 2139 loss: 2.415609836578369\n",
      "epoch 5 batch 2140 loss: 2.494580030441284\n",
      "epoch 5 batch 2141 loss: 2.33107852935791\n",
      "epoch 5 batch 2142 loss: 2.3185784816741943\n",
      "epoch 5 batch 2143 loss: 2.4560327529907227\n",
      "epoch 5 batch 2144 loss: 2.3151090145111084\n",
      "epoch 5 batch 2145 loss: 2.312246322631836\n",
      "epoch 5 batch 2146 loss: 2.1600842475891113\n",
      "epoch 5 batch 2147 loss: 2.1575980186462402\n",
      "epoch 5 batch 2148 loss: 2.30617094039917\n",
      "epoch 5 batch 2149 loss: 2.652676582336426\n",
      "epoch 5 batch 2150 loss: 2.3432939052581787\n",
      "epoch 5 batch 2151 loss: 2.423753261566162\n",
      "epoch 5 batch 2152 loss: 2.11979341506958\n",
      "epoch 5 batch 2153 loss: 2.2506752014160156\n",
      "epoch 5 batch 2154 loss: 2.465954303741455\n",
      "epoch 5 batch 2155 loss: 2.2861318588256836\n",
      "epoch 5 batch 2156 loss: 2.154357433319092\n",
      "epoch 5 batch 2157 loss: 2.3549580574035645\n",
      "epoch 5 batch 2158 loss: 2.5336670875549316\n",
      "epoch 5 batch 2159 loss: 2.485060214996338\n",
      "epoch 5 batch 2160 loss: 2.2930665016174316\n",
      "epoch 5 batch 2161 loss: 2.2481560707092285\n",
      "epoch 5 batch 2162 loss: 2.2810912132263184\n",
      "epoch 5 batch 2163 loss: 2.105943441390991\n",
      "epoch 5 batch 2164 loss: 2.479212760925293\n",
      "epoch 5 batch 2165 loss: 2.06964111328125\n",
      "epoch 5 batch 2166 loss: 2.2503671646118164\n",
      "epoch 5 batch 2167 loss: 2.17525577545166\n",
      "epoch 5 batch 2168 loss: 2.1721487045288086\n",
      "epoch 5 batch 2169 loss: 2.02138614654541\n",
      "epoch 5 batch 2170 loss: 2.2799603939056396\n",
      "epoch 5 batch 2171 loss: 2.3168864250183105\n",
      "epoch 5 batch 2172 loss: 2.4373462200164795\n",
      "epoch 5 batch 2173 loss: 2.0528106689453125\n",
      "epoch 5 batch 2174 loss: 2.4007933139801025\n",
      "epoch 5 batch 2175 loss: 2.308295726776123\n",
      "epoch 5 batch 2176 loss: 2.2842190265655518\n",
      "epoch 5 batch 2177 loss: 2.4253947734832764\n",
      "epoch 5 batch 2178 loss: 2.2741546630859375\n",
      "epoch 5 batch 2179 loss: 2.1225106716156006\n",
      "epoch 5 batch 2180 loss: 1.9503364562988281\n",
      "epoch 5 batch 2181 loss: 2.2168617248535156\n",
      "epoch 5 batch 2182 loss: 2.328761100769043\n",
      "epoch 5 batch 2183 loss: 2.0291101932525635\n",
      "epoch 5 batch 2184 loss: 2.114656448364258\n",
      "epoch 5 batch 2185 loss: 2.2412962913513184\n",
      "epoch 5 batch 2186 loss: 2.2884230613708496\n",
      "epoch 5 batch 2187 loss: 2.1859283447265625\n",
      "epoch 5 batch 2188 loss: 2.171004295349121\n",
      "epoch 5 batch 2189 loss: 2.2979652881622314\n",
      "epoch 5 batch 2190 loss: 2.230400323867798\n",
      "epoch 5 batch 2191 loss: 2.183488607406616\n",
      "epoch 5 batch 2192 loss: 2.515010356903076\n",
      "epoch 5 batch 2193 loss: 2.3686766624450684\n",
      "epoch 5 batch 2194 loss: 2.361210823059082\n",
      "epoch 5 batch 2195 loss: 2.2008023262023926\n",
      "epoch 5 batch 2196 loss: 2.493976593017578\n",
      "epoch 5 batch 2197 loss: 2.303542137145996\n",
      "epoch 5 batch 2198 loss: 2.2086384296417236\n",
      "epoch 5 batch 2199 loss: 2.169581890106201\n",
      "epoch 5 batch 2200 loss: 2.329516649246216\n",
      "epoch 5 batch 2201 loss: 2.1175081729888916\n",
      "epoch 5 batch 2202 loss: 2.349630832672119\n",
      "epoch 5 batch 2203 loss: 2.1505208015441895\n",
      "epoch 5 batch 2204 loss: 2.025174856185913\n",
      "epoch 5 batch 2205 loss: 2.3706517219543457\n",
      "epoch 5 batch 2206 loss: 2.28238844871521\n",
      "epoch 5 batch 2207 loss: 2.265756130218506\n",
      "epoch 5 batch 2208 loss: 2.1606409549713135\n",
      "epoch 5 batch 2209 loss: 2.2982866764068604\n",
      "epoch 5 batch 2210 loss: 2.3919053077697754\n",
      "epoch 5 batch 2211 loss: 2.155557155609131\n",
      "epoch 5 batch 2212 loss: 2.287440776824951\n",
      "epoch 5 batch 2213 loss: 2.2783150672912598\n",
      "epoch 5 batch 2214 loss: 2.1563303470611572\n",
      "epoch 5 batch 2215 loss: 2.1754753589630127\n",
      "epoch 5 batch 2216 loss: 2.1814661026000977\n",
      "epoch 5 batch 2217 loss: 2.279162645339966\n",
      "epoch 5 batch 2218 loss: 2.285980701446533\n",
      "epoch 5 batch 2219 loss: 2.344449281692505\n",
      "epoch 5 batch 2220 loss: 2.2525315284729004\n",
      "epoch 5 batch 2221 loss: 2.3567585945129395\n",
      "epoch 5 batch 2222 loss: 2.299626111984253\n",
      "epoch 5 batch 2223 loss: 2.296154737472534\n",
      "epoch 5 batch 2224 loss: 2.416792154312134\n",
      "epoch 5 batch 2225 loss: 1.9558939933776855\n",
      "epoch 5 batch 2226 loss: 2.2293195724487305\n",
      "epoch 5 batch 2227 loss: 2.4273295402526855\n",
      "epoch 5 batch 2228 loss: 2.3792006969451904\n",
      "epoch 5 batch 2229 loss: 2.258742332458496\n",
      "epoch 5 batch 2230 loss: 2.1019845008850098\n",
      "epoch 5 batch 2231 loss: 2.2156898975372314\n",
      "epoch 5 batch 2232 loss: 2.2957346439361572\n",
      "epoch 5 batch 2233 loss: 2.099453926086426\n",
      "epoch 5 batch 2234 loss: 2.230853319168091\n",
      "epoch 5 batch 2235 loss: 2.419201374053955\n",
      "epoch 5 batch 2236 loss: 2.189444065093994\n",
      "epoch 5 batch 2237 loss: 2.096745491027832\n",
      "epoch 5 batch 2238 loss: 2.197416305541992\n",
      "epoch 5 batch 2239 loss: 2.1337380409240723\n",
      "epoch 5 batch 2240 loss: 2.474769353866577\n",
      "epoch 5 batch 2241 loss: 2.3697447776794434\n",
      "epoch 5 batch 2242 loss: 2.1229653358459473\n",
      "epoch 5 batch 2243 loss: 2.1746225357055664\n",
      "epoch 5 batch 2244 loss: 2.196690559387207\n",
      "epoch 5 batch 2245 loss: 2.4170053005218506\n",
      "epoch 5 batch 2246 loss: 2.1718950271606445\n",
      "epoch 5 batch 2247 loss: 2.2871971130371094\n",
      "epoch 5 batch 2248 loss: 2.2990145683288574\n",
      "epoch 5 batch 2249 loss: 2.497971296310425\n",
      "epoch 5 batch 2250 loss: 2.3108127117156982\n",
      "epoch 5 batch 2251 loss: 2.1222569942474365\n",
      "epoch 5 batch 2252 loss: 2.1705970764160156\n",
      "epoch 5 batch 2253 loss: 2.292135000228882\n",
      "epoch 5 batch 2254 loss: 2.125974655151367\n",
      "epoch 5 batch 2255 loss: 2.120283603668213\n",
      "epoch 5 batch 2256 loss: 2.1325268745422363\n",
      "epoch 5 batch 2257 loss: 2.1514699459075928\n",
      "epoch 5 batch 2258 loss: 2.077139139175415\n",
      "epoch 5 batch 2259 loss: 2.4434070587158203\n",
      "epoch 5 batch 2260 loss: 2.4000792503356934\n",
      "epoch 5 batch 2261 loss: 2.342991352081299\n",
      "epoch 5 batch 2262 loss: 2.088979959487915\n",
      "epoch 5 batch 2263 loss: 2.202906608581543\n",
      "epoch 5 batch 2264 loss: 1.920896053314209\n",
      "epoch 5 batch 2265 loss: 2.3637237548828125\n",
      "epoch 5 batch 2266 loss: 2.246140480041504\n",
      "epoch 5 batch 2267 loss: 2.326732635498047\n",
      "epoch 5 batch 2268 loss: 2.247337818145752\n",
      "epoch 5 batch 2269 loss: 2.3156652450561523\n",
      "epoch 5 batch 2270 loss: 2.3044824600219727\n",
      "epoch 5 batch 2271 loss: 2.4472153186798096\n",
      "epoch 5 batch 2272 loss: 2.393059730529785\n",
      "epoch 5 batch 2273 loss: 2.080995559692383\n",
      "epoch 5 batch 2274 loss: 2.090287923812866\n",
      "epoch 5 batch 2275 loss: 2.342921733856201\n",
      "epoch 5 batch 2276 loss: 2.229978084564209\n",
      "epoch 5 batch 2277 loss: 2.2641634941101074\n",
      "epoch 5 batch 2278 loss: 2.4027552604675293\n",
      "epoch 5 batch 2279 loss: 2.351499319076538\n",
      "epoch 5 batch 2280 loss: 2.133066177368164\n",
      "epoch 5 batch 2281 loss: 2.20371413230896\n",
      "epoch 5 batch 2282 loss: 2.233497142791748\n",
      "epoch 5 batch 2283 loss: 2.568096399307251\n",
      "epoch 5 batch 2284 loss: 2.378520965576172\n",
      "epoch 5 batch 2285 loss: 2.350598096847534\n",
      "epoch 5 batch 2286 loss: 2.171091079711914\n",
      "epoch 5 batch 2287 loss: 2.17042875289917\n",
      "epoch 5 batch 2288 loss: 2.347567558288574\n",
      "epoch 5 batch 2289 loss: 2.1435909271240234\n",
      "epoch 5 batch 2290 loss: 2.2401082515716553\n",
      "epoch 5 batch 2291 loss: 2.2128944396972656\n",
      "epoch 5 batch 2292 loss: 2.3795201778411865\n",
      "epoch 5 batch 2293 loss: 2.239497423171997\n",
      "epoch 5 batch 2294 loss: 2.331479072570801\n",
      "epoch 5 batch 2295 loss: 2.3001902103424072\n",
      "epoch 5 batch 2296 loss: 2.199944496154785\n",
      "epoch 5 batch 2297 loss: 2.069474697113037\n",
      "epoch 5 batch 2298 loss: 2.1999287605285645\n",
      "epoch 5 batch 2299 loss: 2.307192325592041\n",
      "epoch 5 batch 2300 loss: 2.310579299926758\n",
      "epoch 5 batch 2301 loss: 2.23048996925354\n",
      "epoch 5 batch 2302 loss: 2.1487069129943848\n",
      "epoch 5 batch 2303 loss: 2.1238086223602295\n",
      "epoch 5 batch 2304 loss: 2.3917884826660156\n",
      "epoch 5 batch 2305 loss: 2.219433069229126\n",
      "epoch 5 batch 2306 loss: 2.4516186714172363\n",
      "epoch 5 batch 2307 loss: 2.042612075805664\n",
      "epoch 5 batch 2308 loss: 2.2755699157714844\n",
      "epoch 5 batch 2309 loss: 2.353804588317871\n",
      "epoch 5 batch 2310 loss: 2.1179580688476562\n",
      "epoch 5 batch 2311 loss: 2.367692470550537\n",
      "epoch 5 batch 2312 loss: 2.3081469535827637\n",
      "epoch 5 batch 2313 loss: 2.329890012741089\n",
      "epoch 5 batch 2314 loss: 2.276811361312866\n",
      "epoch 5 batch 2315 loss: 2.056434392929077\n",
      "epoch 5 batch 2316 loss: 2.248506546020508\n",
      "epoch 5 batch 2317 loss: 2.305675983428955\n",
      "epoch 5 batch 2318 loss: 2.43037748336792\n",
      "epoch 5 batch 2319 loss: 2.273876667022705\n",
      "epoch 5 batch 2320 loss: 2.3821277618408203\n",
      "epoch 5 batch 2321 loss: 2.1495559215545654\n",
      "epoch 5 batch 2322 loss: 2.206364154815674\n",
      "epoch 5 batch 2323 loss: 2.4366471767425537\n",
      "epoch 5 batch 2324 loss: 2.2224786281585693\n",
      "epoch 5 batch 2325 loss: 2.2662463188171387\n",
      "epoch 5 batch 2326 loss: 2.1599512100219727\n",
      "epoch 5 batch 2327 loss: 2.5940065383911133\n",
      "epoch 5 batch 2328 loss: 2.0748329162597656\n",
      "epoch 5 batch 2329 loss: 2.5261106491088867\n",
      "epoch 5 batch 2330 loss: 2.1146819591522217\n",
      "epoch 5 batch 2331 loss: 2.3202834129333496\n",
      "epoch 5 batch 2332 loss: 1.9809192419052124\n",
      "epoch 5 batch 2333 loss: 2.253298759460449\n",
      "epoch 5 batch 2334 loss: 2.37656307220459\n",
      "epoch 5 batch 2335 loss: 2.096926689147949\n",
      "epoch 5 batch 2336 loss: 2.5580430030822754\n",
      "epoch 5 batch 2337 loss: 2.1657137870788574\n",
      "epoch 5 batch 2338 loss: 2.412597894668579\n",
      "epoch 5 batch 2339 loss: 2.3296146392822266\n",
      "epoch 5 batch 2340 loss: 2.22027850151062\n",
      "epoch 5 batch 2341 loss: 1.9850327968597412\n",
      "epoch 5 batch 2342 loss: 2.1484599113464355\n",
      "epoch 5 batch 2343 loss: 2.1865110397338867\n",
      "epoch 5 batch 2344 loss: 2.1412885189056396\n",
      "epoch 5 batch 2345 loss: 2.0781702995300293\n",
      "epoch 5 batch 2346 loss: 2.31181001663208\n",
      "epoch 5 batch 2347 loss: 2.4283394813537598\n",
      "epoch 5 batch 2348 loss: 2.081418514251709\n",
      "epoch 5 batch 2349 loss: 2.1820149421691895\n",
      "epoch 5 batch 2350 loss: 2.117889642715454\n",
      "epoch 5 batch 2351 loss: 2.1327357292175293\n",
      "epoch 5 batch 2352 loss: 2.2756171226501465\n",
      "epoch 5 batch 2353 loss: 2.558429718017578\n",
      "epoch 5 batch 2354 loss: 2.603637933731079\n",
      "epoch 5 batch 2355 loss: 2.2133474349975586\n",
      "epoch 5 batch 2356 loss: 2.2745249271392822\n",
      "epoch 5 batch 2357 loss: 2.0615434646606445\n",
      "epoch 5 batch 2358 loss: 2.4422338008880615\n",
      "epoch 5 batch 2359 loss: 2.3576369285583496\n",
      "epoch 5 batch 2360 loss: 2.508713722229004\n",
      "epoch 5 batch 2361 loss: 2.335228204727173\n",
      "epoch 5 batch 2362 loss: 2.2426934242248535\n",
      "epoch 5 batch 2363 loss: 2.142302989959717\n",
      "epoch 5 batch 2364 loss: 2.258800506591797\n",
      "epoch 5 batch 2365 loss: 2.174391031265259\n",
      "epoch 5 batch 2366 loss: 2.3266043663024902\n",
      "epoch 5 batch 2367 loss: 2.1788270473480225\n",
      "epoch 5 batch 2368 loss: 2.338871717453003\n",
      "epoch 5 batch 2369 loss: 2.1352756023406982\n",
      "epoch 5 batch 2370 loss: 2.0595030784606934\n",
      "epoch 5 batch 2371 loss: 2.3687658309936523\n",
      "epoch 5 batch 2372 loss: 2.005740165710449\n",
      "epoch 5 batch 2373 loss: 2.331929922103882\n",
      "epoch 5 batch 2374 loss: 2.2095658779144287\n",
      "epoch 5 batch 2375 loss: 2.257335662841797\n",
      "epoch 5 batch 2376 loss: 2.157367706298828\n",
      "epoch 5 batch 2377 loss: 2.196049213409424\n",
      "epoch 5 batch 2378 loss: 2.172524929046631\n",
      "epoch 5 batch 2379 loss: 2.083756923675537\n",
      "epoch 5 batch 2380 loss: 2.389157772064209\n",
      "epoch 5 batch 2381 loss: 2.112001895904541\n",
      "epoch 5 batch 2382 loss: 2.255953788757324\n",
      "epoch 5 batch 2383 loss: 2.565093994140625\n",
      "epoch 5 batch 2384 loss: 2.560337543487549\n",
      "epoch 5 batch 2385 loss: 2.4997735023498535\n",
      "epoch 5 batch 2386 loss: 2.2832281589508057\n",
      "epoch 5 batch 2387 loss: 2.336730480194092\n",
      "epoch 5 batch 2388 loss: 2.0899107456207275\n",
      "epoch 5 batch 2389 loss: 2.123548746109009\n",
      "epoch 5 batch 2390 loss: 2.262756824493408\n",
      "epoch 5 batch 2391 loss: 2.157289981842041\n",
      "epoch 5 batch 2392 loss: 2.277580738067627\n",
      "epoch 5 batch 2393 loss: 2.2905421257019043\n",
      "epoch 5 batch 2394 loss: 2.0328264236450195\n",
      "epoch 5 batch 2395 loss: 2.4439666271209717\n",
      "epoch 5 batch 2396 loss: 2.098381757736206\n",
      "epoch 5 batch 2397 loss: 2.3285605907440186\n",
      "epoch 5 batch 2398 loss: 2.1449193954467773\n",
      "epoch 5 batch 2399 loss: 2.2508180141448975\n",
      "epoch 5 batch 2400 loss: 2.3664069175720215\n",
      "epoch 5 batch 2401 loss: 2.167363166809082\n",
      "epoch 5 batch 2402 loss: 2.1677725315093994\n",
      "epoch 5 batch 2403 loss: 2.4297831058502197\n",
      "epoch 5 batch 2404 loss: 2.359477996826172\n",
      "epoch 5 batch 2405 loss: 2.208998918533325\n",
      "epoch 5 batch 2406 loss: 2.0221056938171387\n",
      "epoch 5 batch 2407 loss: 2.5330758094787598\n",
      "epoch 5 batch 2408 loss: 2.333523750305176\n",
      "epoch 5 batch 2409 loss: 2.1147048473358154\n",
      "epoch 5 batch 2410 loss: 2.515688896179199\n",
      "epoch 5 batch 2411 loss: 2.3471412658691406\n",
      "epoch 5 batch 2412 loss: 2.226409912109375\n",
      "epoch 5 batch 2413 loss: 2.035146713256836\n",
      "epoch 5 batch 2414 loss: 2.449188709259033\n",
      "epoch 5 batch 2415 loss: 1.9806947708129883\n",
      "epoch 5 batch 2416 loss: 2.4368298053741455\n",
      "epoch 5 batch 2417 loss: 2.322166919708252\n",
      "epoch 5 batch 2418 loss: 2.321688652038574\n",
      "epoch 5 batch 2419 loss: 2.125689744949341\n",
      "epoch 5 batch 2420 loss: 2.1124868392944336\n",
      "epoch 5 batch 2421 loss: 2.1224184036254883\n",
      "epoch 5 batch 2422 loss: 2.2983736991882324\n",
      "epoch 5 batch 2423 loss: 2.299602508544922\n",
      "epoch 5 batch 2424 loss: 2.3712849617004395\n",
      "epoch 5 batch 2425 loss: 2.411588191986084\n",
      "epoch 5 batch 2426 loss: 2.339407205581665\n",
      "epoch 5 batch 2427 loss: 2.4866065979003906\n",
      "epoch 5 batch 2428 loss: 2.3187851905822754\n",
      "epoch 5 batch 2429 loss: 2.085800886154175\n",
      "epoch 5 batch 2430 loss: 2.0604324340820312\n",
      "epoch 5 batch 2431 loss: 2.191495180130005\n",
      "epoch 5 batch 2432 loss: 2.258591651916504\n",
      "epoch 5 batch 2433 loss: 2.4794399738311768\n",
      "epoch 5 batch 2434 loss: 2.2021658420562744\n",
      "epoch 5 batch 2435 loss: 2.224658727645874\n",
      "epoch 5 batch 2436 loss: 2.240450382232666\n",
      "epoch 5 batch 2437 loss: 2.472797393798828\n",
      "epoch 5 batch 2438 loss: 2.2638020515441895\n",
      "epoch 5 batch 2439 loss: 2.3207943439483643\n",
      "epoch 5 batch 2440 loss: 2.2430484294891357\n",
      "epoch 5 batch 2441 loss: 2.319032907485962\n",
      "epoch 5 batch 2442 loss: 2.188196897506714\n",
      "epoch 5 batch 2443 loss: 2.2824337482452393\n",
      "epoch 5 batch 2444 loss: 2.277207851409912\n",
      "epoch 5 batch 2445 loss: 2.638092517852783\n",
      "epoch 5 batch 2446 loss: 2.059394359588623\n",
      "epoch 5 batch 2447 loss: 2.0719497203826904\n",
      "epoch 5 batch 2448 loss: 2.2191081047058105\n",
      "epoch 5 batch 2449 loss: 2.2194085121154785\n",
      "epoch 5 batch 2450 loss: 2.3257501125335693\n",
      "epoch 5 batch 2451 loss: 2.1367380619049072\n",
      "epoch 5 batch 2452 loss: 2.532439708709717\n",
      "epoch 5 batch 2453 loss: 2.333878517150879\n",
      "epoch 5 batch 2454 loss: 2.2477424144744873\n",
      "epoch 5 batch 2455 loss: 2.2985455989837646\n",
      "epoch 5 batch 2456 loss: 2.414060592651367\n",
      "epoch 5 batch 2457 loss: 2.3869874477386475\n",
      "epoch 5 batch 2458 loss: 2.40535306930542\n",
      "epoch 5 batch 2459 loss: 2.0761184692382812\n",
      "epoch 5 batch 2460 loss: 2.3092355728149414\n",
      "epoch 5 batch 2461 loss: 2.3994340896606445\n",
      "epoch 5 batch 2462 loss: 2.2740983963012695\n",
      "epoch 5 batch 2463 loss: 2.2889790534973145\n",
      "epoch 5 batch 2464 loss: 2.3514962196350098\n",
      "epoch 5 batch 2465 loss: 2.445002555847168\n",
      "epoch 5 batch 2466 loss: 2.200319766998291\n",
      "epoch 5 batch 2467 loss: 2.4427061080932617\n",
      "epoch 5 batch 2468 loss: 2.25586199760437\n",
      "epoch 5 batch 2469 loss: 2.1565146446228027\n",
      "epoch 5 batch 2470 loss: 2.4016594886779785\n",
      "epoch 5 batch 2471 loss: 2.2633042335510254\n",
      "epoch 5 batch 2472 loss: 2.3524560928344727\n",
      "epoch 5 batch 2473 loss: 2.294861316680908\n",
      "epoch 5 batch 2474 loss: 2.2319159507751465\n",
      "epoch 5 batch 2475 loss: 2.1269843578338623\n",
      "epoch 5 batch 2476 loss: 2.575977325439453\n",
      "epoch 5 batch 2477 loss: 2.108675956726074\n",
      "epoch 5 batch 2478 loss: 2.1081690788269043\n",
      "epoch 5 batch 2479 loss: 2.2960047721862793\n",
      "epoch 5 batch 2480 loss: 2.0516772270202637\n",
      "epoch 5 batch 2481 loss: 2.26623797416687\n",
      "epoch 5 batch 2482 loss: 2.1729300022125244\n",
      "epoch 5 batch 2483 loss: 2.2806544303894043\n",
      "epoch 5 batch 2484 loss: 2.336049795150757\n",
      "epoch 5 batch 2485 loss: 2.565030336380005\n",
      "epoch 5 batch 2486 loss: 2.302917957305908\n",
      "epoch 5 batch 2487 loss: 2.204556703567505\n",
      "epoch 5 batch 2488 loss: 2.390934467315674\n",
      "epoch 5 batch 2489 loss: 2.2873544692993164\n",
      "epoch 5 batch 2490 loss: 2.182499885559082\n",
      "epoch 5 batch 2491 loss: 2.2500429153442383\n",
      "epoch 5 batch 2492 loss: 2.1910407543182373\n",
      "epoch 5 batch 2493 loss: 2.3900794982910156\n",
      "epoch 5 batch 2494 loss: 2.1082260608673096\n",
      "epoch 5 batch 2495 loss: 2.2924697399139404\n",
      "epoch 5 batch 2496 loss: 2.1812193393707275\n",
      "epoch 5 batch 2497 loss: 2.3013455867767334\n",
      "epoch 5 batch 2498 loss: 2.3535213470458984\n",
      "epoch 5 batch 2499 loss: 2.0755839347839355\n",
      "epoch 5 batch 2500 loss: 2.253215789794922\n",
      "epoch 5 batch 2501 loss: 2.2314352989196777\n",
      "epoch 5 batch 2502 loss: 2.207002639770508\n",
      "epoch 5 batch 2503 loss: 2.27685546875\n",
      "epoch 5 batch 2504 loss: 2.393568754196167\n",
      "epoch 5 batch 2505 loss: 2.3852248191833496\n",
      "epoch 5 batch 2506 loss: 2.1661672592163086\n",
      "epoch 5 batch 2507 loss: 2.1937527656555176\n",
      "epoch 5 batch 2508 loss: 2.2141828536987305\n",
      "epoch 5 batch 2509 loss: 2.3590784072875977\n",
      "epoch 5 batch 2510 loss: 2.4011471271514893\n",
      "epoch 5 batch 2511 loss: 2.3228840827941895\n",
      "epoch 5 batch 2512 loss: 2.219513416290283\n",
      "epoch 5 batch 2513 loss: 2.126452922821045\n",
      "epoch 5 batch 2514 loss: 2.2997820377349854\n",
      "epoch 5 batch 2515 loss: 1.9831279516220093\n",
      "epoch 5 batch 2516 loss: 2.3936920166015625\n",
      "epoch 5 batch 2517 loss: 2.279849052429199\n",
      "epoch 5 batch 2518 loss: 2.1259961128234863\n",
      "epoch 5 batch 2519 loss: 2.819019317626953\n",
      "epoch 5 batch 2520 loss: 2.010890483856201\n",
      "epoch 5 batch 2521 loss: 2.1834206581115723\n",
      "epoch 5 batch 2522 loss: 2.3259716033935547\n",
      "epoch 5 batch 2523 loss: 2.116044521331787\n",
      "epoch 5 batch 2524 loss: 2.213665246963501\n",
      "epoch 5 batch 2525 loss: 2.1980299949645996\n",
      "epoch 5 batch 2526 loss: 2.3194732666015625\n",
      "epoch 5 batch 2527 loss: 2.278665542602539\n",
      "epoch 5 batch 2528 loss: 2.0595364570617676\n",
      "epoch 5 batch 2529 loss: 2.220578908920288\n",
      "epoch 5 batch 2530 loss: 2.300615072250366\n",
      "epoch 5 batch 2531 loss: 2.179365634918213\n",
      "epoch 5 batch 2532 loss: 2.154966354370117\n",
      "epoch 5 batch 2533 loss: 2.312704086303711\n",
      "epoch 5 batch 2534 loss: 2.3541479110717773\n",
      "epoch 5 batch 2535 loss: 2.2787327766418457\n",
      "epoch 5 batch 2536 loss: 2.218048334121704\n",
      "epoch 5 batch 2537 loss: 2.2640254497528076\n",
      "epoch 5 batch 2538 loss: 2.2876319885253906\n",
      "epoch 5 batch 2539 loss: 2.1718201637268066\n",
      "epoch 5 batch 2540 loss: 2.1744225025177\n",
      "epoch 5 batch 2541 loss: 2.562025785446167\n",
      "epoch 5 batch 2542 loss: 2.1140494346618652\n",
      "epoch 5 batch 2543 loss: 2.4100260734558105\n",
      "epoch 5 batch 2544 loss: 2.0296144485473633\n",
      "epoch 5 batch 2545 loss: 2.22279691696167\n",
      "epoch 5 batch 2546 loss: 2.32637095451355\n",
      "epoch 5 batch 2547 loss: 2.1417064666748047\n",
      "epoch 5 batch 2548 loss: 2.2705423831939697\n",
      "epoch 5 batch 2549 loss: 2.257021903991699\n",
      "epoch 5 batch 2550 loss: 2.453317880630493\n",
      "epoch 5 batch 2551 loss: 2.213252544403076\n",
      "epoch 5 batch 2552 loss: 2.381864547729492\n",
      "epoch 5 batch 2553 loss: 2.3941235542297363\n",
      "epoch 5 batch 2554 loss: 2.1009278297424316\n",
      "epoch 5 batch 2555 loss: 2.210198402404785\n",
      "epoch 5 batch 2556 loss: 2.267822742462158\n",
      "epoch 5 batch 2557 loss: 2.129903793334961\n",
      "epoch 5 batch 2558 loss: 2.198519706726074\n",
      "epoch 5 batch 2559 loss: 2.4444098472595215\n",
      "epoch 5 batch 2560 loss: 2.2915754318237305\n",
      "epoch 5 batch 2561 loss: 2.3370275497436523\n",
      "epoch 5 batch 2562 loss: 2.2781982421875\n",
      "epoch 5 batch 2563 loss: 2.355158805847168\n",
      "epoch 5 batch 2564 loss: 2.1588640213012695\n",
      "epoch 5 batch 2565 loss: 2.3732104301452637\n",
      "epoch 5 batch 2566 loss: 2.319075584411621\n",
      "epoch 5 batch 2567 loss: 2.125338315963745\n",
      "epoch 5 batch 2568 loss: 2.363158702850342\n",
      "epoch 5 batch 2569 loss: 2.250232219696045\n",
      "epoch 5 batch 2570 loss: 2.3101961612701416\n",
      "epoch 5 batch 2571 loss: 2.1795716285705566\n",
      "epoch 5 batch 2572 loss: 2.3721442222595215\n",
      "epoch 5 batch 2573 loss: 2.6354026794433594\n",
      "epoch 5 batch 2574 loss: 2.3637642860412598\n",
      "epoch 5 batch 2575 loss: 2.168829917907715\n",
      "epoch 5 batch 2576 loss: 2.3158979415893555\n",
      "epoch 5 batch 2577 loss: 2.1750128269195557\n",
      "epoch 5 batch 2578 loss: 2.1022067070007324\n",
      "epoch 5 batch 2579 loss: 2.291973114013672\n",
      "epoch 5 batch 2580 loss: 2.418767213821411\n",
      "epoch 5 batch 2581 loss: 2.146362781524658\n",
      "epoch 5 batch 2582 loss: 2.1887545585632324\n",
      "epoch 5 batch 2583 loss: 2.7153067588806152\n",
      "epoch 5 batch 2584 loss: 2.4518346786499023\n",
      "epoch 5 batch 2585 loss: 2.354583263397217\n",
      "epoch 5 batch 2586 loss: 2.20162034034729\n",
      "epoch 5 batch 2587 loss: 2.552769422531128\n",
      "epoch 5 batch 2588 loss: 2.4580016136169434\n",
      "epoch 5 batch 2589 loss: 2.4883973598480225\n",
      "epoch 5 batch 2590 loss: 2.255186080932617\n",
      "epoch 5 batch 2591 loss: 2.337209463119507\n",
      "epoch 5 batch 2592 loss: 2.1331820487976074\n",
      "epoch 5 batch 2593 loss: 2.4776194095611572\n",
      "epoch 5 batch 2594 loss: 2.5283238887786865\n",
      "epoch 5 batch 2595 loss: 2.312948703765869\n",
      "epoch 5 batch 2596 loss: 2.526468515396118\n",
      "epoch 5 batch 2597 loss: 2.4826996326446533\n",
      "epoch 5 batch 2598 loss: 2.1587963104248047\n",
      "epoch 5 batch 2599 loss: 2.090494155883789\n",
      "epoch 5 batch 2600 loss: 2.4704928398132324\n",
      "epoch 5 batch 2601 loss: 2.217984437942505\n",
      "epoch 5 batch 2602 loss: 2.3649792671203613\n",
      "epoch 5 batch 2603 loss: 2.2959280014038086\n",
      "epoch 5 batch 2604 loss: 2.1931848526000977\n",
      "epoch 5 batch 2605 loss: 2.2521567344665527\n",
      "epoch 5 batch 2606 loss: 2.1608970165252686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 过滤掉未识别的参数\u001b[39;00m\n\u001b[0;32m     73\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[1;32m---> 75\u001b[0m train(args)\n",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 梯度更新\u001b[39;00m\n\u001b[0;32m     48\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 49\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     50\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\fengq\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import net\n",
    "import config\n",
    "import loaddataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# train stage one\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:0\")  # 使用GPU\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current device:\", DEVICE)\n",
    "\n",
    "    # 数据加载\n",
    "    train_dataset = loaddataset.PreDataset(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    # 模型和优化器\n",
    "    model = net.SimCLRStage1().to(DEVICE)\n",
    "    lossLR = net.Loss().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "    \n",
    "    # 混合精度训练\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, args.max_epoch + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch, (imgL, imgR, labels) in enumerate(train_data):\n",
    "            imgL, imgR, labels = imgL.to(DEVICE), imgR.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():  # 使用混合精度训练\n",
    "                _, pre_L = model(imgL)\n",
    "                _, pre_R = model(imgR)\n",
    "                loss = lossLR(pre_L, pre_R, args.batch_size)\n",
    "            \n",
    "            # 梯度更新\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        # 打印并记录每个epoch的损失\n",
    "        print(\"epoch loss:\", total_loss / len(train_dataset) * args.batch_size)\n",
    "        with open(os.path.join(config.save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset) * args.batch_size) + \" \")\n",
    "\n",
    "        # 每5个epoch保存一次模型\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "        \n",
    "        # 释放显存\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='Batch size for training')\n",
    "    parser.add_argument('--max_epoch', default=50, type=int, help='Maximum number of epochs for training')\n",
    "\n",
    "    # 过滤掉未识别的参数\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、有监督训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainstage2.py\n",
    "import torch,argparse,os\n",
    "import net,config\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# train stage two\n",
    "def train(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:\" + str(2))   #config.gpu_name\n",
    "        # 每次训练计算图改动较小使用，在开始前选取较优的基础算法（比如选择一种当前高效的卷积算法）\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current deveice:\", DEVICE)\n",
    "\n",
    "    # load dataset for train and eval\n",
    "    train_dataset = CIFAR10(root='dataset', train=True, transform=config.train_transform, download=True)\n",
    "    train_data = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "    eval_dataset = CIFAR10(root='dataset', train=False, transform=config.test_transform, download=True)\n",
    "    eval_data = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    model =net.SimCLRStage2(num_class=len(train_dataset.classes)).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(args.pre_model, map_location='cpu'),strict=False)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(config.save_path, exist_ok=True)\n",
    "    for epoch in range(1,args.max_epoch+1):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for batch, (data, target) in enumerate(train_data):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            pred = model(data)\n",
    "\n",
    "            loss = loss_criterion(pred, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch\",epoch,\"loss:\", total_loss / len(train_dataset)*args.batch_size)\n",
    "        with open(os.path.join(config.save_path, \"stage2_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset)*args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, 'model_stage2_epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                print(\"batch\", \" \" * 1, \"top1 acc\", \" \" * 1, \"top5 acc\")\n",
    "                total_loss, total_correct_1, total_correct_5, total_num = 0.0, 0.0, 0.0, 0\n",
    "                for batch, (data, target) in enumerate(train_data):\n",
    "                    data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                    pred = model(data)\n",
    "\n",
    "                    total_num += data.size(0)\n",
    "                    prediction = torch.argsort(pred, dim=-1, descending=True)\n",
    "                    top1_acc = torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "                    top5_acc = torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "                    total_correct_1 += top1_acc\n",
    "                    total_correct_5 += top5_acc\n",
    "\n",
    "                    print(\"  {:02}  \".format(batch + 1), \" {:02.3f}%  \".format(top1_acc / data.size(0) * 100),\n",
    "                          \"{:02.3f}%  \".format(top5_acc / data.size(0) * 100))\n",
    "\n",
    "                print(\"all eval dataset:\", \"top1 acc: {:02.3f}%\".format(total_correct_1 / total_num * 100),\n",
    "                          \"top5 acc:{:02.3f}%\".format(total_correct_5 / total_num * 100))\n",
    "                with open(os.path.join(config.save_path, \"stage2_top1_acc.txt\"), \"a\") as f:\n",
    "                    f.write(str(total_correct_1 / total_num * 100) + \" \")\n",
    "                with open(os.path.join(config.save_path, \"stage2_top5_acc.txt\"), \"a\") as f:\n",
    "                    f.write(str(total_correct_5 / total_num * 100) + \" \")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='')\n",
    "    parser.add_argument('--max_epoch', default=200, type=int, help='')\n",
    "    parser.add_argument('--pre_model', default=config.pre_model, type=str, help='')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练并查看过程\n",
    "\n",
    "使用visdom，对训练过程保存的loss、acc进行可视化\n",
    "由于时间关系，只训练了较少的epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showbyvisdom.py\n",
    "import numpy as np\n",
    "import visdom\n",
    "\n",
    "\n",
    "def show_loss(path, name, step=1):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\" \")[:-1]\n",
    "    x = np.linspace(1, len(data) + 1, len(data)) * step\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        y.append(float(data[i]))\n",
    "\n",
    "    vis = visdom.Visdom(env='loss')\n",
    "    vis.line(X=x, Y=y, win=name, opts={'title': name, \"xlabel\": \"epoch\", \"ylabel\": name})\n",
    "\n",
    "\n",
    "def compare2(path_1, path_2, title=\"xxx\", legends=[\"a\", \"b\"], x=\"epoch\", step=20):\n",
    "    with open(path_1, \"r\") as f:\n",
    "        data_1 = f.read()\n",
    "    data_1 = data_1.split(\" \")[:-1]\n",
    "\n",
    "    with open(path_2, \"r\") as f:\n",
    "        data_2 = f.read()\n",
    "    data_2 = data_2.split(\" \")[:-1]\n",
    "\n",
    "    x = np.linspace(1, len(data_1) + 1, len(data_1)) * step\n",
    "    y = []\n",
    "    for i in range(len(data_1)):\n",
    "        y.append([float(data_1[i]), float(data_2[i])])\n",
    "\n",
    "    vis = visdom.Visdom(env='loss')\n",
    "    vis.line(X=x, Y=y, win=\"compare\",\n",
    "             opts={\"title\": \"compare \" + title, \"legend\": legends, \"xlabel\": \"epoch\", \"ylabel\": title})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    show_loss(\"stage1_loss.txt\", \"loss1\")\n",
    "    show_loss(\"stage2_loss.txt\", \"loss2\")\n",
    "    show_loss(\"stage2_top1_acc.txt\", \"acc1\")\n",
    "    show_loss(\"stage2_top5_acc.txt\", \"acc1\")\n",
    "\n",
    "    # compare2(\"precision1.txt\", \"precision2.txt\", title=\"precision\", step=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、验证集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval.py\n",
    "import torch,argparse\n",
    "from torchvision.datasets import CIFAR10\n",
    "import net,config\n",
    "\n",
    "\n",
    "def eval(args):\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:\" + str(config.gpu_name))\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    eval_dataset=CIFAR10(root='dataset', train=False, transform=config.test_transform, download=True)\n",
    "    eval_data=torch.utils.data.DataLoader(eval_dataset,batch_size=args.batch_size, shuffle=False, num_workers=16, )\n",
    "\n",
    "    model=net.SimCLRStage2(num_class=len(eval_dataset.classes)).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(config.pre_model, map_location='cpu'), strict=False)\n",
    "\n",
    "    # total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(eval_data)\n",
    "    total_correct_1, total_correct_5, total_num = 0.0, 0.0, 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"batch\", \" \"*1, \"top1 acc\", \" \"*1,\"top5 acc\" )\n",
    "        for batch, (data, target) in enumerate(eval_data):\n",
    "            data, target = data.to(DEVICE) ,target.to(DEVICE)\n",
    "            pred=model(data)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            prediction = torch.argsort(pred, dim=-1, descending=True)\n",
    "            top1_acc = torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            top5_acc = torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "            total_correct_1 += top1_acc\n",
    "            total_correct_5 += top5_acc\n",
    "\n",
    "            print(\"  {:02}  \".format(batch+1),\" {:02.3f}%  \".format(top1_acc / data.size(0) * 100),\"{:02.3f}%  \".format(top5_acc / data.size(0) * 100))\n",
    "\n",
    "        print(\"all eval dataset:\",\"top1 acc: {:02.3f}%\".format(total_correct_1 / total_num * 100), \"top5 acc:{:02.3f}%\".format(total_correct_5 / total_num * 100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='test SimCLR')\n",
    "    parser.add_argument('--batch_size', default=512, type=int, help='')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    eval(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、自定义图片测试\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import torch,argparse\n",
    "import net,config\n",
    "from torchvision.datasets import CIFAR10\n",
    "import cv2\n",
    "\n",
    "\n",
    "def show_CIFAR10(index):\n",
    "    eval_dataset=CIFAR10(root='dataset', train=False, download=False)\n",
    "    print(eval_dataset.__len__())\n",
    "    print(eval_dataset.class_to_idx,eval_dataset.classes)\n",
    "    img, target=eval_dataset[index][0], eval_dataset[index][1]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(str(target))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    classes={'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
    "    index2class=[x  for x in classes.keys()]\n",
    "    print(\"calss:\",index2class)\n",
    "\n",
    "    if torch.cuda.is_available() and config.use_gpu:\n",
    "        DEVICE = torch.device(\"cuda:\" + str(config.gpu_name))\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    transform = config.test_transform\n",
    "\n",
    "    ori_img=cv2.imread(args.img_path,1)\n",
    "    img=cv2.resize(ori_img,(32,32)) # evry important，influence the result\n",
    "\n",
    "    img=transform(img).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "    model=net.SimCLRStage2(num_class=10).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(args.pre_model, map_location='cpu'), strict=False)\n",
    "\n",
    "    pred = model(img)\n",
    "\n",
    "    prediction = torch.argsort(pred, dim=-1, descending=True)\n",
    "\n",
    "    label=index2class[prediction[:, 0:1].item()]\n",
    "    cv2.putText(ori_img,\"this is \"+label,(30,30),cv2.FONT_HERSHEY_DUPLEX,1, (0,255,0), 1)\n",
    "    cv2.imshow(label,ori_img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # show_CIFAR10(2)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='test SimCLR')\n",
    "    parser.add_argument('--pre_model', default=config.pre_model, type=str, help='')\n",
    "    parser.add_argument('--img_path', default=\"bird.jpg\", type=str, help='')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    test(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_43027065/article/details/118657728"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
