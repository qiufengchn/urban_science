{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic example in Python using TensorFlow to start with feature extraction from figure-ground maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load and preprocess figure-ground map\n",
    "img = cv2.imread('wuhan_figure_ground_map.png', 0)  # Load as grayscale\n",
    "img = cv2.resize(img, (256, 256))  # Resize to match model input\n",
    "img = img / 255.0  # Normalize pixel values\n",
    "img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')  # Assuming 5 morphology classes\n",
    "])\n",
    "\n",
    "# Compile and train the model (use your dataset)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Extract features from the map\n",
    "features = model.predict(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a Python example using PyTorch to extract features from figure-ground maps for urban morphology analysis, similar to the study referenced.\n",
    "\n",
    "### 1. **Setup and Data Preparation**\n",
    "   - First, you'll need to ensure that you have your figure-ground maps of Wuhan prepared and preprocessed (e.g., resized, normalized).\n",
    "\n",
    "### 2. **PyTorch Implementation**\n",
    "\n",
    "Here's a simplified example of how to build a Convolutional Neural Network (CNN) in PyTorch for extracting features from these maps:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset for loading figure-ground maps\n",
    "class UrbanMorphologyDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx], 0)  # Load image as grayscale\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "image_paths = ['wuhan_map_1.png', 'wuhan_map_2.png']  # Replace with your paths\n",
    "dataset = UrbanMorphologyDataset(image_paths=image_paths, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define the CNN Model\n",
    "class UrbanMorphologyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UrbanMorphologyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)  # Assuming 5 classes of urban morphology\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model, define loss and optimizer\n",
    "model = UrbanMorphologyCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, images in enumerate(dataloader):\n",
    "        # Assume target labels are available\n",
    "        labels = torch.randint(0, 5, (images.size(0),))  # Replace with actual labels\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Extract features from the CNN for analysis\n",
    "with torch.no_grad():\n",
    "    for images in dataloader:\n",
    "        features = model(images)\n",
    "        print(features)\n",
    "```\n",
    "\n",
    "### 3. **Explanation of the Code:**\n",
    "- **Dataset**: We define a custom PyTorch `Dataset` class to load the figure-ground maps. Each map is resized to `256x256` pixels and converted to a tensor.\n",
    "- **Model**: The `UrbanMorphologyCNN` class defines a simple CNN with three convolutional layers, followed by a fully connected layer to classify the urban morphology types.\n",
    "- **Training**: The training loop iterates over the dataset, computes the loss using `CrossEntropyLoss`, and updates the model parameters using Adam optimizer.\n",
    "- **Feature Extraction**: After training, you can use the trained CNN to extract features from the images. These features can be used for clustering or further analysis to identify and classify different urban forms.\n",
    "\n",
    "### 4. **Next Steps:**\n",
    "- **Clustering**: You can cluster the extracted features using K-means or any other clustering technique to classify different morphology types.\n",
    "- **Visualization**: Map these classifications back onto the city map of Wuhan for visual analysis, similar to the figure from the original study.\n",
    "\n",
    "### 5. **Further Customization:**\n",
    "- You can refine the CNN architecture or hyperparameters based on the complexity of your dataset.\n",
    "- For more accurate modeling, consider using a pre-trained model and fine-tuning it on your urban morphology dataset.\n",
    "\n",
    "This code provides a starting point for your study on Wuhan's urban morphology. You can expand it based on the specific requirements of your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Custom Dataset for loading figure-ground maps\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset for loading figure-ground maps\n",
    "class UrbanMorphologyDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx], 0)  # Load image as grayscale\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "image_paths = ['wuhan_map_1.png', 'wuhan_map_2.png']  # Replace with your paths\n",
    "dataset = UrbanMorphologyDataset(image_paths=image_paths, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define the CNN Model\n",
    "class UrbanMorphologyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UrbanMorphologyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)  # Assuming 5 classes of urban morphology\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model, define loss and optimizer\n",
    "model = UrbanMorphologyCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, images in enumerate(dataloader):\n",
    "        # Assume target labels are available\n",
    "        labels = torch.randint(0, 5, (images.size(0),))  # Replace with actual labels\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Extract features from the CNN for analysis\n",
    "with torch.no_grad():\n",
    "    for images in dataloader:\n",
    "        features = model(images)\n",
    "        print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"building_footprint_layer\"] > 0  # This creates a binary map where buildings are '1' and others are '0'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can definitely use shapefiles (SHP files) and GIS data to reproduce the kind of urban morphology analysis that you would do with figure-ground maps. Here’s how you can achieve that:\n",
    "\n",
    "### 1. **Data Preparation**\n",
    "   - **Obtain GIS Data**: Start by acquiring GIS data for Wuhan. This could include layers such as building footprints, land use, road networks, etc. You can source these from platforms like OpenStreetMap, government portals, or other GIS data providers.\n",
    "   - **Convert GIS Data to Raster**: The next step is to convert your vector data (shapefiles) into a raster format, which can be more directly analogous to figure-ground maps. Tools like QGIS or ArcGIS can be used for this conversion. Typically, you'll want to create binary rasters where buildings or other structures are represented by one value (e.g., 1) and open spaces by another (e.g., 0).\n",
    "\n",
    "   **In QGIS**:\n",
    "   - Use the \"Rasterize (vector to raster)\" tool.\n",
    "   - Select your shapefile layer, choose the attribute to rasterize (e.g., building footprints), and define the output resolution.\n",
    "\n",
    "### 2. **Generating Figure-Ground Maps**\n",
    "   - **Create Binary Maps**: Once the raster files are generated, you may need to threshold the raster values to produce binary maps, where the built environment (like buildings, roads) is shown in black and open spaces in white. This can be done using QGIS’s raster calculator or similar tools.\n",
    "\n",
    "   **Raster Calculator Example**:\n",
    "   ```sh\n",
    "   [\"building_footprint_layer\"] > 0  # This creates a binary map where buildings are '1' and others are '0'.\n",
    "   ```\n",
    "   - **Mosaic and Merge Layers**: If you have multiple layers (e.g., buildings, roads, etc.), you can merge them into a single raster layer that represents the entire built environment.\n",
    "\n",
    "### 3. **Feature Extraction and Analysis**\n",
    "   - **Export as Images**: Once you have your figure-ground maps, export them as images. These images will serve as input for the deep learning or morphological analysis you intend to conduct.\n",
    "   - **Feature Extraction with CNN**: Follow the same process as previously described, where you use a convolutional neural network (CNN) to extract features from these binary figure-ground images.\n",
    "\n",
    "### 4. **Clustering and Classification**\n",
    "   - Use the CNN to extract feature vectors from the images of Wuhan's urban areas.\n",
    "   - Apply clustering algorithms (e.g., K-means, SOM) to group these areas into different morphological types.\n",
    "\n",
    "### 5. **Mapping and Visualization**\n",
    "   - **Map the Results**: Using GIS tools, map the classified areas back onto the city map. You can color-code different morphological types for clear visualization, similar to the visualizations in the referenced study.\n",
    "   - **Spatial Analysis**: Conduct spatial analysis on the distribution of different morphology types and compare them with socio-economic data or urban regulations to derive meaningful insights.\n",
    "\n",
    "### 6. **Python Implementation Example**:\n",
    "   Here’s a basic outline of how you might proceed with Python using `geopandas` and `rasterio`:\n",
    "\n",
    "   ```python\n",
    "   import geopandas as gpd\n",
    "   import rasterio\n",
    "   from rasterio.features import rasterize\n",
    "   import numpy as np\n",
    "\n",
    "   # Load shapefile\n",
    "   gdf = gpd.read_file('wuhan_buildings.shp')\n",
    "\n",
    "   # Convert to raster\n",
    "   out_shape = (height, width)  # Define the desired output size\n",
    "   transform = rasterio.transform.from_bounds(*gdf.total_bounds, width=width, height=height)\n",
    "\n",
    "   raster = rasterize(\n",
    "       ((geom, 1) for geom in gdf.geometry),\n",
    "       out_shape=out_shape,\n",
    "       transform=transform,\n",
    "       fill=0,\n",
    "       dtype='uint8'\n",
    "   )\n",
    "\n",
    "   # Save the raster as an image\n",
    "   with rasterio.open('wuhan_raster.tif', 'w', driver='GTiff', height=out_shape[0],\n",
    "                      width=out_shape[1], count=1, dtype=raster.dtype, transform=transform) as out_file:\n",
    "       out_file.write(raster, 1)\n",
    "   ```\n",
    "\n",
    "### 7. **Further Analysis:**\n",
    "   - **Urban Form Metrics**: Once you have the binary raster maps, you can calculate various urban form metrics (e.g., fractal dimension, compactness, connectivity) directly on these images or through spatial analysis in GIS.\n",
    "   - **Comparison with Historical Data**: If possible, compare your results with historical maps to analyze how urban forms in Wuhan have evolved over time.\n",
    "\n",
    "By using SHP files and GIS data, you can recreate the figure-ground maps and conduct a detailed urban morphology study of Wuhan, applying deep learning and clustering methods to classify and analyze the city’s structure. This approach combines traditional GIS techniques with modern machine learning methods to offer powerful insights into urban form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，我们一步步用中文来讨论这个课题。以下是我们之前讨论过的内容的逐步说明：\n",
    "\n",
    "### 1. **使用图形地块图进行城市形态学分析**\n",
    "   - 您想在武汉进行类似于“Learning visual features from figure-ground maps for urban morphology discovery”这篇论文中的城市形态学分析。\n",
    "   - **步骤**：\n",
    "     - **获取数据**：首先，您需要获取武汉的图形地块图。这些图像应显示出建筑环境和开放空间的分布情况。可以通过GIS数据、政府部门或在线地图服务（如OpenStreetMap）获取这些图像。\n",
    "     - **数据处理**：将这些图像标准化，确保所有图像的尺寸和分辨率一致，并将其转换为二值图像（黑白图像），以便于后续分析。\n",
    "\n",
    "### 2. **使用深度学习提取特征**\n",
    "   - **深度学习模型**：接下来，您可以使用卷积神经网络（CNN）来从这些二值图像中提取视觉特征。这些特征可以用于分类城市形态类型，类似于原始论文中的方法。\n",
    "   - **实现**：您可以使用TensorFlow或PyTorch来实现这个CNN模型，并对图像进行特征提取。然后，使用聚类算法对这些特征进行分类，找到不同的城市形态类型。\n",
    "\n",
    "### 3. **使用SHP文件和GIS数据替代图形地块图**\n",
    "   - 如果无法使用武汉的图形地块图，您可以使用SHP文件和GIS数据来进行分析。\n",
    "   - **步骤**：\n",
    "     - **将矢量数据转换为栅格数据**：使用GIS软件（如QGIS或ArcGIS）将矢量数据（如建筑轮廓、道路网络）转换为栅格图像，这些图像可以模拟出图形地块图的效果。\n",
    "     - **创建二值图像**：将这些栅格图像处理为二值图像，其中建筑物和其他结构用一个值表示（例如1），开放空间用另一个值表示（例如0）。\n",
    "     - **特征提取和分析**：使用CNN对这些二值图像进行特征提取，接着进行聚类分析，分类出不同的城市形态类型。\n",
    "\n",
    "### 4. **实际操作中的Python代码**\n",
    "   - 您可以使用Python和PyTorch实现这些步骤。例如，可以使用`geopandas`和`rasterio`来处理SHP文件并生成栅格数据。接下来，用PyTorch实现CNN模型，提取并分析这些图像的特征。\n",
    "\n",
    "```python\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "\n",
    "# 读取矢量数据（SHP文件）\n",
    "gdf = gpd.read_file('wuhan_buildings.shp')\n",
    "\n",
    "# 将矢量数据转换为栅格图像\n",
    "out_shape = (height, width)  # 定义输出图像的尺寸\n",
    "transform = rasterio.transform.from_bounds(*gdf.total_bounds, width=width, height=height)\n",
    "\n",
    "raster = rasterize(\n",
    "    ((geom, 1) for geom in gdf.geometry),\n",
    "    out_shape=out_shape,\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    dtype='uint8'\n",
    ")\n",
    "\n",
    "# 将栅格图像保存为文件\n",
    "with rasterio.open('wuhan_raster.tif', 'w', driver='GTiff', height=out_shape[0],\n",
    "                   width=out_shape[1], count=1, dtype=raster.dtype, transform=transform) as out_file:\n",
    "    out_file.write(raster, 1)\n",
    "```\n",
    "\n",
    "### 5. **进一步的分析与可视化**\n",
    "   - **聚类结果的映射**：使用GIS工具将聚类后的城市形态类型映射到武汉的城市地图上，您可以用不同颜色表示不同的形态类型，类似于您最初提到的图形。\n",
    "   - **空间分析**：通过对不同形态类型的分布进行空间分析，结合社会经济数据或城市规划法规，深入探讨城市形态对城市发展的影响。\n",
    "\n",
    "通过以上步骤，您可以利用SHP文件和GIS数据，在武汉进行深入的城市形态学分析，结合深度学习和聚类方法，分类和分析武汉的城市结构。这个过程既结合了传统的GIS技术，又融入了现代机器学习方法，为城市形态学研究提供了强有力的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抱歉之前没有详细解释如何在PyTorch中实现CNN模型来提取和分析图像特征。下面我将一步步详细介绍如何使用PyTorch实现这个过程。\n",
    "\n",
    "### 1. **安装和导入必要的库**\n",
    "\n",
    "首先，确保已经安装了必要的Python库：\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision numpy matplotlib\n",
    "```\n",
    "\n",
    "导入相关库：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "### 2. **创建自定义数据集类**\n",
    "\n",
    "假设您已经生成了武汉城市形态的二值图像，这些图像可以通过SHP文件和GIS数据生成的栅格图像转换得到。接下来，我们创建一个PyTorch `Dataset` 类，用于加载这些图像。\n",
    "\n",
    "```python\n",
    "class UrbanMorphologyDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 读取图像并转为灰度图\n",
    "        image = cv2.imread(self.image_paths[idx], 0)  \n",
    "        image = cv2.resize(image, (256, 256))  # 调整图像大小以适应模型输入\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "```\n",
    "\n",
    "### 3. **定义图像转换和加载数据**\n",
    "\n",
    "定义图像转换并加载数据集：\n",
    "\n",
    "```python\n",
    "# 定义图像转换（将图像转换为张量，并进行标准化）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# 假设您已经有图像路径列表\n",
    "image_paths = ['path_to_image1.png', 'path_to_image2.png']  # 替换为实际路径\n",
    "dataset = UrbanMorphologyDataset(image_paths=image_paths, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "```\n",
    "\n",
    "### 4. **定义CNN模型**\n",
    "\n",
    "接下来，我们定义一个简单的卷积神经网络（CNN）来提取图像特征。这个网络可以有多个卷积层，每层后面跟随一个池化层来减少特征图的尺寸。\n",
    "\n",
    "```python\n",
    "class UrbanMorphologyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UrbanMorphologyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 128)  # 将卷积层输出展平后连接全连接层\n",
    "        self.fc2 = nn.Linear(128, 5)  # 假设有5个城市形态分类\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)  # 展平\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### 5. **训练模型**\n",
    "\n",
    "使用交叉熵损失函数和Adam优化器来训练这个模型。由于我们没有实际的标签，这里使用随机标签做示例，您应该替换为实际数据。\n",
    "\n",
    "```python\n",
    "# 实例化模型、定义损失函数和优化器\n",
    "model = UrbanMorphologyCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 模型训练\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, images in enumerate(dataloader):\n",
    "        # 假设有实际标签，这里使用随机生成的标签\n",
    "        labels = torch.randint(0, 5, (images.size(0),))  # 替换为实际标签\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'第 [{epoch+1}/{num_epochs}] 轮, 损失值: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "### 6. **特征提取**\n",
    "\n",
    "模型训练完成后，您可以使用该模型来提取输入图像的特征。这些特征可以用于进一步的分析，例如聚类或分类。\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for images in dataloader:\n",
    "        features = model(images)\n",
    "        print(features)\n",
    "```\n",
    "\n",
    "### 7. **进一步分析与可视化**\n",
    "   - 提取的特征可以通过聚类算法（例如K-means）进行分类，然后将分类结果映射回武汉的城市地图，以可视化不同的城市形态类型。\n",
    "   - 您可以使用GIS工具将这些分类结果与实际的城市形态数据进行对比分析。\n",
    "\n",
    "通过上述步骤，您可以使用PyTorch实现CNN模型，从武汉的二值图像中提取城市形态特征，并进行分类和分析。希望这些步骤能帮助您更好地理解和实现您的城市形态学研究！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题\n",
    "based on the above pictures, can you tell me how the author make the classifications and Patches-chosen in this article, please?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据您提供的图片，可以推测出这篇文章作者是如何进行分类和选择图像块（patches）的。\n",
    "\n",
    "### 1. **图像块（Patches）的选择**\n",
    "   - **基于规则的系统抽样**：作者可能是通过一种系统抽样的方式来选择城市中的图像块。这意味着在城市区域内，按照一定的规则（如每隔一定距离或在特定的网格中）选取固定大小的图像块。这种方法确保了覆盖整个城市区域的同时，也能够捕捉到不同形态的城市部分。\n",
    "   - **基于地理或功能区域的选择**：此外，图像块可能是基于特定地理或功能区域进行选择的。例如，从市中心（CBD）、住宅区、工业区等不同功能区域中分别选取代表性的图像块，以捕捉这些区域的典型城市形态。\n",
    "\n",
    "### 2. **形态分类的方式**\n",
    "   - **特征提取与聚类分析**：从图中可以看出，作者使用了某种特征提取方法（可能是基于CNN或其他视觉特征提取技术）来处理这些图像块。提取到的特征可能包括建筑物的密度、道路网络的结构、块面形状等。随后，使用聚类算法（如K-means或其他无监督学习方法）将具有相似特征的图像块分为同一类，这就是形态类型的分类方式。\n",
    "   - **形态学特征的多样性**：分类结果表明，城市形态具有明显的多样性。每个类别中的图像块都表现出不同的城市形态特征，这些特征可能是基于建筑密度、街区布局、道路网结构等不同的标准来定义的。\n",
    "\n",
    "### 3. **结果的可视化**\n",
    "   - **分类结果映射到城市地图上**：分类完成后，作者将这些形态类型映射回城市的空间分布地图上。每种颜色代表一种特定的形态类型，这种可视化方式可以直观地显示出不同形态类型在城市中的分布情况。\n",
    "\n",
    "### 4. **总结**\n",
    "   - **形态类型的发现**：作者通过系统的图像块选择、特征提取与聚类分析，成功识别出了城市中的不同形态类型，并通过可视化展示了这些类型在城市中的空间分布。这种方法既可以应用于探索未知城市的形态结构，也可以用于验证城市规划或形态学研究中的假设。\n",
    "\n",
    "这些分类和选择图像块的过程结合了机器学习和城市形态学的知识，为复杂的城市形态结构提供了新的分析方法。如果需要更详细的技术细节，可以参考文献中的方法部分，或使用类似的方法在其他城市进行实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何进行分类和确定分类数量的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据图中显示的内容，可以看出作者在巴塞罗那的城市形态学分析中，通过调整聚类的数量来发现不同的形态类型。这一过程可以通过以下几个步骤来实现：\n",
    "\n",
    "### 1. **图像块（Patches）的特征提取**\n",
    "   - 首先，作者可能使用了卷积神经网络（CNN）或其他图像处理方法对巴塞罗那的图像块进行特征提取。提取的特征包括建筑物的密度、道路网络的结构、街区布局等，这些都是城市形态学中常见的分析指标。\n",
    "\n",
    "### 2. **聚类分析**\n",
    "   - **聚类算法的使用**：在提取了图像块的特征之后，作者使用聚类算法（如K-means）对这些特征进行聚类分析。K-means聚类算法通过将数据点分配到K个簇中，以最小化簇内的方差。\n",
    "   \n",
    "   - **调整聚类数量**：作者通过改变聚类数量K来分析不同的分类结果。例如，在图(a)中，K值设为3，这意味着所有的图像块被分为3类；在图(b)中，K值设为7，图像块被分为7类。这种方法允许研究者观察城市形态在不同聚类粒度下的变化。\n",
    "\n",
    "### 3. **确定聚类数量**\n",
    "   - **肘部法（Elbow Method）**：在K-means聚类中，作者可能使用了肘部法来确定合适的聚类数量。通过绘制簇内平方误差总和（SSE）与聚类数量K的关系图，可以识别出“肘部”位置，这通常对应最佳的K值。\n",
    "   \n",
    "   - **轮廓系数（Silhouette Coefficient）**：作者可能还计算了不同K值下的平均轮廓系数，轮廓系数衡量每个样本与其所在簇的紧密度及与最近簇的分离度。通过比较不同K值下的轮廓系数，选择系数最高的K值作为最佳聚类数。\n",
    "\n",
    "### 4. **结果可视化与分析**\n",
    "   - **结果可视化**：作者将聚类结果映射到巴塞罗那的地图上，并使用不同颜色编码不同的形态类型。这种可视化方法直观地展示了不同聚类粒度下城市形态的空间分布。\n",
    "   \n",
    "   - **分类数量的影响**：从图中可以看出，当K值为3时，巴塞罗那被划分为三大类形态类型；而当K值增加到7时，城市被划分得更加细致，识别出了更多的形态类型。这表明聚类数量的变化会影响分类的粒度和细致程度。\n",
    "\n",
    "### 5. **进一步的分析**\n",
    "   - **优化聚类数量**：研究者可以通过实验和验证进一步优化K值，使其既能够捕捉到足够细致的城市形态类型，又不至于过度细分。\n",
    "   \n",
    "   - **与实际数据对比**：通过将不同K值下的分类结果与巴塞罗那的实际城市功能区划和历史数据对比，研究者可以评估这些分类是否合理并具有实际意义。\n",
    "\n",
    "通过以上步骤和方法，研究者能够有效地确定巴塞罗那的不同城市形态类型，并探索这些类型在城市空间中的分布特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离散度分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一步中，作者进行了离散度分析，其主要目的是评估每个聚类内部的均匀性（homogeneity）以及不同聚类之间的分离度（separation）。通过分析**类内离散度**（Intra-cluster dispersion）和**类间离散度**（Extra-cluster dispersion），可以进一步验证聚类结果的合理性和稳健性。\n",
    "\n",
    "### 1. **类内离散度（Intra-cluster Dispersion）**\n",
    "   - **含义**：类内离散度指的是在同一个聚类内，各个图像块之间特征的差异程度。较低的类内离散度表明该聚类内的图像块在特征空间中更为相似，即它们具有更均匀的城市形态。\n",
    "   - **意义**：类内离散度低意味着聚类算法成功地将相似的图像块分到了一类。这种均匀性有助于确认所识别的城市形态类型是有意义的和一致的。\n",
    "\n",
    "### 2. **类间离散度（Extra-cluster Dispersion）**\n",
    "   - **含义**：类间离散度指的是不同聚类之间特征的差异程度。较高的类间离散度表明不同聚类在特征空间中的分布更加分离，显示出不同城市形态类型的明显差异。\n",
    "   - **意义**：较高的类间离散度表明所发现的聚类不仅在内部是均匀的，而且在与其他聚类的对比中也是明显不同的。这种分离度增强了聚类的辨识度，确认不同形态类型之间的实际差异。\n",
    "\n",
    "### 3. **离散度分析的目的**\n",
    "   - **验证聚类效果**：离散度分析可以帮助研究者判断聚类结果的合理性。如果一个聚类既具有较低的类内离散度，又具有较高的类间离散度，那么这个聚类可以被认为是成功的，因为它很好地捕捉到了特定城市形态类型的共同特征，同时与其他类型的形态保持了明显的区分。\n",
    "   - **优化聚类算法**：通过离散度分析，研究者可以发现聚类算法的潜在问题，例如聚类内存在较大差异或不同聚类之间的界限不清晰。根据这些分析结果，可以调整聚类算法的参数或采用其他更合适的聚类方法。\n",
    "   - **识别城市形态类型的异质性**：离散度分析还可以揭示城市形态的异质性。例如，如果某个城市的类内离散度较高，可能表明该城市的某些区域具有高度多样化的形态特征，这可能与其复杂的历史发展、社会经济条件或规划政策有关。\n",
    "\n",
    "### 4. **在图中的具体应用**\n",
    "   - **不同城市之间的比较**：通过对比San Francisco、Singapore、Amsterdam和Barcelona的离散度分析，可以发现这些城市在形态类型上的一致性和差异性。例如，某些城市的类内离散度较低，表明它们的城市形态类型更加均匀；而类间离散度的变化则展示了城市形态的复杂性和多样性。\n",
    "   - **识别最优聚类数**：离散度分析还可以辅助选择最优的聚类数量。例如，通过对比不同K值下的类内和类间离散度，可以找到一个平衡点，既保证聚类内部的均匀性，又确保不同聚类之间的显著差异。\n",
    "\n",
    "通过离散度分析，研究者能够更深入地理解聚类结果的结构，并基于这些结果做出更可靠的城市形态学研究结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两张图的含义和生成过程如下：\n",
    "\n",
    "### 图 1 (Fig. 13): t-SNE可视化城市形态类型\n",
    "- **含义**：该图展示了通过t-SNE（t-distributed Stochastic Neighbor Embedding）方法对城市形态类型进行的降维和可视化。每个点代表一个训练的图像块（patch），颜色代表之前发现的城市形态类型。\n",
    "  - **颜色编码**：图中不同的颜色对应不同的城市形态类型。例如，图中有代表阿姆斯特丹（AM）、旧金山（SF）、新加坡（SG）、巴塞罗那（BC）的颜色编码。\n",
    "  - **点的分布**：相似的城市形态类型在t-SNE的二维平面上相互靠近，而差异较大的类型则距离较远。这种可视化方法能够直观地展示不同城市形态类型之间的相似性和差异性。\n",
    "\n",
    "### 图 2 (Fig. 14): 形态配对余弦距离\n",
    "- **含义**：该图展示了不同城市形态类型之间的配对余弦距离（cosine distance），其中较低的值表示形态类型之间的相似性较高。\n",
    "  - **矩阵形式**：矩阵中的每个单元格表示两个城市形态类型之间的余弦距离。数值越小，表示这两个类型在特征空间中的角度越接近，也就是形态越相似。\n",
    "  - **颜色编码**：颜色从绿色到蓝色表示距离从大到小，颜色越深表示距离越小，即形态类型之间越相似。\n",
    "\n",
    "### 生成代码示例\n",
    "\n",
    "下面是使用Python和相关库生成这两张图的示例代码。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# 假设已有形态特征数据X和对应的标签labels\n",
    "# X 是形态特征矩阵，labels 是对应的城市形态类型标签\n",
    "\n",
    "# 生成 t-SNE 图 (Fig. 13)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    plt.scatter(X_tsne[labels == label, 0], X_tsne[labels == label, 1], label=label)\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE visualisation of morphology types\")\n",
    "plt.show()\n",
    "\n",
    "# 计算余弦距离 (Fig. 14)\n",
    "dist_matrix = cosine_distances(X)\n",
    "sns.heatmap(dist_matrix, xticklabels=labels, yticklabels=labels, cmap='viridis')\n",
    "plt.title(\"Morphology pairwise cosine distance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 代码说明：\n",
    "1. **t-SNE**：\n",
    "   - `TSNE` 函数将高维特征数据降维到二维空间，并使用散点图（scatter plot）进行可视化。\n",
    "   - 不同标签的点用不同颜色显示，帮助区分不同城市形态类型的相似性。\n",
    "\n",
    "2. **余弦距离**：\n",
    "   - `cosine_distances` 函数计算不同形态类型之间的余弦距离。\n",
    "   - 使用 `seaborn` 的 `heatmap` 函数绘制余弦距离矩阵，并通过颜色深浅表示距离的大小。\n",
    "\n",
    "通过这些步骤，您可以生成类似的可视化图形，用于分析城市形态类型之间的相似性和差异性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使减少了数据量，t-SNE 的计算仍然较为耗时。这是因为 t-SNE 是一种非线性降维方法，需要大量的计算资源，尤其是在处理高维数据时。\n",
    "\n",
    "为了加快运行速度，可以进一步减少数据量，或者使用替代的降维方法，如PCA（主成分分析），虽然它的效果可能不如t-SNE那么好，但计算速度更快。\n",
    "\n",
    "接下来，我将尝试使用PCA来代替t-SNE进行降维，并重新生成图表。\n",
    "\n",
    "现在已经成功生成了两张图：\n",
    "\n",
    "1. **PCA 可视化图**：替代 t-SNE 方法，我们使用了 PCA（主成分分析）将高维形态特征数据降维到二维空间，并通过散点图展示了不同城市形态类型的分布。每种颜色代表一种不同的城市形态类型，点的位置显示了它们在低维空间中的相对关系。\n",
    "\n",
    "2. **余弦距离热力图**：这张图展示了不同城市形态类型之间的余弦距离。颜色越深表示形态类型之间的相似性越高（即距离越小）。\n",
    "\n",
    "这些图可以用于分析和可视化不同城市形态类型之间的相似性和差异性。PCA虽然在捕捉非线性关系方面不如t-SNE，但其计算效率更高，适用于快速分析和初步探索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
